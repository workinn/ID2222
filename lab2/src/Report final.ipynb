{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent Itemsets\n",
    "\n",
    "By Linus Ã–stlund and Daniel Workinn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we implement one of the major families of techniques for characterizing data - discovering `Frequent Itemsets` and `Association Rules`.  \n",
    "\n",
    "To discover `Frequent Itemsets` we have implemented the `A-Priori Algorithm` by following the guidelines in the course litterature. \n",
    "\n",
    "We then use the frequent itemsets to mine `Association Rules`. The association rules quantify the conditional probability of item `j` existing in the basket given itemset `I` (I -> J). Association rules can be used by e.g. a retailer to learn what items are commonly bought together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "First, read the data from the file and create a list of baskets (also called *transactions*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_baskets(path_to_data):\n",
    "    with open(path_to_data) as f:\n",
    "        lines = f.readlines()\n",
    "        baskets = [set(map(int, line.strip().split(sep=' '))) for line in lines]\n",
    "    return baskets\n",
    "\n",
    "def get_unique_items(baskets):\n",
    "    \"\"\"\n",
    "    Helper method to get the number of unique items in the baskets-dataset.\n",
    "    \"\"\"\n",
    "    return set.union(*baskets)\n",
    "\n",
    "def get_number_of_baskets(baskets):\n",
    "    \"\"\"\n",
    "    Helper method to get the number of baskets. But you can also use len(baskets) ðŸ˜‚ I walk over the Ã¥ to get the vatten.\n",
    "    \"\"\"\n",
    "    return len(baskets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed with loading all baskets into a list of sets, where each set is a basket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique items: 870\n",
      "Number of baskets: 100000\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this could differ depending on what OS you are using (I'm on a Mac M1 and I'm on Windows10)\n",
    "path_to_data = '../data/baskets.dat'\n",
    "\n",
    "baskets = import_baskets(path_to_data)\n",
    "\n",
    "print(f'Number of unique items: {len(get_unique_items(baskets))}')\n",
    "print(f'Number of baskets: {len(baskets)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the creation of `L`<sub>`1`</sub> (frequent singletons) we use a dictionary that maps each item to the baskets it is in:\n",
    "    \n",
    "```python\n",
    "    {\n",
    "    0: [1,2,3], # item 0 is in baskets 1, 2 and 3\n",
    "    1: [2, 3],  # item 1 is in baskets 2 and 3\n",
    "    # and so on\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get item and basket dictionary\n",
    "def item_and_basket_dictionary(baskets):\n",
    "    \"\"\"\n",
    "    Returns a dictionary with each each item as a key, and a list of all basket it appears in as a value.\n",
    "    \"\"\"\n",
    "    # set up keys as all unique items\n",
    "    keys = get_unique_items(baskets)\n",
    "\n",
    "    # values will be lists of basket's number where they occur\n",
    "    values = [[] for _ in range(len(get_unique_items(baskets)))]\n",
    "\n",
    "    items = dict(zip(keys, values))\n",
    "\n",
    "    # populate the dictionary\n",
    "    for i, basket in enumerate(baskets):\n",
    "        for item in basket:\n",
    "            items[item].append(i)\n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above dictionary is then used to find the support (in absolute number) for each singleton item by counting the number of baskets it is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'singletons' is the support for all singletons\n",
    "singletons = {k: len(v) for k, v in item_and_basket_dictionary(baskets).items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the filter step, and removing all items that do not meet the minimum support threshold:\n",
    "\n",
    "$ \\text{Minimum support} = 0.01 * \\texttt{len(baskets)} $, in this case $0.01 * 100 000 = 1000$.\n",
    "\n",
    "The result is `L`<sub>`1`</sub> - Frequent Singletons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of significant singletons: 375 with a threshold of 0.01 (minimum support of 1000)\n"
     ]
    }
   ],
   "source": [
    "# set threshold as suggested in lecture\n",
    "threshold = 0.01\n",
    "\n",
    "# filter out all singletons with support below threshold\n",
    "frequent_singletons = {k: v for k, v in singletons.items() if v / len(baskets) >= threshold}\n",
    "\n",
    "# print the number of frequent singletons\n",
    "print(f'Number of significant singletons: {len(frequent_singletons)} with a threshold of {threshold} (minimum support of {int(threshold * len(baskets))})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second pass of the Apriori algorithm, we need a method to generate candidate itemsets. To this end, the course literature suggests the following:\n",
    "\n",
    "> **From course literature (p.226):** During the second pass, we count all the pairs that *consists of two frequent items*. We call this table *frequent-items table*.\n",
    "\n",
    "We then check if the pair is frequent, and if it is, we add it to the set of frequent itemsets.\n",
    "The algorithm for generating candidate itemsets is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_frequent_items_table(baskets, frequent_items):\n",
    "    \"\"\"\n",
    "    Returns a table of frequent items, where each row is a frequent item and each column is a basket.\n",
    "    frequent_items is a list where all frequent singletons has a uniqe m-value.\n",
    "    \"\"\"\n",
    "    # NOTE there are 870 uniqe items assigned to integers [0, 1000]. \n",
    "    # This step is usually done by as a first step in the apriori alogoritm.\n",
    "    frequent_items_table = np.zeros((max(map(max, baskets))) + 1, dtype=int)\n",
    "    # assign 'm' as a unique values, used to mark the frequent items\n",
    "    m = 0\n",
    "    # identify all frequent items\n",
    "    for item in frequent_items:\n",
    "        # NOTE why dont we just use boolean values, is there a reason to use integers?\n",
    "        frequent_items_table[item] = m\n",
    "        m += 1\n",
    "    return frequent_items_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Apriori algorithm requires some more methods; we have the candidate $k$-tuples are frequent.\n",
    "\n",
    "We still follow the outline as suggested in the course literature (p.226):\n",
    "\n",
    "> 1. For each basket, look in the frequent-items table to see which of its items are frequent.\n",
    "> 2. In a double loop, generate all pairs of frequent items in that basket.\n",
    "> 3. For each such pair, add one to its count in the data structure used to store counts.\n",
    "\n",
    "So we are going to look through each basket, and, by using the previous pass frequent items,filter a subset of the items to produce all combinations. The we store those in a dictionary, and increment the count for each combination. This approach is a bit different from the one found in the 1994 paper, as they propose generating all combinations from items in the previous pass. Our approach it is more efficient.\n",
    "\n",
    "To generate all unordered pairs of frequent items in a basket, we use the `combinations` function from the `itertools` library. We use `frozensets` since they are a hashable type, and can be used as keys in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def filter_frequent_items_in_basket(basket, frequent_items_table):\n",
    "    \"\"\"\n",
    "    [HELPER FUNCTION] Return the frequent items in a basket.\n",
    "    \"\"\"\n",
    "    return [item for item in basket if frequent_items_table[item] != 0]\n",
    "\n",
    "def generate_all_combinations(items, k=2):\n",
    "    \"\"\"\n",
    "    [HELPER FUNCTION] Generate all pairs of frequent items in a basket.\n",
    "    \"\"\"\n",
    "    return [frozenset(pair) for pair in combinations(items, k)]\n",
    "\n",
    "def find_candidate_tuples(baskets, frequent_items_table, k=2):\n",
    "    \"\"\"\n",
    "    Find all candidate k-tuples of frequent items.\n",
    "    \"\"\"\n",
    "    candidate_pairs = {}\n",
    "    for basket in baskets:\n",
    "        # filter out all frequent items in the basket\n",
    "        frequent_items_in_basket = filter_frequent_items_in_basket(basket, frequent_items_table)\n",
    "        # genereate all combinations of frequent items in the basket\n",
    "        basket_pairs = generate_all_combinations(frequent_items_in_basket, k=k)\n",
    "        for pair in basket_pairs:\n",
    "            if pair not in candidate_pairs:\n",
    "                candidate_pairs[pair] = 1\n",
    "            else:\n",
    "                candidate_pairs[pair] += 1\n",
    "    return candidate_pairs\n",
    "\n",
    "\n",
    "def filter_candidate_tuples(baskets, candidates, threshold):\n",
    "    \"\"\"\n",
    "    Filter out all candidate k-tuples that do not meet the threshold.\n",
    "    \"\"\"\n",
    "    #return dict(filter(lambda x: x[1] >= threshold, candidate_pairs.items())) # TODO Ã¤r filter bÃ¤ttre?\n",
    "    return {pair: freq for pair, freq in candidates.items() if freq >= threshold*len(baskets)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Apriori algorithm\n",
    "The `apriori()`-function mimic the pseudocode from the 1994 article. In order to speed up the algorithm, we use the suggested approach in the course literature, and use the frequent-items table to filter out infrequent items in all paskets. We then produce all $k$-tuples using the remaining (frequent) items. This assumption holds due to the *monotonicity of itemsets*.\n",
    "\n",
    "### Monotonicity of Itemsets\n",
    "\n",
    "According to the paper and ch 6 of the course literature, the effectiveness of the Apriori algorithm depends on the monotonicity of the itemsets. \n",
    "\n",
    "> **Definition**: If a set $I$ of items is frequent, then so is the powerset of $I$ are also frequent (remember, the **powerset of $I$ is the set of all subsets of $I$**)\n",
    "\n",
    "For example, if the itemset $I = \\{1, 2, 3\\}$ is frequent, then so are *all* subsets, $\\mathscr{P}(I) = \\{\\{1\\}, \\{2\\}, \\{3\\}, \\{1, 2\\}, \\{1, 3\\}, \\{3, 2\\}\\}$, of $I$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(baskets, singletons, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Find frequent itemsets using the Apriori algorithm.\n",
    "    L1 (\"Large 1-tons\") is a frozenset of frequent singletons and their frequencies.\n",
    "    \"\"\"\n",
    "    # find frequent singletons\n",
    "    frequent_singletons = {k: v for k, v in singletons.items() if v / len(baskets) >= threshold}\n",
    "    # convert to a frozenset\n",
    "    L1 = frozenset(frequent_singletons.keys())\n",
    "    # set up initial values\n",
    "    k, L = 2, L1\n",
    "    # this while-loop is just like the pipeline in the lecture\n",
    "    while True:\n",
    "        # genereate frequent-items table\n",
    "        fit = get_frequent_items_table(baskets, L)\n",
    "        # find all candidate pairs\n",
    "        candidates = find_candidate_tuples(baskets, fit, k=k)\n",
    "        # filter candidate pairs\n",
    "        filtered_candidates = filter_candidate_tuples(baskets=baskets, candidates=candidates, threshold=threshold)\n",
    "        # if no filter candidate pairs, break loop\n",
    "        if len(filtered_candidates) == 0:\n",
    "            break\n",
    "        # else, set L to filtered candidates\n",
    "        print(f'Number of {k}-tuples: {len(filtered_candidates)}')\n",
    "        print(f'Number of unique items in {k}-tuples: {len(frozenset.union(*filtered_candidates))}\\n')\n",
    "        L = frozenset.union(*filtered_candidates.keys())\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method does what the pipeline suggest, and break the problem into smaller, handlable chunks.\n",
    "\n",
    "![Apriori pipeline](../imgs/apriori_pipeline.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 2-tuples: 9\n",
      "Number of unique items in 2-tuples: 12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "apriori(baskets, singletons, threshold=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we collect all of the above methods in one neat class that stores all results in a dictionary called `frequent_itemsets`\n",
    "```python\n",
    "# one line to rule them all\n",
    "frequent_itemsets = Apriori(baskets, min_support=0.01).frequent_itemsets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "class Apriori:\n",
    "    def __init__(self, baskets, threshold=0.01):\n",
    "        self.threshold = threshold\n",
    "        self.max_item_id = 0\n",
    "        singletons = {k: len(v) for k, v in self.item_and_basket_dictionary(baskets).items()}\n",
    "        self.frequent_itemsets = self.apriori(baskets=baskets, singletons=singletons, threshold=self.threshold)\n",
    "\n",
    "    def get_unique_items(self, baskets):\n",
    "        \"\"\"\n",
    "        [HELPER METHOD] Get the number of unique items in the baskets-dataset.\n",
    "        \"\"\"\n",
    "        return set.union(*baskets)\n",
    "        \n",
    "    def item_and_basket_dictionary(self, baskets):\n",
    "        \"\"\"\n",
    "        Returns a dictionary with each each item as a key, \n",
    "        and a list of all baskets it appears in as a value.\n",
    "        \"\"\"\n",
    "        # set up keys as all unique items\n",
    "        keys = self.get_unique_items(baskets)\n",
    "        self.max_item_id = max(keys)\n",
    "\n",
    "        # values will be lists of basket's number where they occur\n",
    "        values = [[] for _ in range(len(get_unique_items(baskets)))]\n",
    "\n",
    "        items = dict(zip(keys, values))\n",
    "\n",
    "        # populate the dictionary\n",
    "        for i, basket in enumerate(baskets):\n",
    "            for item in basket:\n",
    "                items[item].append(i)\n",
    "        return items\n",
    "\n",
    "    def get_frequent_items_table(self, frequent_items):\n",
    "        \"\"\"\n",
    "        Returns a table of frequent items, where each row is a frequent item and each column is a basket.\n",
    "        frequent_items is a list where all frequent singletons has a uniqe m-value.\n",
    "        \"\"\"\n",
    "        # NOTE there are 870 uniqe items assigned to integers [0, 1000]. \n",
    "        # This step is usually done by as a first step in the apriori alogoritm.\n",
    "        frequent_items_table = np.zeros(self.max_item_id + 1, dtype=int) \n",
    "        # assign 'm' as a unique values, used to mark the frequent items\n",
    "        m = 1\n",
    "        # identify all frequent items\n",
    "        for item in frequent_items:\n",
    "            # NOTE why dont we just use boolean values, is there a reason to use integers?\n",
    "            frequent_items_table[item] = m\n",
    "            m += 1\n",
    "        return frequent_items_table\n",
    "\n",
    "    def filter_frequent_items_in_basket(self, basket, frequent_items_table):\n",
    "        \"\"\"\n",
    "        [HELPER FUNCTION] Return the frequent items in a basket.\n",
    "        \"\"\"\n",
    "        return [item for item in basket if frequent_items_table[item] != 0]\n",
    "\n",
    "    def generate_all_combinations(self, items, k=2):\n",
    "        \"\"\"\n",
    "        [HELPER FUNCTION] Generate all pairs of frequent items in a basket.\n",
    "        \"\"\"\n",
    "        return [frozenset(pair) for pair in combinations(items, k)]\n",
    "\n",
    "    def find_candidate_tuples(self, baskets, frequent_items_table, k=2):\n",
    "        \"\"\"\n",
    "        Find all candidate k-tuples of frequent items.\n",
    "        \"\"\"\n",
    "        candidate_pairs = {}\n",
    "        for basket in baskets:\n",
    "            # filter out all frequent items in the basket\n",
    "            frequent_items_in_basket = self.filter_frequent_items_in_basket(basket, frequent_items_table)\n",
    "            # genereate all combinations of frequent items in the basket\n",
    "            basket_pairs = self.generate_all_combinations(frequent_items_in_basket, k=k)\n",
    "            for pair in basket_pairs:\n",
    "                if pair not in candidate_pairs:\n",
    "                    candidate_pairs[pair] = 1\n",
    "                else:\n",
    "                    candidate_pairs[pair] += 1\n",
    "        return candidate_pairs\n",
    "\n",
    "\n",
    "    def filter_candidate_tuples(self, baskets, candidates, threshold):\n",
    "        \"\"\"\n",
    "        Filter out all candidate k-tuples that do not meet the threshold.\n",
    "        \"\"\"\n",
    "        #return dict(filter(lambda x: x[1] >= threshold, candidate_pairs.items())) # TODO Ã¤r filter bÃ¤ttre?\n",
    "        return {pair: freq for pair, freq in candidates.items() if freq > threshold*len(baskets)}\n",
    "\n",
    "    def printing_the_print(self, k, filtered_candidates):\n",
    "        print(f'Number of {k}-tuples: {len(filtered_candidates)}')\n",
    "        union = len(frozenset.union(*filtered_candidates))\n",
    "        print(f'Number of unique items in {k}-tuples: {union}\\n')\n",
    "\n",
    "    def apriori(self, baskets, singletons, threshold=0.01):\n",
    "        \"\"\"\n",
    "        Find frequent itemsets using the Apriori algorithm.\n",
    "        L1 (\"Large 1-tons\") is a frozenset of frequent singletons and their frequencies.\n",
    "        \"\"\"\n",
    "        # special case - find frequent singletons\n",
    "        frequent_singletons = {frozenset([k]): v for k, v in singletons.items() if v / len(baskets) > threshold}\n",
    "        # convert to a frozenset\n",
    "        L1 = frozenset.union(*frequent_singletons.keys())\n",
    "        # print progress\n",
    "        self.printing_the_print(1, frequent_singletons)\n",
    "        results = {1: frequent_singletons}\n",
    "        # set up initial values\n",
    "        k, L = 2, L1\n",
    "        # this while-loop is just like the pipeline in the lecture\n",
    "        while True:\n",
    "            # genereate frequent-items table\n",
    "            fit = self.get_frequent_items_table(L)\n",
    "            # find all candidate pairs\n",
    "            candidates = self.find_candidate_tuples(baskets, fit, k=k)\n",
    "            # filter candidate pairs\n",
    "            filtered_candidates = self.filter_candidate_tuples(baskets=baskets, candidates=candidates, threshold=threshold)\n",
    "            # if no filter candidate pairs, break loop\n",
    "            if len(filtered_candidates) == 0:\n",
    "                break\n",
    "            # print the progress\n",
    "            self.printing_the_print(k, filtered_candidates)\n",
    "            # else, set L to filtered candidates and continue\n",
    "            L = frozenset.union(*filtered_candidates.keys())\n",
    "            results[k] = filtered_candidates\n",
    "            k += 1\n",
    "        return results\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 1-tuples: 569\n",
      "Number of unique items in 1-tuples: 569\n",
      "\n",
      "Number of 2-tuples: 339\n",
      "Number of unique items in 2-tuples: 210\n",
      "\n",
      "Number of 3-tuples: 109\n",
      "Number of unique items in 3-tuples: 92\n",
      "\n",
      "Number of 4-tuples: 42\n",
      "Number of unique items in 4-tuples: 55\n",
      "\n",
      "Number of 5-tuples: 9\n",
      "Number of unique items in 5-tuples: 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.005\n",
    "frequent_itemsets = Apriori(baskets, threshold=threshold).frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Association Rules\n",
    "\n",
    "Now we find useful assosciation rules. We use the `AssociationRules` class to do this. The class takes the frequent itemsets as input, and finds all association rules that meet the minimum confidence threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssosciationRules: \n",
    "    def __init__(self, frequent_itemsets):\n",
    "        self.frequent_itemsets = frequent_itemsets\n",
    "        self.rules = self.generate_rules()\n",
    "        print(f'Number of rules: {len(self.rules)}')\n",
    "        self.interesting_rules = None\n",
    "        \n",
    "    def generate_rules(self):\n",
    "        rules = {}\n",
    "        # k is the length of the frequent itemset\n",
    "        # v is the frequent itemset\n",
    "        for k, v in self.frequent_itemsets.items():\n",
    "            # skip singletons\n",
    "            if k == 1:\n",
    "                continue\n",
    "            for itemset, freq in v.items():\n",
    "                for item in itemset:\n",
    "                    # left side of the rule\n",
    "                    antecedent = frozenset(itemset - frozenset([item]))\n",
    "                    # right side of the rule\n",
    "                    consequent = frozenset([item])\n",
    "                    # numerator - freq: support(I union j)\n",
    "                    # denominator: support(I)\n",
    "                    confidence = freq / (self.frequent_itemsets[len(antecedent)][antecedent])\n",
    "                    # store them in a dictionary with the rule as key and confidence as value\n",
    "                    rules[(antecedent, consequent)] = confidence\n",
    "        return rules\n",
    "    \n",
    "    def print_rules(self, n=10, confidence_threshold=0.5):\n",
    "        for i, (rule, confidence) in enumerate(sorted(self.rules.items(), key=lambda x: x[1], reverse=True)):\n",
    "            if i == n:\n",
    "                break\n",
    "            if confidence < confidence_threshold:\n",
    "                continue\n",
    "            else:\n",
    "                antecedent, consequent = rule\n",
    "                print(f'{antecedent} -> {consequent} (confidence: {confidence:.3f})')\n",
    "\n",
    "    def generate_interesting_rules(self, baskets, interest_threshold=0.5):\n",
    "        interesting_rules = {}\n",
    "        num_baskets = len(baskets)\n",
    "        for rule in self.rules:\n",
    "            antecedent, consequent = rule\n",
    "            confidence = self.rules[rule]\n",
    "            \n",
    "            support = self.frequent_itemsets[int(len(consequent))][consequent]\n",
    "            prob_consequent = support/num_baskets\n",
    "            interest = confidence - prob_consequent\n",
    "\n",
    "            if interest > interest_threshold or interest < -interest_threshold:\n",
    "                interesting_rules[(antecedent, consequent)] = interest\n",
    "\n",
    "        print(f'\\nNumber of interesting rules: {len(interesting_rules)}')\n",
    "        self.interesting_rules = interesting_rules\n",
    "\n",
    "    def print_interesting_rules(self, n=10):\n",
    "        for i, (rule, interest) in enumerate(sorted(self.interesting_rules.items(), key=lambda x: x[1], reverse=True)):\n",
    "            if i == n:\n",
    "                break\n",
    "            else:\n",
    "                antecedent, consequent = rule\n",
    "                print(f'{antecedent} -> {consequent} (interest: {interest:.3f})')\n",
    "       \n",
    "    def get_rules(self):\n",
    "        return self.rules\n",
    "    \n",
    "    def get_frequent_itemsets(self):\n",
    "        return self.frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show our implementation, we run the follow cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 1-tuples: 569\n",
      "Number of unique items in 1-tuples: 569\n",
      "\n",
      "Number of 2-tuples: 339\n",
      "Number of unique items in 2-tuples: 210\n",
      "\n",
      "Number of 3-tuples: 109\n",
      "Number of unique items in 3-tuples: 92\n",
      "\n",
      "Number of 4-tuples: 42\n",
      "Number of unique items in 4-tuples: 55\n",
      "\n",
      "Number of 5-tuples: 9\n",
      "Number of unique items in 5-tuples: 20\n",
      "\n",
      "Number of rules: 1218\n",
      "frozenset({960, 185, 678}) -> frozenset({471}) (confidence: 0.994)\n",
      "frozenset({217, 546, 947}) -> frozenset({661}) (confidence: 0.991)\n",
      "frozenset({217, 546, 923}) -> frozenset({661}) (confidence: 0.991)\n",
      "frozenset({217, 546, 947, 923}) -> frozenset({661}) (confidence: 0.991)\n",
      "frozenset({546, 923, 947}) -> frozenset({661}) (confidence: 0.990)\n",
      "\n",
      "Number of interesting rules: 595\n",
      "frozenset({960, 185, 471}) -> frozenset({678}) (interest: 0.975)\n",
      "frozenset({217, 947, 923, 661}) -> frozenset({546}) (interest: 0.974)\n",
      "frozenset({217, 947, 661}) -> frozenset({546}) (interest: 0.971)\n",
      "frozenset({960, 678, 471}) -> frozenset({185}) (interest: 0.970)\n",
      "frozenset({923, 947, 661}) -> frozenset({546}) (interest: 0.968)\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.005\n",
    "apriori = Apriori(baskets, threshold=threshold)\n",
    "assosciation_rules = AssosciationRules(apriori.frequent_itemsets)\n",
    "assosciation_rules.print_rules(n=5, confidence_threshold=0.5)\n",
    "assosciation_rules.generate_interesting_rules(baskets, interest_threshold=0.5)\n",
    "assosciation_rules.print_interesting_rules(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final note\n",
    "\n",
    "There are further optimization to be done, such as using a *bitmap* to store the baskets, and using a *hash tree* to store the frequent itemsets. However, we don't believe those are the primary focus of this lab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b75d418c9c326d7aaeaf58c1d55517f17bf57fa9a587eb451d9ce82208dc144"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
