
Chapter 1IntroductionInventors have long dreamed of creating machines that think. This desire datesback to at least the time of ancient Greece. The mythical ﬁgures Pygmalion,Daedalus, and Hephaestus may all be interpreted as legendary inventors, andGalatea, Talos, and Pandora may all be regarded as artiﬁcial life (Ovid and Martin,2004; Sparkes, 1996; Tandy, 1997).When programmable computers were ﬁrst conceived, people wondered whethersuch machines might become intelligent, over a hundred years before one wasbuilt (Lovelace, 1842). Today,artiﬁcial intelligence(AI) is a thriving ﬁeld withmany practical applications and active research topics. We look to intelligentsoftware to automate routine labor, understand speech or images, make diagnosesin medicine and support basic scientiﬁc research.In the early days of artiﬁcial intelligence, the ﬁeld rapidly tackled and solvedproblems that are intellectually diﬃcult for human beings but relatively straight-forward for computers—problems that can be described by a list of formal, math-ematical rules. The true challenge to artiﬁcial intelligence proved to be solvingthe tasks that are easy for people to perform but hard for people to describeformally—problems that we solve intuitively, that feel automatic, like recognizingspoken words or faces in images.This book is about a solution to these more intuitive problems. This solution isto allow computers to learn from experience and understand the world in terms ofa hierarchy of concepts, with each concept deﬁned through its relation to simplerconcepts. By gathering knowledge from experience, this approach avoids the needfor human operators to formally specify all the knowledge that the computer needs.The hierarchy of concepts enables the computer to learn complicated concepts bybuilding them out of simpler ones. If we draw a graph showing how these concepts1
CHAPTER 1. INTRODUCTIONare built on top of each other, the graph is deep, with many layers. For this reason,we call this approach to AI deep learning.Many of the early successes of AI took place in relatively sterile and formalenvironments and did not require computers to have much knowledge aboutthe world. For example, IBM’s Deep Blue chess-playing system defeated worldchampion Garry Kasparov in 1997 (Hsu, 2002). Chess is of course a very simpleworld, containing only sixty-four locations and thirty-two pieces that can movein only rigidly circumscribed ways. Devising a successful chess strategy is atremendous accomplishment, but the challenge is not due to the diﬃculty ofdescribing the set of chess pieces and allowable moves to the computer. Chesscan be completely described by a very brief list of completely formal rules, easilyprovided ahead of time by the programmer.Ironically, abstract and formal tasks that are among the most diﬃcult mentalundertakings for a human being are among the easiest for a computer. Computershave long been able to defeat even the best human chess player but only recentlyhave begun matching some of the abilities of average human beings to recognizeobjects or speech. A person’s everyday life requires an immense amount ofknowledge about the world. Much of this knowledge is subjective and intuitive,and therefore diﬃcult to articulate in a formal way. Computers need to capturethis same knowledge in order to behave in an intelligent way. One of the keychallenges in artiﬁcial intelligence is how to get this informal knowledge into acomputer.Several artiﬁcial intelligence projects have sought to hard-code knowledgeabout the world in formal languages. A computer can reason automatically aboutstatements in these formal languages using logical inference rules. This is known astheknowledge baseapproach to artiﬁcial intelligence. None of these projects hasled to a major success. One of the most famous such projects is Cyc (Lenat andGuha, 1989). Cyc is an inference engine and a database of statements in a languagecalled CycL. These statements are entered by a staﬀ of human supervisors. It is anunwieldy process. People struggle to devise formal rules with enough complexityto accurately describe the world. For example, Cyc failed to understand a storyabout a person named Fred shaving in the morning (Linde, 1992). Its inferenceengine detected an inconsistency in the story: it knew that people do not haveelectrical parts, but because Fred was holding an electric razor, it believed theentity “FredWhileShaving” contained electrical parts. It therefore asked whetherFred was still a person while he was shaving.The diﬃculties faced by systems relying on hard-coded knowledge suggestthat AI systems need the ability to acquire their own knowledge, by extracting2
CHAPTER 1. INTRODUCTIONpatterns from raw data. This capability is known asmachine learning. Theintroduction of machine learning enabled computers to tackle problems involvingknowledge of the real world and make decisions that appear subjective. A simplemachine learning algorithm calledlogistic regressioncan determine whether torecommend cesarean delivery (Mor-Yosef et al., 1990). A simple machine learningalgorithm called naive Bayes can separate legitimate e-mail from spam e-mail.The performance of these simple machine learning algorithms depends heavilyon therepresentationof the data they are given. For example, when logisticregression is used to recommend cesarean delivery, the AI system does not examinethe patient directly. Instead, the doctor tells the system several pieces of relevantinformation, such as the presence or absence of a uterine scar. Each piece ofinformation included in the representation of the patient is known as afeature.Logistic regression learns how each of these features of the patient correlates withvarious outcomes. However, it cannot inﬂuence how features are deﬁned in anyway. If logistic regression were given an MRI scan of the patient, rather thanthe doctor’s formalized report, it would not be able to make useful predictions.Individual pixels in an MRI scan have negligible correlation with any complicationsthat might occur during delivery.This dependence on representations is a general phenomenon that appearsthroughout computer science and even daily life. In computer science, operationssuch as searching a collection of data can proceed exponentially faster if the collec-tion is structured and indexed intelligently. People can easily perform arithmeticon Arabic numerals but ﬁnd arithmetic on Roman numerals much more timeconsuming. It is not surprising that the choice of representation has an enormouseﬀect on the performance of machine learning algorithms. For a simple visualexample, see ﬁgure 1.1.Many artiﬁcial intelligence tasks can be solved by designing the right set offeatures to extract for that task, then providing these features to a simple machinelearning algorithm. For example, a useful feature for speaker identiﬁcation fromsound is an estimate of the size of the speaker’s vocal tract. This feature gives astrong clue as to whether the speaker is a man, woman, or child.For many tasks, however, it is diﬃcult to know what features should beextracted. For example, suppose that we would like to write a program to detectcars in photographs. We know that cars have wheels, so we might like to use thepresence of a wheel as a feature. Unfortunately, it is diﬃcult to describe exactlywhat a wheel looks like in terms of pixel values. A wheel has a simple geometricshape, but its image may be complicated by shadows falling on the wheel, the sunglaring oﬀ the metal parts of the wheel, the fender of the car or an object in the3
CHAPTER 1. INTRODUCTIONFigure 1.1: Example of diﬀerent representations: suppose we want to separate twocategories of data by drawing a line between them in a scatterplot. In the plot on the left,we represent some data using Cartesian coordinates, and the task is impossible. In the ploton the right, we represent the data with polar coordinates and the task becomes simple tosolve with a vertical line. (Figure produced in collaboration with David Warde-Farley.)foreground obscuring part of the wheel, and so on.One solution to this problem is to use machine learning to discover not onlythe mapping from representation to output but also the representation itself.This approach is known asrepresentation learning. Learned representationsoften result in much better performance than can be obtained with hand-designedrepresentations. They also enable AI systems to rapidly adapt to new tasks, withminimal human intervention. A representation learning algorithm can discover agood set of features for a simple task in minutes, or for a complex task in hours tomonths. Manually designing features for a complex task requires a great deal ofhuman time and eﬀort; it can take decades for an entire community of researchers.The quintessential example of a representation learning algorithm is theau-toencoder. An autoencoder is the combination of anencoderfunction, whichconverts the input data into a diﬀerent representation, and adecoderfunction,which converts the new representation back into the original format. Autoencodersare trained to preserve as much information as possible when an input is runthrough the encoder and then the decoder, but they are also trained to make thenew representation have various nice properties. Diﬀerent kinds of autoencodersaim to achieve diﬀerent kinds of properties.When designing features or algorithms for learning features, our goal is usuallyto separate thefactors of variationthat explain the observed data. In this4


Part IApplied Math and MachineLearning Basics27
This part of the book introduces the basic mathematical concepts needed tounderstand deep learning. We begin with general ideas from applied math thatenable us to deﬁne functions of many variables, ﬁnd the highest and lowest pointson these functions, and quantify degrees of belief.Next, we describe the fundamental goals of machine learning. We describe howto accomplish these goals by specifying a model that represents certain beliefs,designing a cost function that measures how well those beliefs correspond withreality, and using a training algorithm to minimize that cost function.This elementary framework is the basis for a broad variety of machine learningalgorithms, including approaches to machine learning that are not deep. In thesubsequent parts of the book, we develop deep learning algorithms within thisframework.28


Chapter 2Linear AlgebraLinear algebra is a branch of mathematics that is widely used throughout scienceand engineering. Yet because linear algebra is a form of continuous rather thandiscrete mathematics, many computer scientists have little experience with it. Agood understanding of linear algebra is essential for understanding and workingwith many machine learning algorithms, especially deep learning algorithms. Wetherefore precede our introduction to deep learning with a focused presentation ofthe key linear algebra prerequisites.If you are already familiar with linear algebra, feel free to skip this chapter. Ifyou have previous experience with these concepts but need a detailed referencesheet to review key formulas, we recommend The Matrix Cookbook (Petersen andPedersen, 2006). If you have had no exposure at all to linear algebra, this chapterwill teach you enough to read this book, but we highly recommend that you alsoconsult another resource focused exclusively on teaching linear algebra, such asShilov (1977). This chapter completely omits many important linear algebra topicsthat are not essential for understanding deep learning.2.1 Scalars, Vectors, Matrices and TensorsThe study of linear algebra involves several types of mathematical objects:• Scalars: A scalar is just a single number, in contrast to most of the otherobjects studied in linear algebra, which are usually arrays of multiple numbers.We write scalars in italics. We usually give scalars lowercase variable names.When we introduce them, we specify what kind of number they are. For29
CHAPTER 2. LINEAR ALGEBRAexample, we might say “Lets ∈ Rbe the slope of the line,” while deﬁning areal-valued scalar, or “Letn ∈ Nbe the number of units,” while deﬁning anatural number scalar.• Vectors: A vector is an array of numbers. The numbers are arranged inorder. We can identify each individual number by its index in that ordering.Typically we give vectors lowercase names in bold typeface, such asx. Theelements of the vector are identiﬁed by writing its name in italic typeface,with a subscript. The ﬁrst element ofxisx1, the second element isx2, andso on. We also need to say what kind of numbers are stored in the vector. Ifeach element is inR, and the vector hasnelements, then the vector lies inthe set formed by taking the Cartesian product ofR ntimes, denoted asRn.When we need to explicitly identify the elements of a vector, we write themas a column enclosed in square brackets:x =x1x2...xn. (2.1)We can think of vectors as identifying points in space, with each elementgiving the coordinate along a diﬀerent axis.Sometimes we need to index a set of elements of a vector. In this case, wedeﬁne a set containing the indices and write the set as a subscript. Forexample, to accessx1,x3andx6, we deﬁne the setS={1,3,6}and writexS. We use the−sign to index the complement of a set. For examplex−1isthe vector containing all elements ofxexcept forx1, andx−Sis the vectorcontaining all elements of x except for x1, x3and x6.• Matrices: A matrix is a 2-D array of numbers, so each element is identiﬁedby two indices instead of just one. We usually give matrices uppercasevariable names with bold typeface, such asA. If a real-valued matrixAhasa height ofmand a width ofn, then we say thatA ∈ Rm×n. We usuallyidentify the elements of a matrix using its name in italic but not bold font,and the indices are listed with separating commas. For example,A1,1is theupper left entry ofAandAm,nis the bottom right entry. We can identifyall the numbers with vertical coordinateiby writing a “:” for the horizontalcoordinate. For example,Ai,:denotes the horizontal cross section ofAwithvertical coordinatei. This is known as thei-throwofA. Likewise,A:,iis30
CHAPTER 2. LINEAR ALGEBRAthe i-th column of A. When we need to explicitly identify the elements ofa matrix, we write them as an array enclosed in square brackets:A1,1A1,2A2,1A2,2. (2.2)Sometimes we may need to index matrix-valued expressions that are not justa single letter. In this case, we use subscripts after the expression but do notconvert anything to lowercase. For example,f(A)i,jgives element (i, j) ofthe matrix computed by applying the function f to A.• Tensors: In some cases we will need an array with more than two axes.In the general case, an array of numbers arranged on a regular grid with avariable number of axes is known as a tensor. We denote a tensor named “A”with this typeface:A. We identify the element ofAat coordinates (i, j, k)by writing Ai,j,k.One important operation on matrices is thetranspose. The transpose of amatrix is the mirror image of the matrix across a diagonal line, called themaindiagonal, running down and to the right, starting from its upper left corner. Seeﬁgure 2.1 for a graphical depiction of this operation. We denote the transpose of amatrix A as A, and it is deﬁned such that(A)i,j= Aj,i. (2.3)Vectors can be thought of as matrices that contain only one column. Thetranspose of a vector is therefore a matrix with only one row. Sometimes weA =A1,1A1,2A2,1A2,2A3,1A3,2⇒ A=A1,1A2,1A3,1A1,2A2,2A3,2Figure 2.1: The transpose of the matrix can be thought of as a mirror image across themain diagonal.31
CHAPTER 2. LINEAR ALGEBRAdeﬁne a vector by writing out its elements in the text inline as a row matrix, thenusing the transpose operator to turn it into a standard column vector, for examplex = [x1, x2, x3].A scalar can be thought of as a matrix with only a single entry. From this, wecan see that a scalar is its own transpose: a = a.We can add matrices to each other, as long as they have the same shape, justby adding their corresponding elements: C = A + B where Ci,j= Ai,j+ Bi,j.We can also add a scalar to a matrix or multiply a matrix by a scalar, justby performing that operation on each element of a matrix:D=a · B+cwhereDi,j= a · Bi,j+ c.In the context of deep learning, we also use some less conventional notation. Weallow the addition of a matrix and a vector, yielding another matrix:C=A+b,whereCi,j=Ai,j+bj. In other words, the vectorbis added to each row of thematrix. This shorthand eliminates the need to deﬁne a matrix withbcopied intoeach row before doing the addition. This implicit copying ofbto many locationsis called broadcasting.2.2 Multiplying Matrices and VectorsOne of the most important operations involving matrices is multiplication of twomatrices. Thematrix productof matricesAandBis a third matrixC. Inorder for this product to be deﬁned,Amust have the same number of columns asBhas rows. IfAis of shapem × nandBis of shapen × p, thenCis of shapem × p. We can write the matrix product just by placing two or more matricestogether, for example,C = AB. (2.4)The product operation is deﬁned byCi,j=kAi,kBk,j. (2.5)Note that the standard product of two matrices is not just a matrix containingthe product of the individual elements. Such an operation exists and is called theelement-wise product, or Hadamard product, and is denoted as A B.Thedot productbetween two vectorsxandyof the same dimensionalityis the matrix productxy. We can think of the matrix productC=ABascomputing Ci,jas the dot product between row i of A and column j of B.32


Chapter 3Probability and InformationTheoryIn this chapter, we describe probability theory and information theory.Probability theory is a mathematical framework for representing uncertainstatements. It provides a means of quantifying uncertainty as well as axioms forderiving new uncertain statements. In artiﬁcial intelligence applications, we useprobability theory in two major ways. First, the laws of probability tell us how AIsystems should reason, so we design our algorithms to compute or approximatevarious expressions derived using probability theory. Second, we can use probabilityand statistics to theoretically analyze the behavior of proposed AI systems.Probability theory is a fundamental tool of many disciplines of science andengineering. We provide this chapter to ensure that readers whose background isprimarily in software engineering, with limited exposure to probability theory, canunderstand the material in this book.While probability theory allows us to make uncertain statements and to reasonin the presence of uncertainty, information theory enables us to quantify the amountof uncertainty in a probability distribution.If you are already familiar with probability theory and information theory,you may wish to skip this chapter except for section 3.14, which describes thegraphs we use to describe structured probabilistic models for machine learning. Ifyou have absolutely no prior experience with these subjects, this chapter shouldbe suﬃcient to successfully carry out deep learning research projects, but we dosuggest that you consult an additional resource, such as Jaynes (2003).51
CHAPTER 3. PROBABILITY AND INFORMATION THEORY3.1 Why Probability?Many branches of computer science deal mostly with entities that are entirelydeterministic and certain. A programmer can usually safely assume that a CPU willexecute each machine instruction ﬂawlessly. Errors in hardware do occur but arerare enough that most software applications do not need to be designed to accountfor them. Given that many computer scientists and software engineers work in arelatively clean and certain environment, it can be surprising that machine learningmakes heavy use of probability theory.Machine learning must always deal with uncertain quantities and sometimesstochastic (nondeterministic) quantities. Uncertainty and stochasticity can arisefrom many sources. Researchers have made compelling arguments for quantifyinguncertainty using probability since at least the 1980s. Many of the argumentspresented here are summarized from or inspired by Pearl (1988).Nearly all activities require some ability to reason in the presence of uncertainty.In fact, beyond mathematical statements that are true by deﬁnition, it is diﬃcultto think of any proposition that is absolutely true or any event that is absolutelyguaranteed to occur.There are three possible sources of uncertainty:1.Inherent stochasticity in the system being modeled. For example, mostinterpretations of quantum mechanics describe the dynamics of subatomicparticles as being probabilistic. We can also create theoretical scenarios thatwe postulate to have random dynamics, such as a hypothetical card gamewhere we assume that the cards are truly shuﬄed into a random order.2.Incomplete observability. Even deterministic systems can appear stochasticwhen we cannot observe all the variables that drive the behavior of thesystem. For example, in the Monty Hall problem, a game show contestant isasked to choose between three doors and wins a prize held behind the chosendoor. Two doors lead to a goat while a third leads to a car. The outcomegiven the contestant’s choice is deterministic, but from the contestant’s pointof view, the outcome is uncertain.3.Incomplete modeling. When we use a model that must discard some ofthe information we have observed, the discarded information results inuncertainty in the model’s predictions. For example, suppose we build arobot that can exactly observe the location of every object around it. If therobot discretizes space when predicting the future location of these objects,52
CHAPTER 3. PROBABILITY AND INFORMATION THEORYthen the discretization makes the robot immediately become uncertain aboutthe precise position of objects: each object could be anywhere within thediscrete cell that it was observed to occupy.In many cases, it is more practical to use a simple but uncertain rule ratherthan a complex but certain one, even if the true rule is deterministic and ourmodeling system has the ﬁdelity to accommodate a complex rule. For example, thesimple rule “Most birds ﬂy” is cheap to develop and is broadly useful, while a ruleof the form, “Birds ﬂy, except for very young birds that have not yet learned toﬂy, sick or injured birds that have lost the ability to ﬂy, ﬂightless species of birdsincluding the cassowary, ostrich and kiwi. . .” is expensive to develop, maintainand communicate and, after all this eﬀort, is still brittle and prone to failure.While it should be clear that we need a means of representing and reasoningabout uncertainty, it is not immediately obvious that probability theory can provideall the tools we want for artiﬁcial intelligence applications. Probability theorywas originally developed to analyze the frequencies of events. It is easy to seehow probability theory can be used to study events like drawing a certain hand ofcards in a poker game. These kinds of events are often repeatable. When we saythat an outcome has a probabilitypof occurring, it means that if we repeated theexperiment (e.g., drawing a hand of cards) inﬁnitely many times, then a proportionpof the repetitions would result in that outcome. This kind of reasoning does notseem immediately applicable to propositions that are not repeatable. If a doctoranalyzes a patient and says that the patient has a 40 percent chance of havingthe ﬂu, this means something very diﬀerent—we cannot make inﬁnitely manyreplicas of the patient, nor is there any reason to believe that diﬀerent replicas ofthe patient would present with the same symptoms yet have varying underlyingconditions. In the case of the doctor diagnosing the patient, we use probabilityto represent adegree of belief, with 1 indicating absolute certainty that thepatient has the ﬂu and 0 indicating absolute certainty that the patient does nothave the ﬂu. The former kind of probability, related directly to the rates at whichevents occur, is known asfrequentist probability, while the latter, related toqualitative levels of certainty, is known as Bayesian probability.If we list several properties that we expect common sense reasoning aboutuncertainty to have, then the only way to satisfy those properties is to treatBayesian probabilities as behaving exactly the same as frequentist probabilities.For example, if we want to compute the probability that a player will win a pokergame given that she has a certain set of cards, we use exactly the same formulasas when we compute the probability that a patient has a disease given that shehas certain symptoms. For more details about why a small set of common sense53
CHAPTER 3. PROBABILITY AND INFORMATION THEORYassumptions implies that the same axioms must control both kinds of probability,see Ramsey (1926).Probability can be seen as the extension of logic to deal with uncertainty. Logicprovides a set of formal rules for determining what propositions are implied tobe true or false given the assumption that some other set of propositions is trueor false. Probability theory provides a set of formal rules for determining thelikelihood of a proposition being true given the likelihood of other propositions.3.2 Random VariablesArandom variableis a variable that can take on diﬀerent values randomly. Wetypically denote the random variable itself with a lowercase letter in plain typeface,and the values it can take on with lowercase script letters. For example,x1andx2are both possible values that the random variablexcan take on. For vector-valuedvariables, we would write the random variable asxand one of its values asx. Onits own, a random variable is just a description of the states that are possible; itmust be coupled with a probability distribution that speciﬁes how likely each ofthese states are.Random variables may be discrete or continuous. A discrete random variableis one that has a ﬁnite or countably inﬁnite number of states. Note that thesestates are not necessarily the integers; they can also just be named states thatare not considered to have any numerical value. A continuous random variable isassociated with a real value.3.3 Probability DistributionsAprobability distributionis a description of how likely a random variable orset of random variables is to take on each of its possible states. The way wedescribe probability distributions depends on whether the variables are discrete orcontinuous.3.3.1 Discrete Variables and Probability Mass FunctionsA probability distribution over discrete variables may be described using aproba-bility mass function(PMF). We typically denote probability mass functions witha capitalP. Often we associate each random variable with a diﬀerent probabilitymass function and the reader must infer which PMF to use based on the identity54


Chapter 4Numerical ComputationMachine learning algorithms usually require a high amount of numerical compu-tation. This typically refers to algorithms that solve mathematical problems bymethods that update estimates of the solution via an iterative process, ratherthan analytically deriving a formula to provide a symbolic expression for thecorrect solution. Common operations include optimization (ﬁnding the value of anargument that minimizes or maximizes a function) and solving systems of linearequations. Even just evaluating a mathematical function on a digital computer canbe diﬃcult when the function involves real numbers, which cannot be representedprecisely using a ﬁnite amount of memory.4.1 Overﬂow and UnderﬂowThe fundamental diﬃculty in performing continuous math on a digital computeris that we need to represent inﬁnitely many real numbers with a ﬁnite numberof bit patterns. This means that for almost all real numbers, we incur someapproximation error when we represent the number in the computer. In manycases, this is just rounding error. Rounding error is problematic, especially whenit compounds across many operations, and can cause algorithms that work intheory to fail in practice if they are not designed to minimize the accumulation ofrounding error.One form of rounding error that is particularly devastating isunderﬂow.Underﬂow occurs when numbers near zero are rounded to zero. Many functionsbehave qualitatively diﬀerently when their argument is zero rather than a smallpositive number. For example, we usually want to avoid division by zero (some78
CHAPTER 4. NUMERICAL COMPUTATIONsoftware environments will raise exceptions when this occurs, others will return aresult with a placeholder not-a-number value) or taking the logarithm of zero (thisis usually treated as−∞, which then becomes not-a-number if it is used for manyfurther arithmetic operations).Another highly damaging form of numerical error isoverﬂow. Overﬂow occurswhen numbers with large magnitude are approximated as∞or−∞. Furtherarithmetic will usually change these inﬁnite values into not-a-number values.One example of a function that must be stabilized against underﬂow andoverﬂow is thesoftmax function. The softmax function is often used to predictthe probabilities associated with a multinoulli distribution. The softmax functionis deﬁned to besoftmax(x)i=exp(xi)nj=1exp(xj). (4.1)Consider what happens when all thexiare equal to some constantc. Analytically,we can see that all the outputs should be equal to1n. Numerically, this maynot occur whenchas large magnitude. Ifcis very negative, thenexp(c) willunderﬂow. This means the denominator of the softmax will become 0, so the ﬁnalresult is undeﬁned. Whencis very large and positive,exp(c) will overﬂow, againresulting in the expression as a whole being undeﬁned. Both of these diﬃcultiescan be resolved by instead evaluatingsoftmax(z) wherez=x − maxixi. Simplealgebra shows that the value of the softmax function is not changed analytically byadding or subtracting a scalar from the input vector. Subtractingmaxixiresultsin the largest argument toexpbeing 0, which rules out the possibility of overﬂow.Likewise, at least one term in the denominator has a value of 1, which rules outthe possibility of underﬂow in the denominator leading to a division by zero.There is still one small problem. Underﬂow in the numerator can still causethe expression as a whole to evaluate to zero. This means that if we implementlog softmax(x) by ﬁrst running the softmax subroutine then passing the result tothe log function, we could erroneously obtain−∞. Instead, we must implementa separate function that calculateslog softmaxin a numerically stable way. Thelog softmaxfunction can be stabilized using the same trick as we used to stabilizethe softmax function.For the most part, we do not explicitly detail all the numerical considerationsinvolved in implementing the various algorithms described in this book. Developersof low-level libraries should keep numerical issues in mind when implementingdeep learning algorithms. Most readers of this book can simply rely on low-level libraries that provide stable implementations. In some cases, it is possibleto implement a new algorithm and have the new implementation automatically79
CHAPTER 4. NUMERICAL COMPUTATIONstabilized. Theano (Bergstra et al., 2010; Bastien et al., 2012) is an exampleof a software package that automatically detects and stabilizes many commonnumerically unstable expressions that arise in the context of deep learning.4.2 Poor ConditioningConditioning refers to how rapidly a function changes with respect to small changesin its inputs. Functions that change rapidly when their inputs are perturbed slightlycan be problematic for scientiﬁc computation because rounding errors in the inputscan result in large changes in the output.Consider the functionf(x) =A−1x. WhenA ∈ Rn×nhas an eigenvaluedecomposition, its condition number ismaxi,jλiλj. (4.2)This is the ratio of the magnitude of the largest and smallest eigenvalue. Whenthis number is large, matrix inversion is particularly sensitive to error in the input.This sensitivity is an intrinsic property of the matrix itself, not the resultof rounding error during matrix inversion. Poorly conditioned matrices amplifypre-existing errors when we multiply by the true matrix inverse. In practice, theerror will be compounded further by numerical errors in the inversion process itself.4.3 Gradient-Based OptimizationMost deep learning algorithms involve optimization of some sort. Optimizationrefers to the task of either minimizing or maximizing some functionf(x) by alteringx. We usually phrase most optimization problems in terms of minimizingf(x).Maximization may be accomplished via a minimization algorithm by minimizing−f(x).The function we want to minimize or maximize is called the objective func-tion, orcriterion. When we are minimizing it, we may also call it thecostfunction,loss function, orerror function. In this book, we use these termsinterchangeably, though some machine learning publications assign special meaningto some of these terms.We often denote the value that minimizes or maximizes a function with asuperscript ∗. For example, we might say x∗= arg min f(x).80
CHAPTER 4. NUMERICAL COMPUTATION−2.0 −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5 2.0x−2.0−1.5−1.0−0.50.00.51.01.52.0Global minimum at x = 0.Since f(x) = 0, gradientdescent halts here.For x < 0, we have f(x) < 0,so we can decrease f bymoving rightward.For x > 0, we have f(x) > 0,so we can decrease f bymoving leftward.f(x) =12x2f(x) = xFigure 4.1: Gradient descent. An illustration of how the gradient descent algorithm usesthe derivatives of a function to follow the function downhill to a minimum.We assume the reader is already familiar with calculus but provide a briefreview of how calculus concepts relate to optimization here.Suppose we have a functiony=f(x), where bothxandyare real numbers.Thederivativeof this function is denoted asf(x) or asdydx. The derivativef(x)gives the slope off(x) at the pointx. In other words, it speciﬁes how to scalea small change in the input to obtain the corresponding change in the output:f(x + ) ≈ f(x) + f(x).The derivative is therefore useful for minimizing a function because it tells ushow to changexin order to make a small improvement iny. For example, weknow thatf(x −  sign(f(x))) is less thanf(x) for small enough. We can thusreducef(x) by movingxin small steps with the opposite sign of the derivative.This technique is calledgradient descent(Cauchy, 1847). See ﬁgure 4.1 for anexample of this technique.Whenf(x) = 0, the derivative provides no information about which directionto move. Points wheref(x) = 0 are known ascritical points, orstationarypoints. Alocal minimumis a point wheref(x) is lower than at all neighboringpoints, so it is no longer possible to decreasef(x) by making inﬁnitesimal steps.Alocal maximumis a point wheref(x) is higher than at all neighboring points,81



Chapter 5Machine Learning BasicsDeep learning is a speciﬁc kind of machine learning. To understand deep learningwell, one must have a solid understanding of the basic principles of machine learning.This chapter provides a brief course in the most important general principles thatare applied throughout the rest of the book. Novice readers or those who want awider perspective are encouraged to consider machine learning textbooks with amore comprehensive coverage of the fundamentals, such as Murphy (2012) or Bishop(2006). If you are already familiar with machine learning basics, feel free to skipahead to section 5.11. That section covers some perspectives on traditional machinelearning techniques that have strongly inﬂuenced the development of deep learningalgorithms.We begin with a deﬁnition of what a learning algorithm is and present anexample: the linear regression algorithm. We then proceed to describe how thechallenge of ﬁtting the training data diﬀers from the challenge of ﬁnding patternsthat generalize to new data. Most machine learning algorithms have settingscalled hyperparameters, which must be determined outside the learning algorithmitself; we discuss how to set these using additional data. Machine learning isessentially a form of applied statistics with increased emphasis on the use ofcomputers to statistically estimate complicated functions and a decreased emphasison proving conﬁdence intervals around these functions; we therefore present thetwo central approaches to statistics: frequentist estimators and Bayesian inference.Most machine learning algorithms can be divided into the categories of supervisedlearning and unsupervised learning; we describe these categories and give someexamples of simple learning algorithms from each category. Most deep learningalgorithms are based on an optimization algorithm called stochastic gradient96
CHAPTER 5. MACHINE LEARNING BASICSdescent. We describe how to combine various algorithm components, such asan optimization algorithm, a cost function, a model, and a dataset, to build amachine learning algorithm. Finally, in section 5.11, we describe some of thefactors that have limited the ability of traditional machine learning to generalize.These challenges have motivated the development of deep learning algorithms thatovercome these obstacles.5.1 Learning AlgorithmsA machine learning algorithm is an algorithm that is able to learn from data.But what do we mean by learning? Mitchell (1997) provides a succinct deﬁnition:“A computer program is said to learn from experienceEwith respect to someclass of tasksTand performance measureP, if its performance at tasks inT, asmeasured byP, improves with experienceE.” One can imagine a wide variety ofexperiencesE, tasksT, and performance measuresP, and we do not attempt inthis book to formally deﬁne what may be used for each of these entities. Instead,in the following sections, we provide intuitive descriptions and examples of thediﬀerent kinds of tasks, performance measures, and experiences that can be usedto construct machine learning algorithms.5.1.1 The Task, TMachine learning enables us to tackle tasks that are too diﬃcult to solve withﬁxed programs written and designed by human beings. From a scientiﬁc andphilosophical point of view, machine learning is interesting because developing ourunderstanding of it entails developing our understanding of the principles thatunderlie intelligence.In this relatively formal deﬁnition of the word “task,” the process of learningitself is not the task. Learning is our means of attaining the ability to perform thetask. For example, if we want a robot to be able to walk, then walking is the task.We could program the robot to learn to walk, or we could attempt to directly writea program that speciﬁes how to walk manually.Machine learning tasks are usually described in terms of how the machinelearning system should process anexample. An example is a collection offeaturesthat have been quantitatively measured from some object or event that we wantthe machine learning system to process. We typically represent an example as avectorx ∈ Rnwhere each entryxiof the vector is another feature. For example,the features of an image are usually the values of the pixels in the image.97
CHAPTER 5. MACHINE LEARNING BASICSMany kinds of tasks can be solved with machine learning. Some of the mostcommon machine learning tasks include the following:• Classiﬁcation: In this type of task, the computer program is asked to specifywhich ofkcategories some input belongs to. To solve this task, the learningalgorithm is usually asked to produce a functionf:Rn→ {1, . . . , k}. Wheny=f(x), the model assigns an input described by vectorxto a categoryidentiﬁed by numeric codey. There are other variants of the classiﬁcationtask, for example, wherefoutputs a probability distribution over classes.An example of a classiﬁcation task is object recognition, where the inputis an image (usually described as a set of pixel brightness values), and theoutput is a numeric code identifying the object in the image. For example,the Willow Garage PR2 robot is able to act as a waiter that can recognizediﬀerent kinds of drinks and deliver them to people on command (Good-fellow et al., 2010). Modern object recognition is best accomplished withdeep learning (Krizhevsky et al., 2012; Ioﬀe and Szegedy, 2015). Objectrecognition is the same basic technology that enables computers to recognizefaces (Taigman et al., 2014), which can be used to automatically tag peoplein photo collections and for computers to interact more naturally with theirusers.• Classiﬁcation with missing inputs: Classiﬁcation becomes more chal-lenging if the computer program is not guaranteed that every measurement inits input vector will always be provided. To solve the classiﬁcation task, thelearning algorithm only has to deﬁne a single function mapping from a vectorinput to a categorical output. When some of the inputs may be missing,rather than providing a single classiﬁcation function, the learning algorithmmust learn a set of functions. Each function corresponds to classifyingxwitha diﬀerent subset of its inputs missing. This kind of situation arises frequentlyin medical diagnosis, because many kinds of medical tests are expensive orinvasive. One way to eﬃciently deﬁne such a large set of functions is tolearn a probability distribution over all the relevant variables, then solve theclassiﬁcation task by marginalizing out the missing variables. Withninputvariables, we can now obtain all 2ndiﬀerent classiﬁcation functions neededfor each possible set of missing inputs, but the computer program needsto learn only a single function describing the joint probability distribution.See Goodfellow et al. (2013b) for an example of a deep probabilistic modelapplied to such a task in this way. Many of the other tasks described in thissection can also be generalized to work with missing inputs; classiﬁcationwith missing inputs is just one example of what machine learning can do.98
CHAPTER 5. MACHINE LEARNING BASICS• Regression: In this type of task, the computer program is asked to predict anumerical value given some input. To solve this task, the learning algorithmis asked to output a functionf:Rn→ R. This type of task is similar toclassiﬁcation, except that the format of output is diﬀerent. An example ofa regression task is the prediction of the expected claim amount that aninsured person will make (used to set insurance premiums), or the predictionof future prices of securities. These kinds of predictions are also used foralgorithmic trading.• Transcription: In this type of task, the machine learning system is askedto observe a relatively unstructured representation of some kind of dataand transcribe the information into discrete textual form. For example, inoptical character recognition, the computer program is shown a photographcontaining an image of text and is asked to return this text in the form ofa sequence of characters (e.g., in ASCII or Unicode format). Google StreetView uses deep learning to process address numbers in this way (Goodfellowet al., 2014d). Another example is speech recognition, where the computerprogram is provided an audio waveform and emits a sequence of characters orword ID codes describing the words that were spoken in the audio recording.Deep learning is a crucial component of modern speech recognition systemsused at major companies, including Microsoft, IBM and Google (Hintonet al., 2012b).• Machine translation: In a machine translation task, the input alreadyconsists of a sequence of symbols in some language, and the computer programmust convert this into a sequence of symbols in another language. This iscommonly applied to natural languages, such as translating from English toFrench. Deep learning has recently begun to have an important impact onthis kind of task (Sutskever et al., 2014; Bahdanau et al., 2015).• Structured output: Structured output tasks involve any task where theoutput is a vector (or other data structure containing multiple values) withimportant relationships between the diﬀerent elements. This is a broadcategory and subsumes the transcription and translation tasks describedabove, as well as many other tasks. One example is parsing—mapping anatural language sentence into a tree that describes its grammatical structureby tagging nodes of the trees as being verbs, nouns, adverbs, and so on.See Collobert (2011) for an example of deep learning applied to a parsingtask. Another example is pixel-wise segmentation of images, where thecomputer program assigns every pixel in an image to a speciﬁc category.99


Part IIDeep Networks: ModernPractices162
This part of the book summarizes the state of modern deep learning as it isused to solve practical applications.Deep learning has a long history and many aspirations. Several proposedapproaches have yet to entirely bear fruit. Several ambitious goals have yet to berealized. These less-developed branches of deep learning appear in the ﬁnal part ofthe book.This part focuses only on those approaches that are essentially working tech-nologies that are already used heavily in industry.Modern deep learning provides a powerful framework for supervised learning.By adding more layers and more units within a layer, a deep network can representfunctions of increasing complexity. Most tasks that consist of mapping an inputvector to an output vector, and that are easy for a person to do rapidly, can beaccomplished via deep learning, given suﬃciently large models and suﬃcientlylarge datasets of labeled training examples. Other tasks, that cannot be describedas associating one vector to another, or that are diﬃcult enough that a personwould require time to think and reﬂect in order to accomplish the task, remainbeyond the scope of deep learning for now.This part of the book describes the core parametric function approximationtechnology that is behind nearly all modern practical applications of deep learning.We begin by describing the feedforward deep network model that is used torepresent these functions. Next, we present advanced techniques for regularizationand optimization of such models. Scaling these models to large inputs such as high-resolution images or long temporal sequences requires specialization. We introducethe convolutional network for scaling to large images and the recurrent neuralnetwork for processing temporal sequences. Finally, we present general guidelinesfor the practical methodology involved in designing, building, and conﬁguring anapplication involving deep learning and review some of its applications.These chapters are the most important for a practitioner—someone who wantsto begin implementing and using deep learning algorithms to solve real-worldproblems today.163


Chapter 6Deep Feedforward NetworksDeep feedforward networks, also calledfeedforward neural networks, ormultilayer perceptrons(MLPs), are the quintessential deep learning models.The goal of a feedforward network is to approximate some functionf∗. For example,for a classiﬁer,y=f∗(x) maps an inputxto a categoryy. A feedforward networkdeﬁnes a mappingy=f(x;θ) and learns the value of the parametersθthat resultin the best function approximation.These models are calledfeedforwardbecause information ﬂows through thefunction being evaluated fromx, through the intermediate computations used todeﬁnef, and ﬁnally to the outputy. There are nofeedbackconnections in whichoutputs of the model are fed back into itself. When feedforward neural networksare extended to include feedback connections, they are calledrecurrent neuralnetworks, as presented in chapter 10.Feedforward networks are of extreme importance to machine learning practi-tioners. They form the basis of many important commercial applications. Forexample, the convolutional networks used for object recognition from photos are aspecialized kind of feedforward network. Feedforward networks are a conceptualstepping stone on the path to recurrent networks, which power many naturallanguage applications.Feedforward neural networks are callednetworksbecause they are typicallyrepresented by composing together many diﬀerent functions. The model is asso-ciated with a directed acyclic graph describing how the functions are composedtogether. For example, we might have three functionsf(1),f(2), andf(3)connectedin a chain, to formf(x) =f(3)(f(2)(f(1)(x))). These chain structures are themost commonly used structures of neural networks. In this case,f(1)is calledtheﬁrst layerof the network,f(2)is called thesecond layer, and so on. The164
CHAPTER 6. DEEP FEEDFORWARD NETWORKSoverall length of the chain gives thedepthof the model. The name “deep learning”arose from this terminology. The ﬁnal layer of a feedforward network is called theoutput layer. During neural network training, we drivef(x) to matchf∗(x).The training data provides us with noisy, approximate examples off∗(x) evaluatedat diﬀerent training points. Each examplexis accompanied by a labely ≈ f∗(x).The training examples specify directly what the output layer must do at each pointx; it must produce a value that is close toy. The behavior of the other layers isnot directly speciﬁed by the training data. The learning algorithm must decidehow to use those layers to produce the desired output, but the training data donot say what each individual layer should do. Instead, the learning algorithm mustdecide how to use these layers to best implement an approximation off∗. Becausethe training data does not show the desired output for each of these layers, theyare called hidden layers.Finally, these networks are called neural because they are loosely inspired byneuroscience. Each hidden layer of the network is typically vector valued. Thedimensionality of these hidden layers determines thewidthof the model. Eachelement of the vector may be interpreted as playing a role analogous to a neuron.Rather than thinking of the layer as representing a single vector-to-vector function,we can also think of the layer as consisting of manyunitsthat act in parallel,each representing a vector-to-scalar function. Each unit resembles a neuron inthe sense that it receives input from many other units and computes its ownactivation value. The idea of using many layers of vector-valued representationsis drawn from neuroscience. The choice of the functionsf(i)(x) used to computethese representations is also loosely guided by neuroscientiﬁc observations aboutthe functions that biological neurons compute. Modern neural network research,however, is guided by many mathematical and engineering disciplines, and thegoal of neural networks is not to perfectly model the brain. It is best to think offeedforward networks as function approximation machines that are designed toachieve statistical generalization, occasionally drawing some insights from what weknow about the brain, rather than as models of brain function.One way to understand feedforward networks is to begin with linear modelsand consider how to overcome their limitations. Linear models, such as logisticregression and linear regression, are appealing because they can be ﬁt eﬃcientlyand reliably, either in closed form or with convex optimization. Linear models alsohave the obvious defect that the model capacity is limited to linear functions, sothe model cannot understand the interaction between any two input variables.To extend linear models to represent nonlinear functions ofx, we can applythe linear model not toxitself but to a transformed inputφ(x), whereφis a165
CHAPTER 6. DEEP FEEDFORWARD NETWORKSnonlinear transformation. Equivalently, we can apply the kernel trick described insection 5.7.2, to obtain a nonlinear learning algorithm based on implicitly applyingtheφmapping. We can think ofφas providing a set of features describingx, oras providing a new representation for x.The question is then how to choose the mapping φ.1.One option is to use a very genericφ, such as the inﬁnite-dimensionalφthatis implicitly used by kernel machines based on the RBF kernel. Ifφ(x) isof high enough dimension, we can always have enough capacity to ﬁt thetraining set, but generalization to the test set often remains poor. Verygeneric feature mappings are usually based only on the principle of localsmoothness and do not encode enough prior information to solve advancedproblems.2.Another option is to manually engineerφ. Until the advent of deep learning,this was the dominant approach. It requires decades of human eﬀort foreach separate task, with practitioners specializing in diﬀerent domains, suchas speech recognition or computer vision, and with little transfer betweendomains.3.The strategy of deep learning is to learnφ. In this approach, we have a modely=f(x;θ, w) =φ(x;θ)w. We now have parametersθthat we use to learnφfrom a broad class of functions, and parameterswthat map fromφ(x) tothe desired output. This is an example of a deep feedforward network, withφdeﬁning a hidden layer. This approach is the only one of the three thatgives up on the convexity of the training problem, but the beneﬁts outweighthe harms. In this approach, we parametrize the representation asφ(x;θ)and use the optimization algorithm to ﬁnd theθthat corresponds to a goodrepresentation. If we wish, this approach can capture the beneﬁt of the ﬁrstapproach by being highly generic—we do so by using a very broad familyφ(x;θ). Deep learning can also capture the beneﬁt of the second approach.Human practitioners can encode their knowledge to help generalization bydesigning familiesφ(x;θ) that they expect will perform well. The advantageis that the human designer only needs to ﬁnd the right general functionfamily rather than ﬁnding precisely the right function.This general principle of improving models by learning features extends beyondthe feedforward networks described in this chapter. It is a recurring theme ofdeep learning that applies to all the kinds of models described throughout thisbook. Feedforward networks are the application of this principle to learning166
CHAPTER 6. DEEP FEEDFORWARD NETWORKSdeterministic mappings fromxtoythat lack feedback connections. Other models,presented later, apply these principles to learning stochastic mappings, functionswith feedback, and probability distributions over a single vector.We begin this chapter with a simple example of a feedforward network. Next,we address each of the design decisions needed to deploy a feedforward network.First, training a feedforward network requires making many of the same designdecisions as are necessary for a linear model: choosing the optimizer, the costfunction, and the form of the output units. We review these basics of gradient-basedlearning, then proceed to confront some of the design decisions that are uniqueto feedforward networks. Feedforward networks have introduced the concept of ahidden layer, and this requires us to choose theactivation functionsthat willbe used to compute the hidden layer values. We must also design the architectureof the network, including how many layers the network should contain, how theselayers should be connected to each other, and how many units should be ineach layer. Learning in deep neural networks requires computing the gradientsof complicated functions. We present theback-propagationalgorithm and itsmodern generalizations, which can be used to eﬃciently compute these gradients.Finally, we close with some historical perspective.6.1 Example: Learning XORTo make the idea of a feedforward network more concrete, we begin with anexample of a fully functioning feedforward network on a very simple task: learningthe XOR function.The XOR function (“exclusive or”) is an operation on two binary values,x1andx2. When exactly one of these binary values is equal to 1, the XOR functionreturns 1. Otherwise, it returns 0. The XOR function provides the target functiony=f∗(x) that we want to learn. Our model provides a functiony=f(x;θ), andour learning algorithm will adapt the parametersθto makefas similar as possibleto f∗.In this simple example, we will not be concerned with statistical generalization.We want our network to perform correctly on the four pointsX={[0,0], [0,1],[1,0], and [1,1]}. We will train the network on all four of these points. Theonly challenge is to ﬁt the training set.We can treat this problem as a regression problem and use a mean squarederror loss function. We have chosen this loss function to simplify the math forthis example as much as possible. In practical applications, MSE is usually not an167


Chapter 7Regularization for Deep LearningA central problem in machine learning is how to make an algorithm that willperform well not just on the training data, but also on new inputs. Many strategiesused in machine learning are explicitly designed to reduce the test error, possiblyat the expense of increased training error. These strategies are known collectivelyas regularization. A great many forms of regularization are available to the deeplearning practitioner. In fact, developing more eﬀective regularization strategieshas been one of the major research eﬀorts in the ﬁeld.Chapter 5 introduced the basic concepts of generalization, underﬁtting, overﬁt-ting, bias, variance and regularization. If you are not already familiar with thesenotions, please refer to that chapter before continuing with this one.In this chapter, we describe regularization in more detail, focusing on regular-ization strategies for deep models or models that may be used as building blocksto form deep models.Some sections of this chapter deal with standard concepts in machine learning.If you are already familiar with these concepts, feel free to skip the relevantsections. However, most of this chapter is concerned with the extension of thesebasic concepts to the particular case of neural networks.In section 5.2.2, we deﬁned regularization as “any modiﬁcation we make to alearning algorithm that is intended to reduce its generalization error but not itstraining error.” There are many regularization strategies. Some put extra con-straints on a machine learning model, such as adding restrictions on theparametervalues. Some add extra terms in the objective function that can be thought of ascorresponding to a soft constraint on the parameter values. If chosen carefully,these extra constraints and penalties can lead to improved performance on the224
CHAPTER 7. REGULARIZATION FOR DEEP LEARNINGtest set. Sometimes these constraints and penalties are designed to encode speciﬁckinds of prior knowledge. Other times, these constraints and penalties are designedto express a generic preference for a simpler model class in order to promotegeneralization. Sometimes penalties and constraints are necessary to make anunderdetermined problem determined. Other forms of regularization, known asensemble methods, combine multiple hypotheses that explain the training data.In the context of deep learning, most regularization strategies are based onregularizing estimators. Regularization of an estimator works by trading increasedbias for reduced variance. An eﬀective regularizer is one that makes a proﬁtabletrade, reducing variance signiﬁcantly while not overly increasing the bias. When wediscussed generalization and overﬁtting in chapter 5, we focused on three situations,where the model family being trained either (1) excluded the true data-generatingprocess—corresponding to underﬁtting and inducing bias, or (2) matched the truedata-generating process, or (3) included the generating process but also manyother possible generating processes—the overﬁtting regime where variance ratherthan bias dominates the estimation error. The goal of regularization is to take amodel from the third regime into the second regime.In practice, an overly complex model family does not necessarily include thetarget function or the true data-generating process, or even a close approximationof either. We almost never have access to the true data-generating process sowe can never know for sure if the model family being estimated includes thegenerating process or not. Most applications of deep learning algorithms, however,are to domains where the true data-generating process is almost certainly outsidethe model family. Deep learning algorithms are typically applied to extremelycomplicated domains such as images, audio sequences and text, for which the truegeneration process essentially involves simulating the entire universe. To someextent, we are always trying to ﬁt a square peg (the data-generating process) intoa round hole (our model family).What this means is that controlling the complexity of the model is not asimple matter of ﬁnding the model of the right size, with the right number ofparameters. Instead, we might ﬁnd—and indeed in practical deep learning scenarios,we almost always do ﬁnd—that the best ﬁtting model (in the sense of minimizinggeneralization error) is a large model that has been regularized appropriately.We now review several strategies for how to create such a large, deep regularizedmodel.225
CHAPTER 7. REGULARIZATION FOR DEEP LEARNING7.1 Parameter Norm PenaltiesRegularization has been used for decades prior to the advent of deep learning. Linearmodels such as linear regression and logistic regression allow simple, straightforward,and eﬀective regularization strategies.Many regularization approaches are based on limiting the capacity of models,such as neural networks, linear regression, or logistic regression, by adding a pa-rameter norm penalty Ω(θ) to the objective functionJ. We denote the regularizedobjective function by˜J:˜J(θ; X, y) = J(θ; X, y) + αΩ(θ), (7.1)whereα ∈[0, ∞) is a hyperparameter that weights the relative contribution of thenorm penalty term, Ω, relative to the standard objective functionJ. Settingαto 0results in no regularization. Larger values ofαcorrespond to more regularization.When our training algorithm minimizes the regularized objective function˜Jitwill decrease both the original objectiveJon the training data and some measureof the size of the parametersθ(or some subset of the parameters). Diﬀerentchoices for the parameter norm Ω can result in diﬀerent solutions being preferred.In this section, we discuss the eﬀects of the various norms when used as penaltieson the model parameters.Before delving into the regularization behavior of diﬀerent norms, we notethat for neural networks, we typically choose to use a parameter norm penaltyΩ that penalizes only the weights of the aﬃne transformation at each layer andleaves the biases unregularized. The biases typically require less data than theweights to ﬁt accurately. Each weight speciﬁes how two variables interact. Fittingthe weight well requires observing both variables in a variety of conditions. Eachbias controls only a single variable. This means that we do not induce too muchvariance by leaving the biases unregularized. Also, regularizing the bias parameterscan introduce a signiﬁcant amount of underﬁtting. We therefore use the vectorwto indicate all the weights that should be aﬀected by a norm penalty, whilethe vectorθdenotes all the parameters, including bothwand the unregularizedparameters.In the context of neural networks, it is sometimes desirable to use a separatepenalty with a diﬀerentαcoeﬃcient for each layer of the network. Because it canbe expensive to search for the correct value of multiple hyperparameters, it is stillreasonable to use the same weight decay at all layers just to reduce the size ofsearch space.226
CHAPTER 7. REGULARIZATION FOR DEEP LEARNING7.1.1 L2Parameter RegularizationWe have already seen, in section 5.2.2, one of the simplest and most common kindsof parameter norm penalty: theL2parameter norm penalty commonly known asweight decay. This regularization strategy drives the weights closer to the origin1by adding a regularization term Ω(θ) =12w22to the objective function. In otheracademic communities,L2regularization is also known asridge regressionorTikhonov regularization.We can gain some insight into the behavior of weight decay regularizationby studying the gradient of the regularized objective function. To simplify thepresentation, we assume no bias parameter, soθis justw. Such a model has thefollowing total objective function:˜J(w; X, y) =α2ww + J(w; X, y), (7.2)with the corresponding parameter gradient∇w˜J(w; X, y) = αw + ∇wJ(w; X, y). (7.3)To take a single gradient step to update the weights, we perform this update:w ← w − (αw + ∇wJ(w; X, y)) . (7.4)Written another way, the update isw ← (1 − α)w −∇wJ(w; X, y). (7.5)We can see that the addition of the weight decay term has modiﬁed the learningrule to multiplicatively shrink the weight vector by a constant factor on each step,just before performing the usual gradient update. This describes what happens ina single step. But what happens over the entire course of training?We will further simplify the analysis by making a quadratic approximationto the objective function in the neighborhood of the value of the weights thatobtains minimal unregularized training cost,w∗=arg minwJ(w). If the objectivefunction is truly quadratic, as in the case of ﬁtting a linear regression model with1More generally, we could regularize the parameters to be near any speciﬁc point in spaceand, surprisingly, still get a regularization eﬀect, but better results will be obtained for a valuecloser to the true one, with zero being a default value that makes sense when we do not know ifthe correct value should be positive or negative. Since it is far more common to regularize themodel parameters toward zero, we will focus on this special case in our exposition.227


Chapter 8Optimization for Training DeepModelsDeep learning algorithms involve optimization in many contexts. For example,performing inference in models such as PCA involves solving an optimizationproblem. We often use analytical optimization to write proofs or design algorithms.Of all the many optimization problems involved in deep learning, the most diﬃcultis neural network training. It is quite common to invest days to months of time onhundreds of machines to solve even a single instance of the neural network trainingproblem. Because this problem is so important and so expensive, a specializedset of optimization techniques have been developed for solving it. This chapterpresents these optimization techniques for neural network training.If you are unfamiliar with the basic principles of gradient-based optimization,we suggest reviewing chapter 4. That chapter includes a brief overview of numericaloptimization in general.This chapter focuses on one particular case of optimization: ﬁnding the param-etersθof a neural network that signiﬁcantly reduce a cost functionJ(θ), whichtypically includes a performance measure evaluated on the entire training set aswell as additional regularization terms.We begin with a description of how optimization used as a training algorithmfor a machine learning task diﬀers from pure optimization. Next, we present severalof the concrete challenges that make optimization of neural networks diﬃcult. Wethen deﬁne several practical algorithms, including both optimization algorithmsthemselves and strategies for initializing the parameters. More advanced algorithmsadapt their learning rates during training or leverage information contained in271
CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELSthe second derivatives of the cost function. Finally, we conclude with a review ofseveral optimization strategies that are formed by combining simple optimizationalgorithms into higher-level procedures.8.1 How Learning Diﬀers from Pure OptimizationOptimization algorithms used for training of deep models diﬀer from traditionaloptimization algorithms in several ways. Machine learning usually acts indirectly.In most machine learning scenarios, we care about some performance measureP, that is deﬁned with respect to the test set and may also be intractable. Wetherefore optimizePonly indirectly. We reduce a diﬀerent cost functionJ(θ) inthe hope that doing so will improveP. This is in contrast to pure optimization,where minimizingJis a goal in and of itself. Optimization algorithms for trainingdeep models also typically include some specialization on the speciﬁc structure ofmachine learning objective functions.Typically, the cost function can be written as an average over the training set,such asJ(θ) = E(x,y)∼ˆpdataL(f(x; θ), y), (8.1)whereLis the per-example loss function,f(x;θ) is the predicted output when theinput isx, andˆpdatais the empirical distribution. In the supervised learning case,yis the target output. Throughout this chapter, we develop the unregularizedsupervised case, where the arguments toLaref(x;θ) andy. It is trivial to extendthis development, for example, to includeθorxas arguments, or to excludeyasarguments, to develop various forms of regularization or unsupervised learning.Equation 8.1 deﬁnes an objective function with respect to the training set. Wewould usually prefer to minimize the corresponding objective function where theexpectation is taken across the data-generating distribution pdatarather than justover the ﬁnite training set:J∗(θ) = E(x,y)∼pdataL(f(x; θ), y). (8.2)8.1.1 Empirical Risk MinimizationThe goal of a machine learning algorithm is to reduce the expected generalizationerror given by equation 8.2. This quantity is known as therisk. We emphasizehere that the expectation is taken over the true underlying distributionpdata. If weknew the true distributionpdata(x, y), risk minimization would be an optimization272
CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELStask solvable by an optimization algorithm. When we do not knowpdata(x, y) butonly have a training set of samples, however, we have a machine learning problem.The simplest way to convert a machine learning problem back into an op-timization problem is to minimize the expected loss on the training set. Thismeans replacing the true distributionp(x, y) with the empirical distributionˆp(x, y)deﬁned by the training set. We now minimize the empirical riskEx,y∼ˆpdata(x,y)[L(f(x; θ), y)] =1mmi=1L(f(x(i); θ), y(i)), (8.3)where m is the number of training examples.The training process based on minimizing this average training error is knownasempirical risk minimization. In this setting, machine learning is still verysimilar to straightforward optimization. Rather than optimizing the risk directly,we optimize the empirical risk and hope that the risk decreases signiﬁcantly aswell. A variety of theoretical results establish conditions under which the true riskcan be expected to decrease by various amounts.Nonetheless, empirical risk minimization is prone to overﬁtting. Models withhigh capacity can simply memorize the training set. In many cases, empiricalrisk minimization is not really feasible. The most eﬀective modern optimizationalgorithms are based on gradient descent, but many useful loss functions, suchas 0-1 loss, have no useful derivatives (the derivative is either zero or undeﬁnedeverywhere). These two problems mean that, in the context of deep learning, werarely use empirical risk minimization. Instead, we must use a slightly diﬀerentapproach, in which the quantity that we actually optimize is even more diﬀerentfrom the quantity that we truly want to optimize.8.1.2 Surrogate Loss Functions and Early StoppingSometimes, the loss function we actually care about (say, classiﬁcation error) is notone that can be optimized eﬃciently. For example, exactly minimizing expected 0-1loss is typically intractable (exponential in the input dimension), even for a linearclassiﬁer (Marcotte and Savard, 1992). In such situations, one typically optimizesasurrogate loss functioninstead, which acts as a proxy but has advantages.For example, the negative log-likelihood of the correct class is typically used as asurrogate for the 0-1 loss. The negative log-likelihood allows the model to estimatethe conditional probability of the classes, given the input, and if the model cando that well, then it can pick the classes that yield the least classiﬁcation error inexpectation.273
CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELSIn some cases, a surrogate loss function actually results in being able to learnmore. For example, the test set 0-1 loss often continues to decrease for a longtime after the training set 0-1 loss has reached zero, when training using thelog-likelihood surrogate. This is because even when the expected 0-1 loss is zero,one can improve the robustness of the classiﬁer by further pushing the classes apartfrom each other, obtaining a more conﬁdent and reliable classiﬁer, thus extractingmore information from the training data than would have been possible by simplyminimizing the average 0-1 loss on the training set.A very important diﬀerence between optimization in general and optimizationas we use it for training algorithms is that training algorithms do not usually haltat a local minimum. Instead, a machine learning algorithm usually minimizesa surrogate loss function but halts when a convergence criterion based on earlystopping (section 7.8) is satisﬁed. Typically the early stopping criterion is basedon the true underlying loss function, such as 0-1 loss measured on a validation set,and is designed to cause the algorithm to halt whenever overﬁtting begins to occur.Training often halts while the surrogate loss function still has large derivatives,which is very diﬀerent from the pure optimization setting, where an optimizationalgorithm is considered to have converged when the gradient becomes very small.8.1.3 Batch and Minibatch AlgorithmsOne aspect of machine learning algorithms that separates them from generaloptimization algorithms is that the objective function usually decomposes as a sumover the training examples. Optimization algorithms for machine learning typicallycompute each update to the parameters based on an expected value of the costfunction estimated using only a subset of the terms of the full cost function.For example, maximum likelihood estimation problems, when viewed in logspace, decompose into a sum over each example:θML= arg maxθmi=1log pmodel(x(i), y(i); θ). (8.4)Maximizing this sum is equivalent to maximizing the expectation over theempirical distribution deﬁned by the training set:J(θ) = Ex,y∼ˆpdatalog pmodel(x, y; θ). (8.5)Most of the properties of the objective functionJused by most of our opti-mization algorithms are also expectations over the training set. For example, the274


Chapter 9Convolutional NetworksConvolutional networks(LeCun, 1989), also known asconvolutional neuralnetworks, or CNNs, are a specialized kind of neural network for processing datathat has a known grid-like topology. Examples include time-series data, which canbe thought of as a 1-D grid taking samples at regular time intervals, and image data,which can be thought of as a 2-D grid of pixels. Convolutional networks have beentremendously successful in practical applications. The name “convolutional neuralnetwork” indicates that the network employs a mathematical operation calledconvolution. Convolution is a specialized kind of linear operation. Convolutionalnetworks are simply neural networks that use convolution in place of general matrixmultiplication in at least one of their layers.In this chapter, we ﬁrst describe what convolution is. Next, we explain themotivation behind using convolution in a neural network. We then describe anoperation calledpooling, which almost all convolutional networks employ. Usually,the operation used in a convolutional neural network does not correspond preciselyto the deﬁnition of convolution as used in other ﬁelds, such as engineering orpure mathematics. We describe several variants on the convolution function thatare widely used in practice for neural networks. We also show how convolutionmay be applied to many kinds of data, with diﬀerent numbers of dimensions. Wethen discuss means of making convolution more eﬃcient. Convolutional networksstand out as an example of neuroscientiﬁc principles inﬂuencing deep learning.We discuss these neuroscientiﬁc principles, then conclude with comments aboutthe role convolutional networks have played in the history of deep learning. Onetopic this chapter does not address is how to choose the architecture of yourconvolutional network. The goal of this chapter is to describe the kinds of toolsthat convolutional networks provide, while chapter 11 describes general guidelines326
CHAPTER 9. CONVOLUTIONAL NETWORKSfor choosing which tools to use in which circumstances. Research into convolutionalnetwork architectures proceeds so rapidly that a new best architecture for a givenbenchmark is announced every few weeks to months, rendering it impractical todescribe in print the best architecture. Nonetheless, the best architectures haveconsistently been composed of the building blocks described here.9.1 The Convolution OperationIn its most general form, convolution is an operation on two functions of a real-valued argument. To motivate the deﬁnition of convolution, we start with examplesof two functions we might use.Suppose we are tracking the location of a spaceship with a laser sensor. Ourlaser sensor provides a single outputx(t), the position of the spaceship at timet.Bothxandtare real valued, that is, we can get a diﬀerent reading from the lasersensor at any instant in time.Now suppose that our laser sensor is somewhat noisy. To obtain a less noisyestimate of the spaceship’s position, we would like to average several measurements.Of course, more recent measurements are more relevant, so we will want this tobe a weighted average that gives more weight to recent measurements. We cando this with a weighting functionw(a), whereais the age of a measurement. Ifwe apply such a weighted average operation at every moment, we obtain a newfunction s providing a smoothed estimate of the position of the spaceship:s(t) =x(a)w(t − a)da. (9.1)This operation is calledconvolution. The convolution operation is typicallydenoted with an asterisk:s(t) = (x ∗ w)(t). (9.2)In our example,wneeds to be a valid probability density function, or the outputwill not be a weighted average. Also,wneeds to be 0 for all negative arguments,or it will look into the future, which is presumably beyond our capabilities. Theselimitations are particular to our example, though. In general, convolution is deﬁnedfor any functions for which the above integral is deﬁned and may be used for otherpurposes besides taking weighted averages.In convolutional network terminology, the ﬁrst argument (in this example, thefunctionx) to the convolution is often referred to as theinput, and the second327
CHAPTER 9. CONVOLUTIONAL NETWORKSargument (in this example, the functionw) as thekernel. The output is sometimesreferred to as the feature map.In our example, the idea of a laser sensor that can provide measurements atevery instant is not realistic. Usually, when we work with data on a computer,time will be discretized, and our sensor will provide data at regular intervals.In our example, it might be more realistic to assume that our laser provides ameasurement once per second. The time indextcan then take on only integervalues. If we now assume thatxandware deﬁned only on integert, we can deﬁnethe discrete convolution:s(t) = (x ∗ w)(t) =∞a=−∞x(a)w(t − a). (9.3)In machine learning applications, the input is usually a multidimensional arrayof data, and the kernel is usually a multidimensional array of parameters that areadapted by the learning algorithm. We will refer to these multidimensional arraysas tensors. Because each element of the input and kernel must be explicitly storedseparately, we usually assume that these functions are zero everywhere but in theﬁnite set of points for which we store the values. This means that in practice, wecan implement the inﬁnite summation as a summation over a ﬁnite number ofarray elements.Finally, we often use convolutions over more than one axis at a time. Forexample, if we use a two-dimensional imageIas our input, we probably also wantto use a two-dimensional kernel K:S(i, j) = (I ∗ K)(i, j) =mnI(m, n)K(i − m, j − n). (9.4)Convolution is commutative, meaning we can equivalently writeS(i, j) = (K ∗I)(i, j) =mnI(i − m, j −n)K(m, n). (9.5)Usually the latter formula is more straightforward to implement in a machinelearning library, because there is less variation in the range of valid values ofmand n.The commutative property of convolution arises because we haveﬂippedthekernel relative to the input, in the sense that asmincreases, the index into theinput increases, but the index into the kernel decreases. The only reason to ﬂipthe kernel is to obtain the commutative property. While the commutative property328
CHAPTER 9. CONVOLUTIONAL NETWORKSis useful for writing proofs, it is not usually an important property of a neuralnetwork implementation. Instead, many neural network libraries implement arelated function called thecross-correlation, which is the same as convolutionbut without ﬂipping the kernel:S(i, j) = (K ∗I)(i, j) =mnI(i + m, j + n)K(m, n). (9.6)Many machine learning libraries implement cross-correlation but call it convolution.In this text we follow this convention of calling both operations convolution andspecify whether we mean to ﬂip the kernel or not in contexts where kernel ﬂippingis relevant. In the context of machine learning, the learning algorithm will learnthe appropriate values of the kernel in the appropriate place, so an algorithm basedon convolution with kernel ﬂipping will learn a kernel that is ﬂipped relative to thekernel learned by an algorithm without the ﬂipping. It is also rare for convolutionto be used alone in machine learning; instead convolution is used simultaneouslywith other functions, and the combination of these functions does not commuteregardless of whether the convolution operation ﬂips its kernel or not.See ﬁgure 9.1 for an example of convolution (without kernel ﬂipping) appliedto a 2-D tensor.Discrete convolution can be viewed as multiplication by a matrix, but thematrix has several entries constrained to be equal to other entries. For example,for univariate discrete convolution, each row of the matrix is constrained to beequal to the row above shifted by one element. This is known as aToeplitzmatrix. In two dimensions, adoubly block circulant matrixcorresponds toconvolution. In addition to these constraints that several elements be equal toeach other, convolution usually corresponds to a very sparse matrix (a matrixwhose entries are mostly equal to zero). This is because the kernel is usually muchsmaller than the input image. Any neural network algorithm that works withmatrix multiplication and does not depend on speciﬁc properties of the matrixstructure should work with convolution, without requiring any further changesto the neural network. Typical convolutional neural networks do make use offurther specializations in order to deal with large inputs eﬃciently, but these arenot strictly necessary from a theoretical perspective.9.2 MotivationConvolution leverages three important ideas that can help improve a machinelearning system:sparse interactions,parameter sharingandequivariant329


Chapter 10Sequence Modeling: Recurrentand Recursive NetsRecurrent neural networks, or RNNs (Rumelhart et al., 1986a), are a familyof neural networks for processing sequential data. Much as a convolutional networkis a neural network that is specialized for processing a grid of valuesXsuch asan image, a recurrent neural network is a neural network that is specialized forprocessing a sequence of valuesx(1), . . . , x(τ). Just as convolutional networkscan readily scale to images with large width and height, and some convolutionalnetworks can process images of variable size, recurrent networks can scale to muchlonger sequences than would be practical for networks without sequence-basedspecialization. Most recurrent networks can also process sequences of variablelength.To go from multilayer networks to recurrent networks, we need to take advantageof one of the early ideas found in machine learning and statistical models of the1980s: sharing parameters across diﬀerent parts of a model. Parameter sharingmakes it possible to extend and apply the model to examples of diﬀerent forms(diﬀerent lengths, here) and generalize across them. If we had separate parametersfor each value of the time index, we could not generalize to sequence lengths notseen during training, nor share statistical strength across diﬀerent sequence lengthsand across diﬀerent positions in time. Such sharing is particularly important whena speciﬁc piece of information can occur at multiple positions within the sequence.For example, consider the two sentences “I went to Nepal in 2009” and “In 2009,I went to Nepal.” If we ask a machine learning model to read each sentence andextract the year in which the narrator went to Nepal, we would like it to recognizethe year 2009 as the relevant piece of information, whether it appears in the sixth367
CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETSword or in the second word of the sentence. Suppose that we trained a feedforwardnetwork that processes sentences of ﬁxed length. A traditional fully connectedfeedforward network would have separate parameters for each input feature, soit would need to learn all the rules of the language separately at each position inthe sentence. By comparison, a recurrent neural network shares the same weightsacross several time steps.A related idea is the use of convolution across a 1-D temporal sequence. Thisconvolutional approach is the basis for time-delay neural networks (Lang andHinton, 1988; Waibel et al., 1989; Lang et al., 1990). The convolution operationallows a network to share parameters across time but is shallow. The output ofconvolution is a sequence where each member of the output is a function of asmall number of neighboring members of the input. The idea of parameter sharingmanifests in the application of the same convolution kernel at each time step.Recurrent networks share parameters in a diﬀerent way. Each member of theoutput is a function of the previous members of the output. Each member of theoutput is produced using the same update rule applied to the previous outputs.This recurrent formulation results in the sharing of parameters through a verydeep computational graph.For the simplicity of exposition, we refer to RNNs as operating on a sequencethat contains vectorsx(t)with the time step indextranging from 1 toτ. Inpractice, recurrent networks usually operate on minibatches of such sequences,with a diﬀerent sequence lengthτfor each member of the minibatch. We haveomitted the minibatch indices to simplify notation. Moreover, the time step indexneed not literally refer to the passage of time in the real world. Sometimes it refersonly to the position in the sequence. RNNs may also be applied in two dimensionsacross spatial data such as images, and even when applied to data involving time,the network may have connections that go backward in time, provided that theentire sequence is observed before it is provided to the network.This chapter extends the idea of a computational graph to include cycles. Thesecycles represent the inﬂuence of the present value of a variable on its own valueat a future time step. Such computational graphs allow us to deﬁne recurrentneural networks. We then describe many diﬀerent ways to construct, train, anduse recurrent neural networks.For more information on recurrent neural networks than is available in thischapter, we refer the reader to the textbook of Graves (2012).368
CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETSs(t−1)s(t−1)s(t)s(t)s(t+1)s(t+1)ffs(... )s(... )s(... )s(... )ffffffFigure 10.1: The classical dynamical system described by equation 10.1, illustrated as anunfolded computational graph. Each node represents the state at some timet, and thefunctionfmaps the state attto the state att+ 1. The same parameters (the same valueof θ used to parametrize f) are used for all time steps.10.1 Unfolding Computational GraphsA computational graph is a way to formalize the structure of a set of computations,such as those involved in mapping inputs and parameters to outputs and loss.Please refer to section 6.5.1 for a general introduction. In this section we explainthe idea ofunfoldinga recursive or recurrent computation into a computationalgraph that has a repetitive structure, typically corresponding to a chain of events.Unfolding this graph results in the sharing of parameters across a deep networkstructure.For example, consider the classical form of a dynamical system:s(t)= f(s(t−1); θ), (10.1)where s(t)is called the state of the system.Equation 10.1 is recurrent because the deﬁnition ofsat timetrefers back tothe same deﬁnition at time t − 1.For a ﬁnite number of time stepsτ, the graph can be unfolded by applyingthe deﬁnitionτ −1 times. For example, if we unfold equation 10.1 forτ= 3 timesteps, we obtains(3)=f(s(2); θ) (10.2)=f(f(s(1); θ); θ). (10.3)Unfolding the equation by repeatedly applying the deﬁnition in this way hasyielded an expression that does not involve recurrence. Such an expression cannow be represented by a traditional directed acyclic computational graph. Theunfolded computational graph of equation 10.1 and equation 10.3 is illustrated inﬁgure 10.1.As another example, let us consider a dynamical system driven by an external369
CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETSffhhxxh(t−1)h(t−1)h(t)h(t)h(t+1)h(t+1)x(t−1)x(t−1)x(t)x(t)x(t+1)x(t+1)h(... )h(... )h(... )h(... )ffUnfoldfffffFigure 10.2: A recurrent network with no outputs. This recurrent network just processesinformation from the inputxby incorporating it into the statehthat is passed forwardthrough time. (Left) Circuit diagram. The black square indicates a delay of a single timestep. (Right) The same network seen as an unfolded computational graph, where eachnode is now associated with one particular time instance.signal x(t),s(t)= f(s(t−1), x(t); θ), (10.4)where we see that the state now contains information about the whole past sequence.Recurrent neural networks can be built in many diﬀerent ways. Much asalmost any function can be considered a feedforward neural network, essentiallyany function involving recurrence can be considered a recurrent neural network.Many recurrent neural networks use equation 10.5 or a similar equation todeﬁne the values of their hidden units. To indicate that the state is the hiddenunits of the network, we now rewrite equation 10.4 using the variablehto representthe state,h(t)= f(h(t−1), x(t); θ), (10.5)illustrated in ﬁgure 10.2; typical RNNs will add extra architectural features suchas output layers that read information out of the state h to make predictions.When the recurrent network is trained to perform a task that requires predictingthe future from the past, the network typically learns to useh(t)as a kind of lossysummary of the task-relevant aspects of the past sequence of inputs up tot. Thissummary is in general necessarily lossy, since it maps an arbitrary length sequence(x(t), x(t−1), x(t−2), . . . , x(2), x(1)) to a ﬁxed length vectorh(t). Depending on thetraining criterion, this summary might selectively keep some aspects of the pastsequence with more precision than other aspects. For example, if the RNN isused in statistical language modeling, typically to predict the next word givenprevious words, storing all the information in the input sequence up to timetmay not be necessary; storing only enough information to predict the rest of thesentence is suﬃcient. The most demanding situation is when we askh(t)to be richenough to allow one to approximately recover the input sequence, as in autoencoder370


Chapter 11Practical MethodologySuccessfully applying deep learning techniques requires more than just a goodknowledge of what algorithms exist and the principles that explain how theywork. A good machine learning practitioner also needs to know how to choose analgorithm for a particular application and how to monitor and respond to feedbackobtained from experiments in order to improve a machine learning system. Duringday-to-day development of machine learning systems, practitioners need to decidewhether to gather more data, increase or decrease model capacity, add or removeregularizing features, improve the optimization of a model, improve approximateinference in a model, or debug the software implementation of the model. All theseoperations are at the very least time consuming to try out, so it is important tobe able to determine the right course of action rather than blindly guessing.Most of this book is about diﬀerent machine learning models, training algo-rithms, and objective functions. This may give the impression that the mostimportant ingredient to being a machine learning expert is knowing a wide varietyof machine learning techniques and being good at diﬀerent kinds of math. In prac-tice, one can usually do much better with a correct application of a commonplacealgorithm than by sloppily applying an obscure algorithm. Correct application ofan algorithm depends on mastering some fairly simple methodology. Many of therecommendations in this chapter are adapted from Ng (2015).We recommend the following practical design process:•Determine your goals—what error metric to use, and your target value forthis error metric. These goals and error metrics should be driven by theproblem that the application is intended to solve.•Establish a working end-to-end pipeline as soon as possible, including the416
CHAPTER 11. PRACTICAL METHODOLOGYestimation of the appropriate performance metrics.•Instrument the system well to determine bottlenecks in performance. Diag-nose which components are performing worse than expected and whetherpoor performance is due to overﬁtting, underﬁtting, or a defect in the dataor software.•Repeatedly make incremental changes such as gathering new data, adjustinghyperparameters, or changing algorithms, based on speciﬁc ﬁndings fromyour instrumentation.As a running example, we will use the Street View address number transcriptionsystem (Goodfellow et al., 2014d). The purpose of this application is to addbuildings to Google Maps. Street View cars photograph the buildings and recordthe GPS coordinates associated with each photograph. A convolutional networkrecognizes the address number in each photograph, allowing the Google Mapsdatabase to add that address in the correct location. The story of how thiscommercial application was developed gives an example of how to follow the designmethodology we advocate.We now describe each of the steps in this process.11.1 Performance MetricsDetermining your goals, in terms of which error metric to use, is a necessary ﬁrststep because your error metric will guide all your future actions. You should alsohave an idea of what level of performance you desire.Keep in mind that for most applications, it is impossible to achieve absolutezero error. The Bayes error deﬁnes the minimum error rate that you can hope toachieve, even if you have inﬁnite training data and can recover the true probabilitydistribution. This is because your input features may not contain completeinformation about the output variable, or because the system might be intrinsicallystochastic. You will also be limited by having a ﬁnite amount of training data.The amount of training data can be limited for a variety of reasons. When yourgoal is to build the best possible real-world product or service, you can typicallycollect more data but must determine the value of reducing error further and weighthis against the cost of collecting more data. Data collection can require time,money, or human suﬀering (for example, if your data collection process involvesperforming invasive medical tests). When your goal is to answer a scientiﬁc questionabout which algorithm performs better on a ﬁxed benchmark, the benchmark417
CHAPTER 11. PRACTICAL METHODOLOGYspeciﬁcation usually determines the training set, and you are not allowed to collectmore data.How can one determine a reasonable level of performance to expect? Typically,in the academic setting, we have some estimate of the error rate that is attainablebased on previously published benchmark results. In the real-word setting, wehave some idea of the error rate that is necessary for an application to be safe,cost-eﬀective, or appealing to consumers. Once you have determined your realisticdesired error rate, your design decisions will be guided by reaching this error rate.Another important consideration besides the target value of the performancemetric is the choice of which metric to use. Several diﬀerent performance metricsmay be used to measure the eﬀectiveness of a complete application that includesmachine learning components. These performance metrics are usually diﬀerentfrom the cost function used to train the model. As described in section 5.1.2, it iscommon to measure the accuracy, or equivalently, the error rate, of a system.However, many applications require more advanced metrics.Sometimes it is much more costly to make one kind of a mistake than another.For example, an e-mail spam detection system can make two kinds of mistakes:incorrectly classifying a legitimate message as spam, and incorrectly allowing aspam message to appear in the inbox. It is much worse to block a legitimatemessage than to allow a questionable message to pass through. Rather thanmeasuring the error rate of a spam classiﬁer, we may wish to measure some formof total cost, where the cost of blocking legitimate messages is higher than the costof allowing spam messages.Sometimes we wish to train a binary classiﬁer that is intended to detect somerare event. For example, we might design a medical test for a rare disease. Supposethat only one in every million people has this disease. We can easily achieve99.9999 percent accuracy on the detection task, by simply hard coding the classiﬁerto always report that the disease is absent. Clearly, accuracy is a poor way tocharacterize the performance of such a system. One way to solve this problem isto instead measureprecisionandrecall. Precision is the fraction of detectionsreported by the model that were correct, while recall is the fraction of true eventsthat were detected. A detector that says no one has the disease would achieveperfect precision, but zero recall. A detector that says everyone has the diseasewould achieve perfect recall, but precision equal to the percentage of people whohave the disease (0.0001 percent in our example of a disease that only one people ina million have). When using precision and recall, it is common to plot aPR curve,with precision on they-axis and recall on thex-axis. The classiﬁer generates a scorethat is higher if the event to be detected occurred. For example, a feedforward418
CHAPTER 11. PRACTICAL METHODOLOGYnetwork designed to detect a disease outputsˆy=P(y= 1| x), estimating theprobability that a person whose medical results are described by featuresxhasthe disease. We choose to report a detection whenever this score exceeds somethreshold. By varying the threshold, we can trade precision for recall. In manycases, we wish to summarize the performance of the classiﬁer with a single numberrather than a curve. To do so, we can convert precisionpand recallrinto anF-score given byF =2prp + r. (11.1)Another option is to report the total area lying beneath the PR curve.In some applications, it is possible for the machine learning system to refuse tomake a decision. This is useful when the machine learning algorithm can estimatehow conﬁdent it should be about a decision, especially if a wrong decision canbe harmful and if a human operator is able to occasionally take over. The StreetView transcription system provides an example of this situation. The task is totranscribe the address number from a photograph to associate the location wherethe photo was taken with the correct address in a map. Because the value of themap degrades considerably if the map is inaccurate, it is important to add anaddress only if the transcription is correct. If the machine learning system thinksthat it is less likely than a human being to obtain the correct transcription, then thebest course of action is to allow a human to transcribe the photo instead. Of course,the machine learning system is only useful if it is able to dramatically reduce theamount of photos that the human operators must process. A natural performancemetric to use in this situation iscoverage. Coverage is the fraction of examplesfor which the machine learning system is able to produce a response. It is possibleto trade coverage for accuracy. One can always obtain 100 percent accuracy byrefusing to process any example, but this reduces the coverage to 0 percent. For theStreet View task, the goal for the project was to reach human-level transcriptionaccuracy while maintaining 95 percent coverage. Human-level performance on thistask is 98 percent accuracy.Many other metrics are possible. We can, for example, measure click-throughrates, collect user satisfaction surveys, and so on. Many specialized applicationareas have application-speciﬁc criteria as well.What is important is to determine which performance metric to improve aheadof time, then concentrate on improving this metric. Without clearly deﬁned goals,it can be diﬃcult to tell whether changes to a machine learning system makeprogress or not.419


Chapter 12ApplicationsIn this chapter, we describe how to use deep learning to solve applications incomputer vision, speech recognition, natural language processing, and other areasof commercial interest. We begin by discussing the large-scale neural networkimplementations required for most serious AI applications. Next, we review severalspeciﬁc application areas that deep learning has been used to solve. While onegoal of deep learning is to design algorithms that are capable of solving a broadvariety of tasks, so far some degree of specialization is needed. For example, visiontasks require processing a large number of input features (pixels) per example.Language tasks require modeling a large number of possible values (words in thevocabulary) per input feature.12.1 Large-Scale Deep LearningDeep learning is based on the philosophy of connectionism: while an individualbiological neuron or an individual feature in a machine learning model is notintelligent, a large population of these neurons or features acting together canexhibit intelligent behavior. It truly is important to emphasize the fact that thenumber of neurons must be large. One of the key factors responsible for theimprovement in neural network’s accuracy and the improvement of the complexityof tasks they can solve between the 1980s and today is the dramatic increase inthe size of the networks we use. As we saw in section 1.2.3, network sizes havegrown exponentially for the past three decades, yet artiﬁcial neural networks areonly as large as the nervous systems of insects.Because the size of neural networks is critical, deep learning requires high438
CHAPTER 12. APPLICATIONSperformance hardware and software infrastructure.12.1.1 Fast CPU ImplementationsTraditionally, neural networks were trained using the CPU of a single machine.Today, this approach is generally considered insuﬃcient. We now mostly use GPUcomputing or the CPUs of many machines networked together. Before moving tothese expensive setups, researchers worked hard to demonstrate that CPUs couldnot manage the high computational workload required by neural networks.A description of how to implement eﬃcient numerical CPU code is beyondthe scope of this book, but we emphasize here that careful implementation forspeciﬁc CPU families can yield large improvements. For example, in 2011, the bestCPUs available could run neural network workloads faster when using ﬁxed-pointarithmetic rather than ﬂoating-point arithmetic. By creating a carefully tuned ﬁxed-point implementation, Vanhoucke et al. (2011) obtained a threefold speedup overa strong ﬂoating-point system. Each new model of CPU has diﬀerent performancecharacteristics, so sometimes ﬂoating-point implementations can be faster too.The important principle is that careful specialization of numerical computationroutines can yield a large payoﬀ. Other strategies, besides choosing whether to useﬁxed or ﬂoating point, include optimizing data structures to avoid cache missesand using vector instructions. Many machine learning researchers neglect theseimplementation details, but when the performance of an implementation restrictsthe size of the model, the accuracy of the model suﬀers.12.1.2 GPU ImplementationsMost modern neural network implementations are based on graphics processingunits. Graphics processing units (GPUs) are specialized hardware componentsthat were originally developed for graphics applications. The consumer market forvideo gaming systems spurred development of graphics processing hardware. Theperformance characteristics needed for good video gaming systems turn out to bebeneﬁcial for neural networks as well.Video game rendering requires performing many operations in parallel quickly.Models of characters and environments are speciﬁed via lists of 3-D coordinates ofvertices. Graphics cards must perform matrix multiplication and division on manyvertices in parallel to convert these 3-D coordinates into 2-D on-screen coordinates.The graphics card must then perform many computations at each pixel in parallel todetermine the color of each pixel. In both cases, the computations are fairly simple439
CHAPTER 12. APPLICATIONSand do not involve much branching compared to the computational workload thata CPU usually encounters. For example, each vertex in the same rigid object willbe multiplied by the same matrix; there is no need to evaluate an if statement pervertex to determine which matrix to multiply by. The computations are also entirelyindependent of each other, and thus may be parallelized easily. The computationsalso involve processing massive buﬀers of memory, containing bitmaps describingthe texture (color pattern) of each object to be rendered. Together, this results ingraphics cards having been designed to have a high degree of parallelism and highmemory bandwidth, at the cost of having a lower clock speed and less branchingcapability relative to traditional CPUs.Neural network algorithms require the same performance characteristics as thereal-time graphics algorithms described above. Neural networks usually involvelarge and numerous buﬀers of parameters, activation values, and gradient values,each of which must be completely updated during every step of training. Thesebuﬀers are large enough to fall outside the cache of a traditional desktop computer,so the memory bandwidth of the system often becomes the rate-limiting factor.GPUs oﬀer a compelling advantage over CPUs because of their high memorybandwidth. Neural network training algorithms typically do not involve muchbranching or sophisticated control, so they are appropriate for GPU hardware.Since neural networks can be divided into multiple individual “neurons” that can beprocessed independently from the other neurons in the same layer, neural networkseasily beneﬁt from the parallelism of GPU computing.GPU hardware was originally so specialized that it could be used only forgraphics tasks. Over time, GPU hardware became more ﬂexible, allowing customsubroutines to be used to transform the coordinates of vertices or to assign colorsto pixels. In principle, there was no requirement that these pixel values actuallybe based on a rendering task. These GPUs could be used for scientiﬁc computingby writing the output of a computation to a buﬀer of pixel values. Steinkrau et al.(2005) implemented a two-layer fully connected neural network on a GPU andreported a three-times speedup over their CPU-based baseline. Shortly thereafter,Chellapilla et al. (2006) demonstrated that the same technique could be used toaccelerate supervised convolutional networks.The popularity of graphics cards for neural network training exploded afterthe advent ofgeneral purpose GPUs. These GP-GPUs could execute arbitrarycode, not just rendering subroutines. NVIDIA’s CUDA programming languageprovided a way to write this arbitrary code in a C-like language. With theirrelatively convenient programming model, massive parallelism, and high memorybandwidth, GP-GPUs now oﬀer an ideal platform for neural network programming.440
CHAPTER 12. APPLICATIONSThis platform was rapidly adopted by deep learning researchers soon after it becameavailable (Raina et al., 2009; Ciresan et al., 2010).Writing eﬃcient code for GP-GPUs remains a diﬃcult task best left to special-ists. The techniques required to obtain good performance on GPU are very diﬀerentfrom those used on CPU. For example, good CPU-based code is usually designedto read information from the cache as much as possible. On GPU, most writablememory locations are not cached, so it can actually be faster to compute the samevalue twice, rather than compute it once and read it back from memory. GPU codeis also inherently multithreaded and the diﬀerent threads must be coordinatedwith each other carefully. For example, memory operations are faster if they canbecoalesced. Coalesced reads or writes occur when several threads can eachread or write a value that they need simultaneously, as part of a single memorytransaction. Diﬀerent models of GPUs are able to coalesce diﬀerent kinds of readpatterns and diﬀerent kinds of write patterns. Typically, memory operations areeasier to coalesce if amongnthreads, threadiaccesses bytei+jof memory, andjis a multiple of some power of 2. The exact speciﬁcations diﬀer between models ofGPU. Another common consideration for GPUs is making sure that each thread ina group executes the same instruction simultaneously. This means that branchingcan be diﬃcult on GPU. Threads are divided into small groups calledwarps. Eachthread in a warp executes the same instruction during each cycle, so if diﬀerentthreads within the same warp need to execute diﬀerent code paths, these diﬀerentcode paths must be traversed sequentially rather than in parallel.Because of the diﬃculty of writing high-performance GPU code, researchersshould structure their workﬂow to avoid needing to write new GPU code to testnew models or algorithms. Typically, one can do this by building a software libraryof high-performance operations like convolution and matrix multiplication, thenspecifying models in terms of calls to this library of operations. For example,the machine learning library Pylearn2 (Goodfellow et al., 2013c) speciﬁes all itsmachine learning algorithms in terms of calls to Theano (Bergstra et al., 2010;Bastien et al., 2012) and cuda-convnet (Krizhevsky, 2010), which provide thesehigh-performance operations. This factored approach can also ease support formultiple kinds of hardware. For example, the same Theano program can run oneither CPU or GPU, without needing to change any of the calls to Theano itself.Other libraries like TensorFlow (Abadi et al., 2015) and Torch (Collobert et al.,2011b) provide similar features.441


Part IIIDeep Learning Research482
This part of the book describes the more ambitious and advanced approachesto deep learning, currently pursued by the research community.In the previous parts of the book, we have shown how to solve supervisedlearning problems—how to learn to map one vector to another, given enoughexamples of the mapping.Not all problems we might want to solve fall into this category. We maywish to generate new examples, or determine how likely some point is, or handlemissing values and take advantage of a large set of unlabeled examples or examplesfrom related tasks. A shortcoming of the current state of the art for industrialapplications is that our learning algorithms require large amounts of superviseddata to achieve good accuracy. In this part of the book, we discuss some ofthe speculative approaches to reducing the amount of labeled data necessaryfor existing models to work well and be applicable across a broader range oftasks. Accomplishing these goals usually requires some form of unsupervised orsemi-supervised learning.Many deep learning algorithms have been designed to tackle unsupervisedlearning problems, but none has truly solved the problem in the same way thatdeep learning has largely solved the supervised learning problem for a wide variety oftasks. In this part of the book, we describe the existing approaches to unsupervisedlearning and some of the popular thought about how we can make progress in thisﬁeld.A central cause of the diﬃculties with unsupervised learning is the high di-mensionality of the random variables being modeled. This brings two distinctchallenges: a statistical challenge and a computational challenge. The statisticalchallenge regards generalization: the number of conﬁgurations we may want todistinguish can grow exponentially with the number of dimensions of interest, andthis quickly becomes much larger than the number of examples one can possiblyhave (or use with bounded computational resources). The computational challengeassociated with high-dimensional distributions arises because many algorithms forlearning or using a trained model (especially those based on estimating an explicitprobability function) involve intractable computations that grow exponentiallywith the number of dimensions.With probabilistic models, this computational challenge arises from the needto perform intractable inference or to normalize the distribution.•Intractable inference: inference is discussed mostly in chapter 19. It regardsthe question of guessing the probable values of some variablesa, given othervariablesb, with respect to a model that captures the joint distribution over483
a,bandc. In order to even compute such conditional probabilities, one needsto sum over the values of the variablesc, as well as compute a normalizationconstant that sums over the values of a and c.•Intractable normalization constants (the partition function): the partitionfunction is discussed mostly in chapter 18. Normalizing constants of proba-bility functions come up in inference (above) as well as in learning. Manyprobabilistic models involve such a normalizing constant. Unfortunately,learning such a model often requires computing the gradient of the loga-rithm of the partition function with respect to the model parameters. Thatcomputation is generally as intractable as computing the partition functionitself. Monte Carlo Markov chain (MCMC) methods (chapter 17) are of-ten used to deal with the partition function (computing it or its gradient).Unfortunately, MCMC methods suﬀer when the modes of the model distribu-tion are numerous and well separated, especially in high-dimensional spaces(section 17.5).One way to confront these intractable computations is to approximate them,and many approaches have been proposed, as discussed in this third part of thebook. Another interesting way, also discussed here, would be to avoid theseintractable computations altogether by design, and methods that do not requiresuch computations are thus very appealing. Several generative models have beenproposed in recent years with that motivation. A wide variety of contemporaryapproaches to generative modeling are discussed in chapter 20.Part III is the most important for a researcher—someone who wants to un-derstand the breadth of perspectives that have been brought to the ﬁeld of deeplearning and push the ﬁeld forward toward true artiﬁcial intelligence.484


Chapter 13Linear Factor ModelsMany of the research frontiers in deep learning involve building a probabilisticmodel of the input,pmodel(x). Such a model can, in principle, use probabilis-tic inference to predict any of the variables in its environment given any ofthe other variables. Many of these models also have latent variablesh, withpmodel(x) = Ehpmodel(x | h).These latent variables provide another means of rep-resenting the data. Distributed representations based on latent variables canobtain all the advantages of representation learning that we have seen with deepfeedforward and recurrent networks.In this chapter, we describe some of the simplest probabilistic models withlatent variables: linear factor models. These models are sometimes used as buildingblocks of mixture models (Hinton et al., 1995a; Ghahramani and Hinton, 1996;Roweis et al., 2002) or of larger, deep probabilistic models (Tang et al., 2012).They also show many of the basic approaches necessary to building generativemodels that the more advanced deep models will extend further.A linear factor model is deﬁned by the use of a stochastic linear decoder functionthat generates x by adding noise to a linear transformation of h.These models are interesting because they allow us to discover explanatoryfactors that have a simple joint distribution. The simplicity of using a linear decodermade these models some of the ﬁrst latent variable models to be extensively studied.A linear factor model describes the data-generation process as follows. First,we sample the explanatory factors h from a distributionh ∼ p(h), (13.1)wherep(h) is a factorial distribution, withp(h) =ip(hi), so that it is easy485
CHAPTER 13. LINEAR FACTOR MODELSh1h1h2h2h3h3x1x1x2x2x3x3x = W h + b + noisex = W h + b + noiseFigure 13.1: The directed graphical model describing the linear factor model family, inwhich we assume that an observed data vectorxis obtained by a linear combination ofindependent latent factorsh, plus some noise. Diﬀerent models, such as probabilisticPCA, factor analysis or ICA, make diﬀerent choices about the form of the noise and ofthe prior p(h).to sample from. Next we sample the real-valued observable variables given thefactorsx = W h + b + noise, (13.2)where the noise is typically Gaussian and diagonal (independent across dimensions).This is illustrated in ﬁgure 13.1.13.1 Probabilistic PCA and Factor AnalysisProbabilistic PCA (principal components analysis), factor analysis and other linearfactor models are special cases of the above equations (13.1 and 13.2) and onlydiﬀer in the choices made for the noise distribution and the model’s prior overlatent variables h before observing x.Infactor analysis(Bartholomew, 1987; Basilevsky, 1994), the latent variableprior is just the unit variance Gaussianh ∼ N(h; 0, I), (13.3)while the observed variablesxiare assumed to beconditionally independent,givenh. Speciﬁcally, the noise is assumed to be drawn from a diagonal co-variance Gaussian distribution, with covariance matrixψ=diag(σ2), withσ2= [σ21, σ22, . . . , σ2n]a vector of per-variable variances.The role of the latent variables is thus to capture the dependencies betweenthe diﬀerent observed variablesxi. Indeed, it can easily be shown thatxis just amultivariate normal random variable, withx ∼ N(x; b, W W+ ψ). (13.4)486
CHAPTER 13. LINEAR FACTOR MODELSTo cast PCA in a probabilistic framework, we can make a slight modiﬁcationto the factor analysis model, making the conditional variancesσ2iequal to eachother. In that case the covariance ofxis justW W+σ2I, whereσ2is now ascalar. This yields the conditional distributionx ∼ N(x; b, W W+ σ2I), (13.5)or equivalentlyx = W h + b + σz, (13.6)wherez ∼ N(z;0, I) is Gaussian noise. Then, as Tipping and Bishop (1999) show,we can use an iterative EM algorithm for estimating the parameters W and σ2.ThisprobabilisticPCA model takes advantage of the observation that mostvariations in the data can be captured by the latent variablesh, up to some smallresidualreconstruction error σ2. As shown by Tipping and Bishop (1999),probabilistic PCA becomes PCA asσ →0. In that case, the conditional expectedvalue ofhgivenxbecomes an orthogonal projection ofx − bonto the spacespanned by the d columns of W , as in PCA.Asσ →0, the density model deﬁned by probabilistic PCA becomes very sharparound theseddimensions spanned by the columns ofW. This can make themodel assign very low likelihood to the data if the data do not actually clusternear a hyperplane.13.2 Independent Component Analysis (ICA)Independent component analysis (ICA) is among the oldest representation learningalgorithms (Herault and Ans, 1984; Jutten and Herault, 1991; Comon, 1994;Hyvärinen, 1999; Hyvärinen et al., 2001a; Hinton et al., 2001; Teh et al., 2003).It is an approach to modeling linear factors that seeks to separate an observedsignal into many underlying signals that are scaled and added together to formthe observed data. These signals are intended to be fully independent, rather thanmerely decorrelated from each other.1Many diﬀerent speciﬁc methodologies are referred to as ICA. The variantthat is most similar to the other generative models we have described here is avariant (Pham et al., 1992) that trains a fully parametric generative model. Theprior distribution over the underlying factors,p(h), must be ﬁxed ahead of time bythe user. The model then deterministically generatesx=W h. We can perform a1See section 3.8 for a discussion of the diﬀerence between uncorrelated variables and indepen-dent variables.487
CHAPTER 13. LINEAR FACTOR MODELSnonlinear change of variables (using equation 3.47) to determinep(x).Learningthe model then proceeds as usual, using maximum likelihood.The motivation for this approach is that by choosingp(h) to be independent,we can recover underlying factors that are as close as possible to independent. Thisis commonly used, not to capture high-level abstract causal factors, but to recoverlow-level signals that have been mixed together. In this setting, each trainingexample is one moment in time, eachxiis one sensor’s observation of the mixedsignals, and eachhiis one estimate of one of the original signals. For example, wemight havenpeople speaking simultaneously. If we havendiﬀerent microphonesplaced in diﬀerent locations, ICA can detect the changes in the volume betweeneach speaker as heard by each microphone and separate the signals so that eachhicontains only one person speaking clearly. This is commonly used in neurosciencefor electroencephalography, a technology for recording electrical signals originatingin the brain. Multiple electrode sensors placed on the subject’s head are usedto measure many electrical signals coming from the body. The experimenter istypically only interested in signals from the brain, but signals from the subject’sheart and eyes are strong enough to confound measurements taken at the subject’sscalp. The signals arrive at the electrodes mixed together, so ICA is necessary toseparate the electrical signature of the heart from the signals originating in thebrain, and to separate signals in diﬀerent brain regions from each other.As mentioned before, many variants of ICA are possible. Some add some noisein the generation ofxrather than using a deterministic decoder. Most do notuse the maximum likelihood criterion, but instead aim to make the elements ofh=W−1xindependent from each other. Many criteria that accomplish this goalare possible. Equation 3.47 requires taking the determinant ofW, which can bean expensive and numerically unstable operation. Some variants of ICA avoid thisproblematic operation by constraining W to be orthogonal.All variants of ICA require thatp(h) be non-Gaussian. This is because ifp(h)is an independent prior with Gaussian components, thenWis not identiﬁable.We can obtain the same distribution overp(x) for many values ofW. This is verydiﬀerent from other linear factor models like probabilistic PCA and factor analysis,which often requirep(h) to be Gaussian in order to make many operations on themodel have closed form solutions. In the maximum likelihood approach, where theuser explicitly speciﬁes the distribution, a typical choice is to usep(hi) =ddhiσ(hi).Typical choices of these non-Gaussian distributions have larger peaks near 0 thandoes the Gaussian distribution, so we can also see most implementations of ICAas learning sparse features.Many variants of ICA are not generative models in the sense that we use the488


Chapter 14AutoencodersAnautoencoderis a neural network that is trained to attempt to copy its inputto its output. Internally, it has a hidden layerhthat describes acodeused torepresent the input. The network may be viewed as consisting of two parts: anencoder functionh=f(x) and a decoder that produces a reconstructionr=g(h).This architecture is presented in ﬁgure 14.1. If an autoencoder succeeds in simplylearning to setg(f(x)) =xeverywhere, then it is not especially useful. Instead,autoencoders are designed to be unable to learn to copy perfectly. Usually they arerestricted in ways that allow them to copy only approximately, and to copy onlyinput that resembles the training data. Because the model is forced to prioritizewhich aspects of the input should be copied, it often learns useful properties of thedata.Modern autoencoders have generalized the idea of an encoder and a de-coder beyond deterministic functions to stochastic mappingspencoder(h | x) andpdecoder(x | h).The idea of autoencoders has been part of the historical landscape of neuralnetworks for decades (LeCun, 1987; Bourlard and Kamp, 1988; Hinton and Zemel,1994). Traditionally, autoencoders were used for dimensionality reduction orfeature learning. Recently, theoretical connections between autoencoders andlatent variable models have brought autoencoders to the forefront of generativemodeling, as we will see in chapter 20. Autoencoders may be thought of as beinga special case of feedforward networks and may be trained with all the sametechniques, typically minibatch gradient descent following gradients computedby back-propagation. Unlike general feedforward networks, autoencoders mayalso be trained usingrecirculation(Hinton and McClelland, 1988), a learningalgorithm based on comparing the activations of the network on the original input499
CHAPTER 14. AUTOENCODERSxxrrhhfgFigure 14.1: The general structure of an autoencoder, mapping an input x to an output(called reconstruction)rthrough an internal representation or codeh. The autoencoderhas two components: the encoderf(mappingxtoh) and the decoderg(mappinghto r).to the activations on the reconstructed input. Recirculation is regarded as morebiologically plausible than back-propagation but is rarely used for machine learningapplications.14.1 Undercomplete AutoencodersCopying the input to the output may sound useless, but we are typically notinterested in the output of the decoder. Instead, we hope that training theautoencoder to perform the input copying task will result inhtaking on usefulproperties.One way to obtain useful features from the autoencoder is to constrainhtohave a smaller dimension thanx. An autoencoder whose code dimension is lessthan the input dimension is calledundercomplete. Learning an undercompleterepresentation forces the autoencoder to capture the most salient features of thetraining data.The learning process is described simply as minimizing a loss functionL(x, g(f(x))), (14.1)whereLis a loss function penalizingg(f(x)) for being dissimilar fromx, such asthe mean squared error.When the decoder is linear andLis the mean squared error, an undercompleteautoencoder learns to span the same subspace as PCA. In this case, an autoencodertrained to perform the copying task has learned the principal subspace of thetraining data as a side eﬀect.Autoencoders with nonlinear encoder functionsfand nonlinear decoder func-tionsgcan thus learn a more powerful nonlinear generalization of PCA. Unfortu-500
CHAPTER 14. AUTOENCODERSnately, if the encoder and decoder are allowed too much capacity, the autoencodercan learn to perform the copying task without extracting useful information aboutthe distribution of the data. Theoretically, one could imagine that an autoencoderwith a one-dimensional code but a very powerful nonlinear encoder could learn torepresent each training examplex(i)with the codei. The decoder could learn tomap these integer indices back to the values of speciﬁc training examples. Thisspeciﬁc scenario does not occur in practice, but it illustrates clearly that an autoen-coder trained to perform the copying task can fail to learn anything useful aboutthe dataset if the capacity of the autoencoder is allowed to become too great.14.2 Regularized AutoencodersUndercomplete autoencoders, with code dimension less than the input dimension,can learn the most salient features of the data distribution. We have seen thatthese autoencoders fail to learn anything useful if the encoder and decoder aregiven too much capacity.A similar problem occurs if the hidden code is allowed to have dimensionequal to the input, and in theovercompletecase in which the hidden code hasdimension greater than the input. In these cases, even a linear encoder and a lineardecoder can learn to copy the input to the output without learning anything usefulabout the data distribution.Ideally, one could train any architecture of autoencoder successfully, choosingthe code dimension and the capacity of the encoder and decoder based on thecomplexity of distribution to be modeled. Regularized autoencoders provide theability to do so. Rather than limiting the model capacity by keeping the encoderand decoder shallow and the code size small, regularized autoencoders use a lossfunction that encourages the model to have other properties besides the abilityto copy its input to its output. These other properties include sparsity of therepresentation, smallness of the derivative of the representation, and robustnessto noise or to missing inputs. A regularized autoencoder can be nonlinear andovercomplete but still learn something useful about the data distribution, even ifthe model capacity is great enough to learn a trivial identity function.In addition to the methods described here, which are most naturally interpretedas regularized autoencoders, nearly any generative model with latent variablesand equipped with an inference procedure (for computing latent representationsgiven input) may be viewed as a particular form of autoencoder. Two generativemodeling approaches that emphasize this connection with autoencoders are thedescendants of the Helmholtz machine (Hinton et al., 1995b), such as the variational501
CHAPTER 14. AUTOENCODERSautoencoder (section 20.10.3) and the generative stochastic networks (section 20.12).These models naturally learn high-capacity, overcomplete encodings of the inputand do not require regularization for these encodings to be useful. Their encodingsare naturally useful because the models were trained to approximately maximizethe probability of the training data rather than to copy the input to the output.14.2.1 Sparse AutoencodersA sparse autoencoder is simply an autoencoder whose training criterion involves asparsity penalty Ω(h) on the code layerh, in addition to the reconstruction error:L(x, g(f(x))) + Ω(h), (14.2)whereg(h) is the decoder output, and typically we haveh=f(x), the encoderoutput.Sparse autoencoders are typically used to learn features for another task, suchas classiﬁcation. An autoencoder that has been regularized to be sparse mustrespond to unique statistical features of the dataset it has been trained on, ratherthan simply acting as an identity function. In this way, training to perform thecopying task with a sparsity penalty can yield a model that has learned usefulfeatures as a byproduct.We can think of the penalty Ω(h) simply as a regularizer term added toa feedforward network whose primary task is to copy the input to the output(unsupervised learning objective) and possibly also perform some supervised task(with a supervised learning objective) that depends on these sparse features.Unlike other regularizers, such as weight decay, there is not a straightforwardBayesian interpretation to this regularizer. As described in section 5.6.1, trainingwith weight decay and other regularization penalties can be interpreted as aMAP approximation to Bayesian inference, with the added regularizing penaltycorresponding to a prior probability distribution over the model parameters. Inthis view, regularized maximum likelihood corresponds to maximizingp(θ | x),which is equivalent to maximizinglog p(x | θ) +log p(θ). Thelog p(x | θ) termis the usual data log-likelihood term, and thelog p(θ) term, the log-prior overparameters, incorporates the preference over particular values ofθ. This view isdescribed in section 5.6. Regularized autoencoders defy such an interpretationbecause the regularizer depends on the data and is therefore by deﬁnition not aprior in the formal sense of the word. We can still think of these regularizationterms as implicitly expressing a preference over functions.Rather than thinking of the sparsity penalty as a regularizer for the copyingtask, we can think of the entire sparse autoencoder framework as approximating502


Chapter 15Representation LearningIn this chapter, we ﬁrst discuss what it means to learn representations and howthe notion of representation can be useful to design deep architectures. We explorehow learning algorithms share statistical strength across diﬀerent tasks, includingusing information from unsupervised tasks to perform supervised tasks. Sharedrepresentations are useful to handle multiple modalities or domains, or to transferlearned knowledge to tasks for which few or no examples are given but a taskrepresentation exists. Finally, we step back and argue about the reasons for thesuccess of representation learning, starting with the theoretical advantages ofdistributed representations (Hinton et al., 1986) and deep representations, endingwith the more general idea of underlying assumptions about the data-generatingprocess, in particular about underlying causes of the observed data.Many information processing tasks can be very easy or very diﬃcult dependingon how the information is represented. This is a general principle applicable todaily life, to computer science in general, and to machine learning. For example, itis straightforward for a person to divide 210 by 6 using long division. The taskbecomes considerably less straightforward if it is instead posed using the Romannumeral representation of the numbers. Most modern people asked to divide CCXby VI would begin by converting the numbers to the Arabic numeral representation,permitting long division procedures that make use of the place value system. Moreconcretely, we can quantify the asymptotic runtime of various operations usingappropriate or inappropriate representations. For example, inserting a numberinto the correct position in a sorted list of numbers is anO(n) operation if thelist is represented as a linked list, but onlyO(log n) if the list is represented as ared-black tree.In the context of machine learning, what makes one representation better than524
CHAPTER 15. REPRESENTATION LEARNINGanother? Generally speaking, a good representation is one that makes a subsequentlearning task easier. The choice of representation will usually depend on the choiceof the subsequent learning task.We can think of feedforward networks trained by supervised learning as per-forming a kind of representation learning. Speciﬁcally, the last layer of the networkis typically a linear classiﬁer, such as a softmax regression classiﬁer. The rest ofthe network learns to provide a representation to this classiﬁer. Training with asupervised criterion naturally leads to the representation at every hidden layer (butmore so near the top hidden layer) taking on properties that make the classiﬁcationtask easier. For example, classes that were not linearly separable in the inputfeatures may become linearly separable in the last hidden layer. In principle, thelast layer could be another kind of model, such as a nearest neighbor classiﬁer(Salakhutdinov and Hinton, 2007a). The features in the penultimate layer shouldlearn diﬀerent properties depending on the type of the last layer.Supervised training of feedforward networks does not involve explicitly imposingany condition on the learned intermediate features. Other kinds of representationlearning algorithms are often explicitly designed to shape the representation insome particular way. For example, suppose we want to learn a representation thatmakes density estimation easier. Distributions with more independences are easierto model, so we could design an objective function that encourages the elementsof the representation vectorhto be independent. Just like supervised networks,unsupervised deep learning algorithms have a main training objective but alsolearn a representation as a side eﬀect. Regardless of how a representation wasobtained, it can be used for another task. Alternatively, multiple tasks (somesupervised, some unsupervised) can be learned together with some shared internalrepresentation.Most representation learning problems face a trade-oﬀ between preserving asmuch information about the input as possible and attaining nice properties (suchas independence).Representation learning is particularly interesting because it provides oneway to perform unsupervised and semi-supervised learning. We often have verylarge amounts of unlabeled training data and relatively little labeled trainingdata. Training with supervised learning techniques on the labeled subset oftenresults in severe overﬁtting. Semi-supervised learning oﬀers the chance to resolvethis overﬁtting problem by also learning from the unlabeled data. Speciﬁcally,we can learn good representations for the unlabeled data, and then use theserepresentations to solve the supervised learning task.Humans and animals are able to learn from very few labeled examples. We do525
CHAPTER 15. REPRESENTATION LEARNINGnot yet know how this is possible. Many factors could explain improved humanperformance—for example, the brain may use very large ensembles of classiﬁersor Bayesian inference techniques. One popular hypothesis is that the brain isable to leverage unsupervised or semi-supervised learning. There are many waysto leverage unlabeled data. In this chapter, we focus on the hypothesis that theunlabeled data can be used to learn a good representation.15.1 Greedy Layer-Wise Unsupervised PretrainingUnsupervised learning played a key historical role in the revival of deep neuralnetworks, enabling researchers for the ﬁrst time to train a deep supervised networkwithout requiring architectural specializations like convolution or recurrence. Wecall this procedureunsupervised pretraining, or more precisely,greedy layer-wise unsupervised pretraining. This procedure is a canonical example of howa representation learned for one task (unsupervised learning, trying to capturethe shape of the input distribution) can sometimes be useful for another task(supervised learning with the same input domain).Greedy layer-wise unsupervised pretraining relies on a single-layer represen-tation learning algorithm such as an RBM, a single-layer autoencoder, a sparsecoding model, or another model that learns latent representations. Each layer ispretrained using unsupervised learning, taking the output of the previous layerand producing as output a new representation of the data, whose distribution (orits relation to other variables, such as categories to predict) is hopefully simpler.See algorithm 15.1 for a formal description.Greedy layer-wise training procedures based on unsupervised criteria have longbeen used to sidestep the diﬃculty of jointly training the layers of a deep neural netfor a supervised task. This approach dates back at least as far as the neocognitron(Fukushima, 1975). The deep learning renaissance of 2006 began with the discoverythat this greedy learning procedure could be used to ﬁnd a good initialization fora joint learning procedure over all the layers, and that this approach could be usedto successfully train even fully connected architectures (Hinton et al., 2006; Hintonand Salakhutdinov, 2006; Hinton, 2006; Bengio et al., 2007; Ranzato et al., 2007a).Prior to this discovery, only convolutional deep networks or networks whose depthresulted from recurrence were regarded as feasible to train. Today, we now knowthat greedy layer-wise pretraining is not required to train fully connected deeparchitectures, but the unsupervised pretraining approach was the ﬁrst method tosucceed.Greedy layer-wise pretraining is calledgreedybecause it is agreedy algo-526
CHAPTER 15. REPRESENTATION LEARNINGrithm, meaning that it optimizes each piece of the solution independently, onepiece at a time, rather than jointly optimizing all pieces. It is calledlayer-wisebecause these independent pieces are the layers of the network. Speciﬁcally, greedylayer-wise pretraining proceeds one layer at a time, training thek-th layer whilekeeping the previous ones ﬁxed. In particular, the lower layers (which are trainedﬁrst) are not adapted after the upper layers are introduced. It is calledunsuper-visedbecause each layer is trained with an unsupervised representation learningalgorithm. However, it is also calledpretrainingbecause it is supposed to beonly a ﬁrst step before a joint training algorithm is applied toﬁne-tuneall thelayers together. In the context of a supervised learning task, it can be viewedas a regularizer (in some experiments, pretraining decreases test error withoutdecreasing training error) and a form of parameter initialization.It is common to use the word “pretraining” to refer not only to the pretrainingstage itself but to the entire two-phase protocol that combines the pretrainingphase and a supervised learning phase. The supervised learning phase may involvetraining a simple classiﬁer on top of the features learned in the pretraining phase,or it may involve supervised ﬁne-tuning of the entire network learned in thepretraining phase. No matter what kind of unsupervised learning algorithm orwhat model type is employed, in most cases, the overall training scheme is nearlythe same. While the choice of unsupervised learning algorithm will obviously aﬀectthe details, most applications of unsupervised pretraining follow this basic protocol.Greedy layer-wise unsupervised pretraining can also be used as initializationfor other unsupervised learning algorithms, such as deep autoencoders (Hintonand Salakhutdinov, 2006) and probabilistic models with many layers of latentvariables. Such models include deep belief networks (Hinton et al., 2006) and deepBoltzmann machines (Salakhutdinov and Hinton, 2009a). These deep generativemodels are described in chapter 20.As discussed in section 8.7.4, it is also possible to have greedy layer-wisesupervised pretraining. This builds on the premise that training a shallow networkis easier than training a deep one, which seems to have been validated in severalcontexts (Erhan et al., 2010).15.1.1 When and Why Does Unsupervised Pretraining Work?On many tasks, greedy layer-wise unsupervised pretraining can yield substantialimprovements in test error for classiﬁcation tasks. This observation was responsiblefor the renewed interested in deep neural networks starting in 2006 (Hinton et al.,527


Chapter 16Structured Probabilistic Modelsfor Deep LearningDeep learning draws on many modeling formalisms that researchers can use to guidetheir design eﬀorts and describe their algorithms. One of these formalisms is theidea ofstructured probabilistic models. We discuss structured probabilisticmodels brieﬂy in section 3.14. That brief presentation is suﬃcient to understandhow to use structured probabilistic models as a language to describe some of thealgorithms in part II. Now, in part III, structured probabilistic models are a keyingredient of many of the most important research topics in deep learning. Toprepare to discuss these research ideas, in this chapter, we describe structuredprobabilistic models in much greater detail. This chapter is intended to be self-contained; the reader does not need to review the earlier introduction beforecontinuing with this chapter.A structured probabilistic model is a way of describing a probability distribution,using a graph to describe which random variables in the probability distributioninteract with each other directly. Here we use “graph” in the graph theory sense—aset of vertices connected to one another by a set of edges. Because the structureof the model is deﬁned by a graph, these models are often also referred to asgraphical models.The graphical models research community is large and has developed manydiﬀerent models, training algorithms, and inference algorithms. In this chapter, weprovide basic background on some of the most central ideas of graphical models,with an emphasis on the concepts that have proved most useful to the deep learningresearch community. If you already have a strong background in graphical models,you may wish to skip most of this chapter. However, even a graphical model555
CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNINGexpert may beneﬁt from reading the ﬁnal section of this chapter, section 16.7, inwhich we highlight some of the unique ways in which graphical models are usedfor deep learning algorithms. Deep learning practitioners tend to use very diﬀerentmodel structures, learning algorithms and inference procedures than are commonlyused by the rest of the graphical models research community. In this chapter, weidentify these diﬀerences in preferences and explain the reasons for them.We ﬁrst describe the challenges of building large-scale probabilistic models.Next, we describe how to use a graph to describe the structure of a probabilitydistribution. While this approach allows us to overcome many challenges, it is notwithout its own complications. One of the major diﬃculties in graphical modelingis understanding which variables need to be able to interact directly, that is, whichgraph structures are most suitable for a given problem. In section 16.5, we outlinetwo approaches to resolving this diﬃculty by learning about the dependencies.Finally, we close with a discussion of the unique emphasis that deep learningpractitioners place on speciﬁc approaches to graphical modeling, in section 16.7.16.1 The Challenge of Unstructured ModelingThe goal of deep learning is to scale machine learning to the kinds of challengesneeded to solve artiﬁcial intelligence. This means being able to understand high-dimensional data with rich structure. For example, we would like AI algorithms tobe able to understand natural images,1audio waveforms representing speech, anddocuments containing multiple words and punctuation characters.Classiﬁcation algorithms can take an input from such a rich high-dimensionaldistribution and summarize it with a categorical label—what object is in a photo,what word is spoken in a recording, what topic a document is about. The processof classiﬁcation discards most of the information in the input and produces asingle output (or a probability distribution over values of that single output). Theclassiﬁer is also often able to ignore many parts of the input. For example, whenrecognizing an object in a photo, it is usually possible to ignore the background ofthe photo.It is possible to ask probabilistic models to do many other tasks. These tasks areoften more expensive than classiﬁcation. Some of them require producing multipleoutput values. Most require a complete understanding of the entire structure ofthe input, with no option to ignore sections of it. These tasks include the following:1Anatural imageis an image that might be captured by a camera in a reasonably ordinaryenvironment, as opposed to a synthetically rendered image, a screenshot of a web page, etc.556
CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING• Density estimation: Given an inputx, the machine learning systemreturns an estimate of the true densityp(x) under the data-generatingdistribution. This requires only a single output, but it also requires acomplete understanding of the entire input. If even one element of the vectoris unusual, the system must assign it a low probability.• Denoising: Given a damaged or incorrectly observed input˜x, the machinelearning system returns an estimate of the original or correctx. For example,the machine learning system might be asked to remove dust or scratchesfrom an old photograph. This requires multiple outputs (every element of theestimated clean examplex) and an understanding of the entire input (sinceeven one damaged area will still reveal the ﬁnal estimate as being damaged).• Missing value imputation: Given the observations of some elements ofx,the model is asked to return estimates of or a probability distribution oversome or all of the unobserved elements ofx. This requires multiple outputs.Because the model could be asked to restore any of the elements ofx, itmust understand the entire input.• Sampling: The model generates new samples from the distributionp(x).Applications include speech synthesis, that is, producing new waveforms thatsound like natural human speech. This requires multiple output values and agood model of the entire input. If the samples have even one element drawnfrom the wrong distribution, then the sampling process is wrong.For an example of a sampling task using small natural images, see ﬁgure 16.1.Modeling a rich distribution over thousands or millions of random variables isa challenging task, both computationally and statistically. Suppose we wanted tomodel only binary variables. This is the simplest possible case, and yet already itseems overwhelming. For a small 32×32 pixel color (RGB) image, there are 23072possible binary images of this form. This number is over 10800times larger thanthe estimated number of atoms in the universe.In general, if we wish to model a distribution over a random vectorxcontainingndiscrete variables capable of taking onkvalues each, then the naive approach ofrepresentingP(x) by storing a lookup table with one probability value per possibleoutcome requires knparameters!This is not feasible for several reasons:•Memory—the cost of storing the representation: For all but very small valuesofnandk, representing the distribution as a table will require too manyvalues to store.557
CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNINGFigure 16.1: Probabilistic modeling of natural images. (Top) Example 32×32 pixel colorimages from the CIFAR-10 dataset (Krizhevsky and Hinton, 2009). (Bottom) Samplesdrawn from a structured probabilistic model trained on this dataset. Each sample appearsat the same position in the grid as the training example that is closest to it in Euclideanspace. This comparison allows us to see that the model is truly synthesizing new images,rather than memorizing the training data. Contrast of both sets of images has beenadjusted for display. Figure reproduced with permission from Courville et al. (2011).558


Chapter 17Monte Carlo MethodsRandomized algorithms fall into two rough categories: Las Vegas algorithms andMonte Carlo algorithms. Las Vegas algorithms always return precisely the correctanswer (or report that they failed). These algorithms consume a random amountof resources, usually memory or time. In contrast, Monte Carlo algorithms returnanswers with a random amount of error. The amount of error can typically bereduced by expending more resources (usually running time and memory). For anyﬁxed computational budget, a Monte Carlo algorithm can provide an approximateanswer.Many problems in machine learning are so diﬃcult that we can never expect toobtain precise answers to them. This excludes precise deterministic algorithms andLas Vegas algorithms. Instead, we must use deterministic approximate algorithmsor Monte Carlo approximations. Both approaches are ubiquitous in machinelearning. In this chapter, we focus on Monte Carlo methods.17.1 Sampling and Monte Carlo MethodsMany important technologies used to accomplish machine learning goals are basedon drawing samples from some probability distribution and using these samples toform a Monte Carlo estimate of some desired quantity.17.1.1 Why Sampling?We may wish to draw samples from a probability distribution for many reasons.Sampling provides a ﬂexible way to approximate many sums and integrals at587
CHAPTER 17. MONTE CARLO METHODSreduced cost. Sometimes we use this to provide a signiﬁcant speedup to a costlybut tractable sum, as in the case when we subsample the full training cost withminibatches. In other cases, our learning algorithm requires us to approximate anintractable sum or integral, such as the gradient of the log partition function of anundirected model. In many other cases, sampling is actually our goal, in the sensethat we want to train a model that can sample from the training distribution.17.1.2 Basics of Monte Carlo SamplingWhen a sum or an integral cannot be computed exactly (for example, the sumhas an exponential number of terms, and no exact simpliﬁcation is known), it isoften possible to approximate it using Monte Carlo sampling. The idea is to viewthe sum or integral as if it were an expectation under some distribution and toapproximate the expectation by a corresponding average. Lets =xp(x)f(x) = Ep[f(x)] (17.1)ors =p(x)f(x)dx = Ep[f(x)] (17.2)be the sum or integral to estimate, rewritten as an expectation, with the constraintthatpis a probability distribution (for the sum) or a probability density (for theintegral) over random variable x.We can approximatesby drawingnsamplesx(1), . . . , x(n)frompand thenforming the empirical averageˆsn=1nni=1f(x(i)). (17.3)This approximation is justiﬁed by a few diﬀerent properties. The ﬁrst trivialobservation is that the estimator ˆs is unbiased, sinceE[ˆsn] =1nni=1E[f(x(i))] =1nni=1s = s. (17.4)But in addition, thelaw of large numbersstates that if the samplesx(i)arei.i.d., then the average converges almost surely to the expected value:limn→∞ˆsn= s, (17.5)588
CHAPTER 17. MONTE CARLO METHODSprovided that the variance of the individual terms,Var[f(x(i))], is bounded. To seethis more clearly, consider the variance ofˆsnasnincreases. The varianceVar[ˆsn]decreases and converges to 0, so long as Var[f(x(i))] < ∞:Var[ˆsn] =1n2ni=1Var[f(x)] (17.6)=Var[f(x)]n. (17.7)This convenient result also tells us how to estimate the uncertainty in a MonteCarlo average or equivalently the amount of expected error of the Monte Carloapproximation. We compute both the empirical average of thef(x(i)) and theirempirical variance,1and then divide the estimated variance by the number ofsamplesnto obtain an estimator ofVar[ˆsn]. Thecentral limit theoremtellsus that the distribution of the average,ˆsn, converges to a normal distributionwith meansand varianceVar[f(x)]n. This allows us to estimate conﬁdence intervalsaround the estimate ˆsn, using the cumulative distribution of the normal density.All this relies on our ability to easily sample from the base distributionp(x),but doing so is not always possible. When it is not feasible to sample fromp, analternative is to use importance sampling, presented in section 17.2. A more generalapproach is to form a sequence of estimators that converge toward the distributionof interest. That is the approach of Monte Carlo Markov chains (section 17.3).17.2 Importance SamplingAn important step in the decomposition of the integrand (or summand) used by theMonte Carlo method in equation 17.2 is deciding which part of the integrand shouldplay the role of probabilityp(x) and which part of the integrand should play therole of the quantityf(x) whose expected value (under that probability distribution)is to be estimated. There is no unique decomposition becausep(x)f(x) can alwaysbe rewritten asp(x)f(x) = q(x)p(x)f(x)q(x), (17.8)where we now sample fromqand averagepfq. In many cases, we wish to computean expectation for a givenpand anf, and the fact that the problem is speciﬁedfrom the start as an expectation suggests that thispandfwould be a natural1The unbiased estimator of the variance is often preferred, in which the sum of squareddiﬀerences is divided by n − 1 instead of n.589
CHAPTER 17. MONTE CARLO METHODSchoice of decomposition. However, the original speciﬁcation of the problem maynot be the the optimal choice in terms of the number of samples required to obtaina given level of accuracy. Fortunately, the form of the optimal choiceq∗can bederived easily. The optimalq∗corresponds to what is called optimal importancesampling.Because of the identity shown in equation 17.8, any Monte Carlo estimatorˆsp=1nni=1,x(i)∼pf(x(i)) (17.9)can be transformed into an importance sampling estimatorˆsq=1nni=1,x(i)∼qp(x(i))f(x(i))q(x(i)). (17.10)We see readily that the expected value of the estimator does not depend on q:Eq[ˆsq] = Eq[ˆsp] = s. (17.11)The variance of an importance sampling estimator, however, can be greatly sensitiveto the choice of q. The variance is given byVar[ˆsq] = Var[p(x)f(x)q(x)]/n. (17.12)The minimum variance occurs when q isq∗(x) =p(x)|f(x)|Z, (17.13)whereZis the normalization constant, chosen so thatq∗(x) sums or integrates to1 as appropriate. Better importance sampling distributions put more weight wherethe integrand is larger. In fact, whenf(x) does not change sign,Var[ˆsq∗] = 0,meaning that a single sample is suﬃcient when the optimal distribution is used.Of course, this is only because the computation ofq∗has essentially solved theoriginal problem, so it is usually not practical to use this approach of drawing asingle sample from the optimal distribution.Any choice of sampling distributionqis valid (in the sense of yielding thecorrect expected value), andq∗is the optimal one (in the sense of yielding minimumvariance). Sampling fromq∗is usually infeasible, but other choices ofqcan befeasible while still reducing the variance somewhat.590


Chapter 18Confronting the PartitionFunctionIn section 16.2.2 we saw that many probabilistic models (commonly known as undi-rected graphical models) are deﬁned by an unnormalized probability distribution˜p(x;θ). We must normalize˜pby dividing by a partition functionZ(θ) to obtain avalid probability distribution:p(x; θ) =1Z(θ)˜p(x; θ). (18.1)The partition function is an integral (for continuous variables) or sum (for discretevariables) over the unnormalized probability of all states:˜p(x)dx (18.2)orx˜p(x). (18.3)This operation is intractable for many interesting models.As we will see in chapter 20, several deep learning models are designed to havea tractable normalizing constant, or are designed to be used in ways that do notinvolve computingp(x) at all. Yet, other models directly confront the challenge ofintractable partition functions. In this chapter, we describe techniques used fortraining and evaluating models that have intractable partition functions.603
CHAPTER 18. CONFRONTING THE PARTITION FUNCTION18.1 The Log-Likelihood GradientWhat makes learning undirected models by maximum likelihood particularlydiﬃcult is that the partition function depends on the parameters. The gradient ofthe log-likelihood with respect to the parameters has a term corresponding to thegradient of the partition function:∇θlog p(x; θ) = ∇θlog ˜p(x; θ) − ∇θlog Z(θ). (18.4)This is a well-known decomposition into thepositive phaseandnegativephase of learning.For most undirected models of interest, the negative phase is diﬃcult. Modelswith no latent variables or with few interactions between latent variables typicallyhave a tractable positive phase. The quintessential example of a model with astraightforward positive phase and a diﬃcult negative phase is the RBM, which hashidden units that are conditionally independent from each other given the visibleunits. The case where the positive phase is diﬃcult, with complicated interactionsbetween latent variables, is primarily covered in chapter 19. This chapter focuseson the diﬃculties of the negative phase.Let us look more closely at the gradient of log Z:∇θlog Z (18.5)=∇θZZ(18.6)=∇θx˜p(x)Z(18.7)=x∇θ˜p(x)Z. (18.8)For models that guaranteep(x)>0 for allx, we can substituteexp (log ˜p(x))for ˜p(x):x∇θexp (log ˜p(x))Z(18.9)=xexp (log ˜p(x)) ∇θlog ˜p(x)Z(18.10)=x˜p(x)∇θlog ˜p(x)Z(18.11)=xp(x)∇θlog ˜p(x) (18.12)604
CHAPTER 18. CONFRONTING THE PARTITION FUNCTION= Ex∼p(x)∇θlog ˜p(x). (18.13)This derivation made use of summation over discretex, but a similar resultapplies using integration over continuousx. In the continuous version of thederivation, we use Leibniz’s rule for diﬀerentiation under the integral sign to obtainthe identity∇θ˜p(x)dx =∇θ˜p(x)dx. (18.14)This identity is applicable only under certain regularity conditions on˜pand∇θ˜p(x).In measure theoretic terms, the conditions are: (1) The unnormalized distribution˜pmust be a Lebesgue-integrable function ofxfor every value ofθ. (2) The gradient∇θ˜p(x) must exist for allθand almost allx. (3) There must exist an integrablefunctionR(x) that bounds∇θ˜p(x) in the sense thatmaxi|∂∂θi˜p(x)| ≤ R(x) for allθand almost allx. Fortunately, most machine learning models of interest havethese properties.This identity∇θlog Z = Ex∼p(x)∇θlog ˜p(x) (18.15)is the basis for a variety of Monte Carlo methods for approximately maximizingthe likelihood of models with intractable partition functions.The Monte Carlo approach to learning undirected models provides an intuitiveframework in which we can think of both the positive phase and the negativephase. In the positive phase, we increaselog ˜p(x) forxdrawn from the data. Inthe negative phase, we decrease the partition function by decreasinglog ˜p(x) drawnfrom the model distribution.In the deep learning literature, it is common to parametrizelog ˜pin terms ofan energy function (equation 16.7). In this case, we can interpret the positivephase as pushing down on the energy of training examples and the negative phaseas pushing up on the energy of samples drawn from the model, as illustrated inﬁgure 18.1.18.2 Stochastic Maximum Likelihood and ContrastiveDivergenceThe naive way of implementing equation 18.15 is to compute it by burning ina set of Markov chains from a random initialization every time the gradient isneeded. When learning is performed using stochastic gradient descent, this meansthe chains must be burned in once per gradient step. This approach leads to the605
CHAPTER 18. CONFRONTING THE PARTITION FUNCTIONAlgorithm 18.1A naive MCMC algorithm for maximizing the log-likelihoodwith an intractable partition function using gradient ascentSet , the step size, to a small positive number.Setk, the number of Gibbs steps, high enough to allow burn in. Perhaps 100 totrain an RBM on a small image patch.while not converged doSample a minibatch of m examples {x(1), . . . , x(m)} from the training setg ←1mmi=1∇θlog ˜p(x(i); θ).Initialize a set ofmsamples{˜x(1), . . . ,˜x(m)}to random values (e.g., froma uniform or normal distribution, or possibly a distribution with marginalsmatched to the model’s marginals).for i = 1 to k dofor j = 1 to m do˜x(j)← gibbs_update(˜x(j)).end forend forg ← g −1mmi=1∇θlog ˜p(˜x(i); θ).θ ← θ + g.end whiletraining procedure presented in algorithm 18.1. The high cost of burning in theMarkov chains in the inner loop makes this procedure computationally infeasible,but this procedure is the starting point that other more practical algorithms aimto approximate.We can view the MCMC approach to maximum likelihood as trying to achievebalance between two forces, one pushing up on the model distribution where thedata occurs, and another pushing down on the model distribution where the modelsamples occur. Figure 18.1 illustrates this process. The two forces correspond tomaximizinglog ˜pand minimizinglog Z. Several approximations to the negativephase are possible. Each of these approximations can be understood as makingthe negative phase computationally cheaper but also making it push down in thewrong locations.Because the negative phase involves drawing samples from the model’s distri-bution, we can think of it as ﬁnding points that the model believes in strongly.Because the negative phase acts to reduce the probability of those points, theyare generally considered to represent the model’s incorrect beliefs about the world.They are frequently referred to in the literature as “hallucinations” or “fantasyparticles.” In fact, the negative phase has been proposed as a possible explanation606


Chapter 19Approximate InferenceMany probabilistic models are diﬃcult to train because it is diﬃcult to performinference in them. In the context of deep learning, we usually have a set of visiblevariablesvand a set of latent variablesh. The challenge of inference usuallyrefers to the diﬃcult problem of computingp(h | v) or taking expectations withrespect to it. Such operations are often necessary for tasks like maximum likelihoodlearning.Many simple graphical models with only one hidden layer, such as restrictedBoltzmann machines and probabilistic PCA, are deﬁned in a way that makesinference operations like computingp(h | v), or taking expectations with respectto it, simple. Unfortunately, most graphical models with multiple layers of hiddenvariables have intractable posterior distributions. Exact inference requires anexponential amount of time in these models. Even some models with only a singlelayer, such as sparse coding, have this problem.In this chapter, we introduce several of the techniques for confronting theseintractable inference problems. In chapter 20, we describe how to use thesetechniques to train probabilistic models that would otherwise be intractable, suchas deep belief networks and deep Boltzmann machines.Intractable inference problems in deep learning usually arise from interactionsbetween latent variables in a structured graphical model. See ﬁgure 19.1 for someexamples. These interactions may be due to direct interactions in undirectedmodels or “explaining away” interactions between mutual ancestors of the samevisible unit in directed models.629
CHAPTER 19. APPROXIMATE INFERENCEFigure 19.1: Intractable inference problems in deep learning are usually the result ofinteractions between latent variables in a structured graphical model. These interactionscan be due to edges directly connecting one latent variable to another or longer pathsthat are activated when the child of a V-structure is observed. (Left)Asemi-restrictedBoltzmann machine(Osindero and Hinton, 2008) with connections between hiddenunits. These direct connections between latent variables make the posterior distributionintractable because of large cliques of latent variables. (Center)A deep Boltzmann machine,organized into layers of variables without intralayer connections, still has an intractableposterior distribution because of the connections between layers. (Right)This directedmodel has interactions between latent variables when the visible variables are observed,because every two latent variables are coparents. Some probabilistic models are ableto provide tractable inference over the latent variables despite having one of the graphstructures depicted above. This is possible if the conditional probability distributionsare chosen to introduce additional independences beyond those described by the graph.For example, probabilistic PCA has the graph structure shown in the right yet still hassimple inference because of special properties of the speciﬁc conditional distributions ituses (linear-Gaussian conditionals with mutually orthogonal basis vectors).630
CHAPTER 19. APPROXIMATE INFERENCE19.1 Inference as OptimizationMany approaches to confronting the problem of diﬃcult inference make use ofthe observation that exact inference can be described as an optimization problem.Approximate inference algorithms may then be derived by approximating theunderlying optimization problem.To construct the optimization problem, assume we have a probabilistic modelconsisting of observed variablesvand latent variablesh. We would like to computethe log-probability of the observed data,log p(v;θ). Sometimes it is too diﬃcultto computelog p(v;θ) if it is costly to marginalize outh. Instead, we can computea lower boundL(v, θ, q) onlog p(v;θ). This bound is called theevidence lowerbound(ELBO). Another commonly used name for this lower bound is the negativevariational free energy. Speciﬁcally, the evidence lower bound is deﬁned to beL(v, θ, q) = log p(v; θ) − DKL(q(h | v)p(h | v; θ)) , (19.1)where q is an arbitrary probability distribution over h.Because the diﬀerence betweenlog p(v) andL(v, θ, q) is given by the KLdivergence, and because the KL divergence is always nonnegative, we can see thatLalways has at most the same value as the desired log-probability. The two areequal if and only if q is the same distribution as p(h | v).Surprisingly,Lcan be considerably easier to compute for some distributionsq.Simple algebra shows that we can rearrangeLinto a much more convenient form:L(v, θ, q) = log p(v; θ) −DKL(q(h | v)p(h | v; θ)) (19.2)= log p(v; θ) − Eh∼qlogq(h | v)p(h | v)(19.3)= log p(v; θ) − Eh∼qlogq(h | v)p(h,v;θ)p(v;θ)(19.4)= log p(v; θ) − Eh∼q[log q(h | v) − log p(h, v; θ) + log p(v; θ)] (19.5)= − Eh∼q[log q(h | v) − log p(h, v; θ)] . (19.6)This yields the more canonical deﬁnition of the evidence lower bound,L(v, θ, q) = Eh∼q[log p(h, v)] + H(q). (19.7)For an appropriate choice ofq,Lis tractable to compute. For any choiceofq,Lprovides a lower bound on the likelihood. Forq(h | v) that are better631
CHAPTER 19. APPROXIMATE INFERENCEapproximations ofp(h | v), the lower boundLwill be tighter, in other words,closer tolog p(v). Whenq(h | v) =p(h | v), the approximation is perfect, andL(v, θ, q) = log p(v; θ).We can thus think of inference as the procedure for ﬁnding theqthat maximizesL. Exact inference maximizesLperfectly by searching over a family of functionsqthat includesp(h | v). Throughout this chapter, we show how to derive diﬀerentforms of approximate inference by using approximate optimization to ﬁndq. Wecan make the optimization procedure less expensive but approximate by restrictingthe family of distributionsqthat the optimization is allowed to search over or byusing an imperfect optimization procedure that may not completely maximizeLbut may merely increase it by a signiﬁcant amount.No matter what choice ofqwe use,Lis a lower bound. We can get tighteror looser bounds that are cheaper or more expensive to compute depending onhow we choose to approach this optimization problem. We can obtain a poorlymatchedqbut reduce the computational cost by using an imperfect optimizationprocedure, or by using a perfect optimization procedure over a restricted family ofq distributions.19.2 Expectation MaximizationThe ﬁrst algorithm we introduce based on maximizing a lower boundLis theexpectation maximization(EM) algorithm, a popular training algorithm formodels with latent variables. We describe here a view on the EM algorithmdeveloped by Neal and Hinton (1999). Unlike most of the other algorithms wedescribe in this chapter, EM is not an approach to approximate inference, butrather an approach to learning with an approximate posterior.The EM algorithm consists of alternating between two steps until convergence:•TheE-step(expectation step): Letθ(0)denote the value of the parametersat the beginning of the step. Setq(h(i)| v) =p(h(i)| v(i);θ(0)) for allindicesiof the training examplesv(i)we want to train on (both batch andminibatch variants are valid). By this we meanqis deﬁned in terms of thecurrent parameter value ofθ(0); if we varyθ, thenp(h | v;θ) will change,but q(h | v) will remain equal to p(h | v; θ(0)).• The M-step (maximization step): Completely or partially maximizeiL(v(i), θ, q) (19.8)632


Chapter 20Deep Generative ModelsIn this chapter, we present several of the speciﬁc kinds of generative models thatcan be built and trained using the techniques presented in chapters16–19. Allthese models represent probability distributions over multiple variables in someway. Some allow the probability distribution function to be evaluated explicitly.Others do not allow the evaluation of the probability distribution function butsupport operations that implicitly require knowledge of it, such as drawing samplesfrom the distribution. Some of these models are structured probabilistic modelsdescribed in terms of graphs and factors, using the language of graphical modelspresented in chapter 16. Others cannot be easily described in terms of factors butrepresent probability distributions nonetheless.20.1 Boltzmann MachinesBoltzmann machines were originally introduced as a general “connectionist” ap-proach to learning arbitrary probability distributions over binary vectors (Fahlmanet al., 1983; Ackley et al., 1985; Hinton et al., 1984; Hinton and Sejnowski, 1986).Variants of the Boltzmann machine that include other kinds of variables have longago surpassed the popularity of the original. In this section we brieﬂy introducethe binary Boltzmann machine and discuss the issues that come up when trying totrain and perform inference in the model.We deﬁne the Boltzmann machine over ad-dimensional binary random vectorx ∈ {0,1}d. The Boltzmann machine is an energy-based model (section 16.2.4),651
CHAPTER 20. DEEP GENERATIVE MODELSmeaning we deﬁne the joint probability distribution using an energy function:P (x) =exp (−E(x))Z, (20.1)whereE(x) is the energy function, andZis the partition function that ensuresthatxP (x) = 1. The energy function of the Boltzmann machine is given byE(x) = −xUx − bx, (20.2)whereUis the “weight” matrix of model parameters andbis the vector of biasparameters.In the general setting of the Boltzmann machine, we are given a set of trainingexamples, each of which aren-dimensional. Equation 20.1 describes the jointprobability distribution over the observed variables. While this scenario is certainlyviable, it does limit the kinds of interactions between the observed variables tothose described by the weight matrix. Speciﬁcally, it means that the probability ofone unit being on is given by a linear model (logistic regression) from the values ofthe other units.The Boltzmann machine becomes more powerful when not all the variables areobserved. In this case, the latent variables can act similarly to hidden units in amultilayer perceptron and model higher-order interactions among the visible units.Just as the addition of hidden units to convert logistic regression into an MLP resultsin the MLP being a universal approximator of functions, a Boltzmann machinewith hidden units is no longer limited to modeling linear relationships betweenvariables. Instead, the Boltzmann machine becomes a universal approximator ofprobability mass functions over discrete variables (Le Roux and Bengio, 2008).Formally, we decompose the unitsxinto two subsets: the visible unitsvandthe latent (or hidden) units h. The energy function becomesE(v, h) = −vRv − vW h − hSh −bv − ch. (20.3)Boltzmann Machine LearningLearning algorithms for Boltzmann machinesare usually based on maximum likelihood. All Boltzmann machines have anintractable partition function, so the maximum likelihood gradient must be ap-proximated using the techniques described in chapter 18.One interesting property of Boltzmann machines when trained with learningrules based on maximum likelihood is that the update for a particular weightconnecting two units depends only on the statistics of those two units, collectedunder diﬀerent distributions:Pmodel(v) andˆPdata(v)Pmodel(h | v). The rest of the652
CHAPTER 20. DEEP GENERATIVE MODELSnetwork participates in shaping those statistics, but the weight can be updatedwithout knowing anything about the rest of the network or how those statistics wereproduced. This means that the learning rule is “local,” which makes Boltzmannmachine learning somewhat biologically plausible. It is conceivable that if eachneuron were a random variable in a Boltzmann machine, then the axons anddendrites connecting two random variables could learn only by observing the ﬁringpattern of the cells that they actually physically touch. In particular, in thepositive phase, two units that frequently activate together have their connectionstrengthened. This is an example of a Hebbian learning rule (Hebb, 1949) oftensummarized with the mnemonic “ﬁre together, wire together.” Hebbian learningrules are among the oldest hypothesized explanations for learning in biologicalsystems and remain relevant today (Giudice et al., 2009).Other learning algorithms that use more information than local statistics seemto require us to hypothesize the existence of more machinery than this. Forexample, for the brain to implement back-propagation in a multilayer perceptron,it seems necessary for the brain to maintain a secondary communication networkfor transmitting gradient information backward through the network. Proposals forbiologically plausible implementations (and approximations) of back-propagationhave been made (Hinton, 2007a; Bengio, 2015) but remain to be validated, andBengio (2015) links back-propagation of gradients to inference in energy-basedmodels similar to the Boltzmann machine (but with continuous latent variables).The negative phase of Boltzmann machine learning is somewhat harder toexplain from a biological point of view. As argued in section 18.2, dream sleepmay be a form of negative phase sampling. This idea is more speculative though.20.2 Restricted Boltzmann MachinesInvented under the nameharmonium(Smolensky, 1986), restricted Boltzmannmachines are some of the most common building blocks of deep probabilisticmodels. We brieﬂy describe RBMs in section 16.7.1. Here we review the previousinformation and go into more detail. RBMs are undirected probabilistic graphicalmodels containing a layer of observable variables and a single layer of latentvariables. RBMs may be stacked (one on top of the other) to form deeper models.See ﬁgure 20.1 for some examples. In particular, ﬁgure 20.1a shows the graphstructure of the RBM itself. It is a bipartite graph, with no connections permittedbetween any variables in the observed layer or between any units in the latentlayer.We begin with the binary version of the restricted Boltzmann machine, but as653
CHAPTER 20. DEEP GENERATIVE MODELSh1h1h2h2h3h3v1v1v2v2v3v3h4h4h(1)1h(1)1h(1)2h(1)2h(1)3h(1)3v1v1v2v2v3v3h(2)1h(2)1h(2)2h(2)2h(2)3h(2)3h(1)4h(1)4(a) (b)h(1)1h(1)1h(1)2h(1)2h(1)3h(1)3v1v1v2v2v3v3h(2)1h(2)1h(2)2h(2)2h(2)3h(2)3h(1)4h(1)4(c)Figure 20.1: Examples of models that may be built with restricted Boltzmann machines.(a) The restricted Boltzmann machine itself is an undirected graphical model based ona bipartite graph, with visible units in one part of the graph and hidden units in theother part. There are no connections among the visible units, nor any connections amongthe hidden units. Typically every visible unit is connected to every hidden unit, but itis possible to construct sparsely connected RBMs such as convolutional RBMs. (b) Adeep belief network is a hybrid graphical model involving both directed and undirectedconnections. Like an RBM, it has no intralayer connections. However, a DBN has multiplehidden layers, and thus connections between hidden units that are in separate layers.All the local conditional probability distributions needed by the deep belief network arecopied directly from the local conditional probability distributions of its constituent RBMs.Alternatively, we could also represent the deep belief network with a completely undirectedgraph, but it would need intralayer connections to capture the dependencies betweenparents. (c) A deep Boltzmann machine is an undirected graphical model with severallayers of latent variables. Like RBMs and DBNs, DBMs lack intralayer connections.DBMs are less closely tied to RBMs than DBNs are. When initializing a DBM from astack of RBMs, it is necessary to modify the RBM parameters slightly. Some kinds ofDBMs may be trained without ﬁrst training a set of RBMs.654
