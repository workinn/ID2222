Chapter 4
Creation of Knowledge Graphs
Anastasia Dimou(B)
Department of Electronics and Information Systems,
Ghent University, Ghent, Belgium
anastasia.dimou@ugent.be
Abstract. This chapter introduces how Knowledge Graphs are generated. The goal is to gain an overview of different approaches that were
proposed and find out more details about the current prevalent ones.
After reading this chapter, the reader should have an understanding
of the different solutions available to generate Knowledge Graphs and
should be able to choose the mapping language that best suits a certain
use case.
1 Introduction
The real power of the Semantic Web will be realized once a significant number of
software agents requiring information from different heterogeneous data sources
become available. However, human and machine agents still have limited ability
to interact with heterogeneous data as most data is not available in the form
of knowledge graphs, which are the fundamental cornerstone of the Semantic
Web. They have different structures (e.g., tabular, hierarchical), appear in heterogeneous formats (e.g., CSV, XML, JSON) and are accessed via heterogeneous
interfaces (e.g., database interfaces or Web APIs).
Therefore, different approaches were proposed to generate knowledge graphs
from existing data. In the beginning, custom implementations were proposed
[67,292] and they remain prevalent today [71,177]; however, more generic
approaches emerged as well. Such approaches were originally focused on
data with specific formats, namely dedicated approaches for, e.g., relational
databases [93], data in Excel (e.g. [274]), or in XML format (e.g. [272]). However, data owners who hold data in different formats need to learn and maintain
several tools [111].
To deal with this, different approaches were proposed for integrating heterogeneous data sources while generating knowledge graphs. Those approaches
follow different directions, but detaching the rules definition from their execution
prevailed, because they render the rules interoperable between implementations,
whilst the systems that process those rules are use-case independent. To generate
knowledge graphs, on the one hand, dedicated mapping languages were proposed,
e.g., RML [111], and, on the other hand, existing languages for other tasks were
repurposed as mapping languages, e.g., SPARQL-Generate [278].
c The Author(s) 2020
V. Janev et al. (Eds.): Knowledge Graphs and Big Data Processing, LNCS 12072, pp. 59–72, 2020.
https://doi.org/10.1007/978-3-030-53199-7_4
60 A. Dimou
We focus on dedicated mapping languages. The most prevalent dedicated
mapping languages are extensions of R2RML [97], the W3C recommendation
on knowledge graph generation from relational databases. RML was the first
language proposed as an extension of R2RML, but there are more alternative approaches and extensions beyond the originally proposed language. For
instance, xR2RML [305], for generating knowledge graphs from heterogeneous
databases, and KR2RML [407], for generating knowledge graphs from heterogeneous data.
In the remainder of this chapter, we introduce the Relational to RDF Mapping Language (R2RML) [97] and the RDF Mapping Language (RML) [111]
which was the first mapping language extending R2RML to support other heterogeneous formats. Then we discuss other mapping languages which extended
or complemented R2RML and RML, or their combination.
2 R2RML
The Relational to RDF Mapping Language (R2RML) [97] is the W3C recommendation to express customized mapping rules from data in relational databases to
generate knowledge graphs represented using the Resource Description Framework (RDF) [94]. R2RML considers any custom target semantic schema which
might be a combination of vocabularies. The R2RML vocabulary namespace is
http://www.w3.org/ns/r2rml# and the preferred prefix is r2rml.
In R2RML, RDF triples are generated from the original data in the relational
database based on one or more Triples Maps (rr:TriplesMap, Listing 4.1, line 3).
Each Triples Map refers to a Logical Table (rr:LogicalTable, line 4), specified
by its table name (rr:tableName). A Logical Table (rr:LogicalTable) is either
a SQL base table or view, or an R2RML view. An R2RML view is a logical table
whose contents are the result of executing a SQL query against the input database.
The SQL query result is used to generate the RDF triples (Table 1).
Table 1. Results of female pole vault for 2019 world championship
Rank Name Nationality Mark Notes
1 Anzhelika Sidorova Authorized Neutral Athlete 4.95 WL,PB
2 Sandi Morris United States (USA) 4.90 SB
3 Katerina Stefanidi Greece 4.85 SB
4 Holly Bradshaw Great Britain 4.80
5 Alysha Newman Canada 4.80
6 Angelica Bengtsson Sweden 4.80 NR
1 @prefix rr: <http://www.w3.org/ns/r2rml#>.
2
3 <#FemalePoleVault> rr:logicalTable <#PoleVaultersDBtable> .
4 <#PoleVaultersDBtable> rr:tableName "femalePoleVaulters" .
Listing 4.1. A Triples Map refers to a Logical Table specified by its name
Chapter 4 Creation of Knowledge Graphs 61
A Triples Map defines how an RDF triple is generated. It consists of three
parts: (i) one Logical Table (rr:LogicalTable, Listing 4.1), (ii) one Subject Map
(rr:SubjectMap, Listing 4.2, line 2), and (iii) zero or more Predicate-Object Maps
(rr:PredicateObjectMap, Listing 4.2, lines 3 and 4).
1 # Triples Map
2 <#FemalePoleVault> rr:subjectMap <#Person_SM> ;
3 rr:predicateObjectMap <#Mark_POM> ;
4 rr:predicateObjectMap <#Nationality_POM> .
Listing 4.2. A Triples Map consists of one Logical Table one Subject Map and zero
or more Predicate Object Maps
The Subject Map (rr:SubjectMap, Listing 4.3, line 2) defines how unique
identifiers, using IRIs [118] or blank nodes, are generated. The RDF term generated from the Subject Map constitutes the subject of all RDF triples generated
from the Triples Map that the Subject Map is related to.
A Predicate-Object Map (rr:PredicateObjectMap, Listing 4.3, lines 5 and
10) consists of (i) one or more Predicate Maps (rr:PredicateMap, line 5), and
(ii) one or more Object Maps (rr:ObjectMap, line 6) or Referencing Object Maps
(rr:ReferencingObjectMap, line 11).
1 # Subject Map
2 <#Person_SM>. rr:template "http:://ex.com/person/{name}"
3
4 # Predicate Object Map with Object Map
5 <#Mark_POM> rr:predicate ex:score ;
6 rr:objectMap [ rr:column "Mark" ;
7 rr:language "en" ] .
8
9 # Predicate Object Map with Referencing Object Map
10 <#Nationality_POM> rr:predicateMap <#Country_PM> ;
11 rr:objectMap <#Country_ROM> ;
Listing 4.3. A Predicate Object Map consists of one or more Predicate Maps and one
or more Object Maps or Referencing Object Maps
A Predicate Map (rr:PredicateMap, Listing 4.3, lines 5 and 10) is a Term
Map (rr:TermMap) defining how a triple’s predicate is generated. An Object Map
(rr:ObjectMap, line 6) or Referencing Object Map (rr:ReferencingObjectMap,
Listing 4.4, line 11) defines how a triple’s object is generated.
A Referencing Object Map defines how the object is generated based on
the Subject Map of another Triples Map. If the Triples Maps refer to different
Logical Tables, a join between the Logical Tables is required. The join condition
(rr:joinCondition, Listing 4.4, line 3) performs joins as joins are executed in
SQL. The join condition consists of a reference to a column name that exists in
the Logical Table of the Triples Map that contains the Referencing Object Map
(rr:child, line 4) and a reference to a column name that exists in the Logical
Table of the Referencing Object Map’s Parent Triples Map (rr:parent, line 5).
62 A. Dimou
1 # Referencing Object Map
2 <#Country_ROM> rr:parentTriplesMap <#Country_TM> ;
3 rr:join [
4 rr:cild "nationality" ;
5 rr:parent "country_name"] .
6
7 <#Country_TM> rr:logicalTable [ rr:tableName "country" ];
8 rr:subjectMap rr:template "http://ex.com/country/{country_name}" .
Listing 4.4. A Referencing Object Map generates an object based on the Subject Map
of another Triples Map
A Term Map (rr:TermMap) defines how an RDF term (an IRI, a blank node,
or a literal) is generated and it can be constant-, column- or template-valued.
A constant-valued Term Map (rr:constant, Listing 4.3, line 5) always generates the same RDF term which is by default an IRI.
A column-valued term map (rr:column, Listing 4.3, line 6) generates a literal by default that is a column in a given Logical Table’s row. The language
(rr:language, line 7) and datatype (rr:datatype) may be optionally defined.
A template-valued Term Map (rr:template, Listing 4.3, line 8) is a valid
string template containing referenced columns and generates an IRI by default.
If the default termtype is desired to be changed, the term type (rr:termType)
needs to be defined explicitly (rr:IRI, rr:Literal, rr:BlankNode).
3 RML
The RDF Mapping Language (RML) [110,111] expresses customized mapping
rules from heterogeneous data structures, formats and serializations to RDF.
RML is a superset of R2RML, aiming to extend its applicability and broaden its
scope, adding support for heterogeneous data. RML keeps the mapping rules as
in R2RML but excludes its database-specific references from the core model. This
way, the input data that is limited to a certain database in R2RML (because
each R2RML processor may be associated to only one database), becomes a
broad set of one or more input data sources in RML.
RML provides a generic way of defining mapping rules referring to different
data structures, combined with case-specific extensions, but remains backwards
compatible with R2RML, as relational databases form such a specific case. RML
enables mapping rules defining how a knowledge graph is generated from a set of
sources that altogether describe a certain domain, can be defined in a combined
and uniform way. The mapping rules may be re-used across different sources
describing the same domain to incrementally form well-integrated datasets.
The RML vocabulary namespace is http://semweb.mmlab.be/ns/rml# and
the preferred prefix is rml.
In the remainder of this subsection, we will talk in more details about data
retrieval and transformations in RML, as well as other representations of RML.
3.1 Data Retrieval
Data can originally (i) reside on diverse locations, e.g., files or databases
on the local network, or published on the Web; (ii) be accessed using different
Chapter 4 Creation of Knowledge Graphs 63
interfaces, e.g., raw files, database connectivity for databases, or different interfaces from the Web such as Web APIs; and (iii) have heterogeneous structures and formats, e.g., tabular, such as databases or CSV files, hierarchical,
such as XML or JSON format, or semi-structured, such as HTML.
In this section, we explain how RML performs the retrieval and extraction
steps required to obtain the data whose semantic representation is desired.
Logical Source. RML’s Logical Source (rml:LogicalSource, Listing 4.5) extends
R2RML’s Logical Table and determines the data source with the data to generate the knowledge graph. The R2RML Logical Table definition determines a
database table, using the Table Name (rr:tableName). In the case of RML,
a broader reference to any input source is required. Thus, the Logical Source
(rml:source) is introduced to specify the source with the original data.
For instance, if the data about countries were in an XML file, instead of a
Logical Table, we would have a Logical Source <#PoleVaultersXML> (Listing 4.5,
line 3):
1 @prefix rml: <http://semweb.mmlab.be/ns/rml#>.
2
3 <#Countries> rml:logicalSource <#CountriesXML> ;
4 <#CountriesXML> rml:source <http://rml.io/data/lambda/countries.xml> .
Listing 4.5. A Triples Map refers to a Logical Source whose data is in XML format
The countries data can then be in XML format as below:
1 <countries>
2 <country continent="Europe">
3 <country_abb>GR</country_abb>
4 <country_name country_language="en">Greece</country_name>
5 <country_name country_language="nl">Griekenland</country_name>
6 </country>
7 <country continent="Europe">
8 <country_abb>UK</country_abb>
9 <country_name country_language="en">United Kingdom</country_name>
10 <country_name country_language="nl">Verenigd Koninkrijk</country_name>
11 </country>
12 <country continent="America">
13 <country_abb>CA</country_abb>
14 <country_name country_language="en">Canada</country_name>
15 <country_name country_language="nl">Canada</country_name>
16 </country>
17 ...
18 </countries>
Listing 4.6. Country data in XML format
Reference Formulation. RML deals with different data serialisations which use
different ways to refer to data fractions. Thus, a dedicated way of referring to
the data’s fractions is considered, while the mapping definitions that define how
the RDF terms and triples are generated remain generic. RML considers that
any reference to the Logical Source should be defined in a form relevant to the
input data, e.g. XPath for XML data or JSONPath for JSON data. To this end,
the Reference Formulation (rml:referenceFormulation) declaration is introduced
(Listing 4.7, line 4), indicating the formulation (for instance, a standard, query
language or grammar) used to refer to its data.
64 A. Dimou
1 @prefix rml: <http://semweb.mmlab.be/ns/rml#>.
2
3 <#Countries> rml:logicalSource <#CountriesXML> .
4 <#CountriesXML> rml:referenceFormulation ql:XPath .
5 <#CountriesXML> rml:iterator "/countries/country" .
Listing 4.7. A Logical Source specifies its Reference Formulation and iterator
Iterator. While in R2RML it is already known that a per-row iteration occurs, as
RML remains generic, the iteration pattern, if any, cannot always be implicitly
assumed, but it needs to be determined. Thereafter, the iterator (rml:iterator)
is introduced (Listing 4.7, line 5). The iterator determines the iteration pattern
over the data source and specifies the extract of the data during each iteration.
The iterator is not required to be explicitly mentioned in the case of tabular
data sources, as the default per-row iteration is implied.
Source. Data can originally reside on diverse, distributed locations and be
accessed using different access interfaces [112]. Data can reside locally, e.g., in
files or in a database at the local network, or can be published on the Web. Data
can be accessed using diverse interfaces. For instance, metadata may describe
how to access the data, such as dataset’s metadata descriptions in the case of
data catalogues, or dedicated access interfaces might be needed to retrieve data
from a repository, such as database connectivity for databases, or different Web
interfaces, such as Web APIs.
RML considers an original data source, but the way this input is retrieved
remains out of scope, in the same way it remains out of scope for R2RML how
the SQL connection is established. Corresponding vocabularies can describe how
to access the data, for instance the dataset’s metadata (Listing 4.8), hypermediadriven Web APIs or services, SPARQL services, and database connectivity
frameworks (Listing 4.9) [112].
1 <#FemalePoleVault> rr:logicalTable <#PoleVaultersCSVtable> ;
2 <#PoleVaultersCSVtable> rml:source <#CSVW_source> .
3
4 <#CSVW_source> a csvw:Table;
5 csvw:url "femalePoleVaulters.csv" ;
6 csvw:dialect [ a csvw:Dialect; csvw:delimiter ";" ] .
Listing 4.8. A CSV file on the Web as RML Data Source
1 <#FemalePoleVault> rr:logicalTable <#PoleVaultersDBtable> ;
2 <#PoleVaultersDBtable> rml:source <#DB_source>;
3 rr:sqlVersion rr:SQL2008;
4 rr:tableName "femalePoleVaulters" .
5
6 <#DB_source> a d2rq:Database;
7 d2rq:jdbcDSN "CONNECTIONDSN";
8 d2rq:jdbcDriver "com.mysql.cj.jdbc.Driver";
9 d2rq:username "root";
10 d2rq:password "" .
Listing 4.9. A table as RML Data Source
Chapter 4 Creation of Knowledge Graphs 65
Logical Reference. According to R2RML, a column-valued or template-valued
term map is defined as referring to a column name. In the case of RML, a more
generic notion is introduced, the logical reference. Its value must be a valid
reference to the data of the input dataset according to the specified reference
formulation. Thus, the reference’s value should be a valid expression according
to the Reference Formulation defined at the Logical Source.
1 # Predicate Object Map with Object Map
2 <#CountryName_POM> rr:predicate ex:name ;
3 rr:objectMap [
4 rml:reference "country_name" ;
5 rml:languageMap [ rml:reference "@country_language"] ] .
Listing 4.10. An Object Map in RML with a reference to data according to the
Reference Formulation and a language Map to define the language.
RDF Term Maps are instantiated with data fractions referred to using a
reference formulation relevant to the corresponding data format. Those fractions
are derived from data extracted at a certain iteration from a Logical Source. Such
a Logical Source is formed by data retrieved from a repository accessed as defined
by the corresponding dataset or service description vocabulary.
Language Map. RML introduces a new Term Map for defining the language, the
Language Map (rml:LanguageMap, Listing 4.10, line 5), which extends R2RML’s
language tag (rr:language). The Language Map allows not only constant values
for language but also references derived from the input data. rr:language is
considered then an abbreviation for the rml:languageMap, as rr:predicate is
for the rr:predicateMap.
3.2 Data Transformations: FnO
Mapping rules involve (re-)modeling the original data, describing how objects are
related by specifying correspondences between data in different schemas [126],
and deciding which vocabularies and ontologies to use. Data transformations, as
opposed to schema transformations that the mapping rules represent, are needed
to support any changes in the structure, representation or content of data [367],
for instance, performing string transformations or computations.
The Function Ontology (FnO) [102,104] describes functions uniformly, unambiguously, and independently of the technology that implements them. As RML
extends R2RML with respect to schema transformations, the combination of
RML with FnO extends R2RML with respect to data transformations.
A function (fno:Function) is an activity which has input parameters, output, and implements certain algorithm(s) (Listing 4.11, line 1). A parameter
(fno:Parameter) is a function’s input value (Listing 4.11, line 4). An output
(fno:Output) is the function’s output value (Listing 4.11, 5). An execution
(fno:Execution) assigns values to the parameters of a function for a certain execution. An implementation (fno:Implementation) defines the internal workings
of one or more functions.
66 A. Dimou
1 grel:string_split a fno:Function;
2 fno:name "split";
3 dcterms:description "split";
4 fno:expects (grel:string_s grel:string_sep);
5 fno:returns (grel:output_array).
Listing 4.11. A function described in FnO that splits a string
The Function Map (fnml:FunctionMap) is another Term Map, introduced
as an extension of RML, to facilitate the alignment of the two, RML and FnO.
A Function Map is generated by executing a function instead of using a constant
or a reference to the raw data values. Once the function is executed, its output
value is the term generated by this Function Map. The fnml:functionValue
property indicates which instance of a function needs to be executed to generate
an output and considering which values.
1 <#FemalePoleVault> rr:predicateObjectMap [
2 rr:predicate ex:record;
3 rr:objectMap [
4 fnml:functionValue [
5 rr:predicateObjectMap [
6 rr:predicate fno:executes ;
7 rr:objectMap [ rr:constant grel:split ] ] ;
8 rr:predicateObjectMap [
9 rr:predicate grel:string_s ;
10 rr:objectMap [ rml:reference "notes" ] ] ;
11 rr:predicateObjectMap [
12 rr:predicate grel:string_sep ;
13 rr:objectMap [ rr:constant "," ] ] ] ].
Listing 4.12. A Function Map aligns FnO with RML
3.3 Other Representations: YARRRML
YARRRML [103,196] is a human readable text-based representation for mapping rules. It is expressed in YAML [46], a widely used human-friendly data
serialization language. YARRRML can be used with both R2RML and RML.
A mapping (Listing 4.13, line 1) contains all definitions that state how subjects, predicates, and objects are generated. Each mapping definition is a keyvalue pair. The key sources (line 3) defines the set of data sources that are used
to generate the entities. Each source is added to this collection via a key-value
pair. The value is a collection with three keys: (i) the key access defines the local
or remote location of the data source; (ii) the key reference formulation defines
the reference formulation used to access the data source; and (iii) the key iterator
(conditionally required) defines the path to the different records over which to
iterate. The key subjects (line 5) defines how the subjects are generated. The key
predicateobjects (line 6) defines how combinations of predicates and objects are
generated. Below the countries example (Listing 4.6) is shown in YARRRML:
Chapter 4 Creation of Knowledge Graphs 67
1 mappings:
2 country:
3 sources:
4 - ['countries.xml~xpath', '/countries/country']
5 s: http://ex.com/$(country_abb)
6 po:
7 - [ex:name, $(country_name)]
8 - [ex:abbreviation, $(country_abb)]
Listing 4.13. A YARRRML set of mapping rules
4 [R2]RML Extensions and Alternatives
Other languages were proposed based on differentiation on (i) data retrieval
and (ii) data transformations. The table below (Table 2) summarizes the
mapping languages extensions, their prefixes and URIs. xR2RML [306] and
KR2RML [407] are the two most prominent solutions that showcase extensions and alternatives respectively for data retrieval. On the one hand, xR2RML
extends R2RML following the RML paradigm to support heterogeneous data
from non-relational databases. On the other hand, KR2RML extends R2RML
relying on the Nested Relational Model (NRM) [455] as an intermediate form
to represent data originally stored in relational databases. KR2RML also provided an alternative for data transformations, but FunUL is the most prominent
alternative to FnO.
Table 2. [R2]RML extensions, their URIs and prefixes
Language Prefix URI
R2RML rr http://www.w3.org/ns/r2rml#
RML rml http://semweb.mmlab.be/ns/rml#
xR2RML xrr http://www.i3s.unice.fr/ns/xr2rml#
FnO+RML fnml http://semweb.mmlab.be/ns/fnml#
FnO fno https://w3id.org/function/ontology#
4.1 XR2RML
xR2RML [306] was proposed in 2014 in the intersection of R2RML and RML.
xR2RML extends R2RML beyond relational databases and RML to include nonrelational databases. xR2RML extends R2RML following the RML paradigm
but is specialized for non-relational databases and, in particular, NoSQL and
XML databases. NoSQL systems have heterogeneous data models (e.g., keyvalue, document, extensible column, or graph store), as opposed to relational
databases. xR2RML assumes, as R2RML does, that a processor executing the
rules is connected to a certain database. How the connection or authentication
is established against the database is out of the language’s scope, as in R2RML.
The xR2RML vocabulary preferred prefix is xrr and the namespace is the
following: http://www.i3s.unice.fr/ns/xr2rml#.
68 A. Dimou
Data Source. Similarly to RML, an xR2RML Triples Map refers to a Logical Source (xrr:logicalSource, Listing 4.14, line 3), but similarly to R2RML,
this Logical Source can be either an xR2RML base table (xrr:sourceName, for
databases where tables exist) or an xR2RML view representing the results of
executing a query against the input database (xrr:query, line 4).
1 @prefix xrr: <http://www.i3s.unice.fr/ns/xr2rml#> .
2
3 <#CountriesXML> xrr:logicalSource [
4 xrr:query """for $i in ///countries/country return $i; """;
5 rml:iterator "//countries/country";];
6 <#CountryName_POM> rr:predicate ex:name ;
7 rr:objectMap [ xrr:reference "country_name"] .
Listing 4.14. xR2RML logical source over an XML database supporting XQuery
Iterator. xR2RML originally introduced the xrr:iterator, according to the
rml:iterator, to iterate over the results. In a later version, xR2RML converged
using the rml:iterator (Listing 4.14, line 5).
Format or Reference Formulation. In contrast to RML that considers a formulation (rml:referenceFormulation) to refer to its input data, xR2RML originally specified explicitly the format of data retrieved from the database using the
property xrr:format (Listing 4.15, line 2). For instance, RML considers XPath
or XQuery or any other formulation to refer to data in XML format, xR2RML
would refer to the format, e.g. xrr:XML. While RML allows for other kinds of
query languages to be introduced, xR2RML decides exactly which query language to use. In an effort to converge with RML, xR2RML considers optionally
a reference formulation.
1 <#FemalePoleVault> xrr:logicalSource <#PoleVaultersCSVtable> ;
2 <#PoleVaultersCSVtable> xrr:format xrr:Row .
Listing 4.15. A CSV file on the Web as xR2RML Logical Source
Reference. Similar to RML, xR2RML uses a reference (xrr:reference) to refer
to the data elements (Listing 4.14, line 7). xR2RML extends RML’s reference
to refer to data elements in data with mixed formats. xR2RML considers cases
where different formats are nested; for instance, a JSON extract is embedded in
a cell of a tabular structure. A path with mixed syntax consists of the concatenation of several path expressions separated by the slash ‘/’ character.
Collections and Containers. Several RDF terms can be generated by a Term
Map during an iteration if multiple values are returned. This can normally generate several triples, but it can also generate hierarchical values in the form of
RDF collections or containers. To achieve the latter, xR2RML extends R2RML
by introducing corresponding datatypes to support the generation of containers. xR2RML introduces new term types (rr:termType): xrr:RdfList for an
rdf:List, xrr:RdfBag for rdf:Bag, xrr:RdfSeq for rdf:Seq and xrr:RdfAlt
for rdf:Alt. All RDF terms produced by the Object Map during one triples
Chapter 4 Creation of Knowledge Graphs 69
map iteration step are then grouped as members of one term. To achieve this,
two more constructs are introduced: Nested Term Maps and Push Downs.
1 <#Countries> rr:predicateObjectMap [
2 rr:predicate ex:name;
3 rr:objectMap [
4 xrr:reference "country_name";
5 rr:termType xrr:RdfList;
6 xrr:pushDown [ xrr:reference "@continent"; xrr:as "continent" ];
7 xrr:nestedTermMap [
8 rr:template "{continent}: {country_name}" ;
9 rr:termType rr:Literal ;
10 rr:dataType xsd:string ] ].
Listing 4.16. An xrr:RdfList in xR2RML
Nested Term Map. A Nested Term Map (xrr:NestedTermMap, Listing 4.16, line 7)
accepts the same properties as a Term Map and can be used to specify a term type,
a language tag or a data type for the members of the generated RDF collection or
container.
Push Down. Within an iteration, it may be needed to access data elements higher
in the hierarchical documents in the context of hierarchical data formats, such
as XML or JSON. To deal with this, xR2RML introduces the xrr:pushDown
property (Listing 4.16, line 6).
4.2 KR2RML
KR2RML [407] extends R2RML in a different way than xR2RML. KR2RML
relies on the Nested Relational Model (NRM) as an intermediate form to represent data. The data is mapped into tables by translating it into tables and rows
where a column in a table can be either a scalar value or a nested table. Besides
the data retrieval part, KR2RML extends R2RML with data transformations
using User Defined Functions (UDFs) written in Python.
Data Source. Mapping tabular data (e.g., CSV) into the Nested Relational Model
is straightforward. The model has a one-to-one mapping of tables, rows, and
columns, unless a transformation like splitting on a column occurs, which will
create a new column that contains a nested table.
Mapping hierarchical data (e.g., JSON, XML) into the Nested Relational
Model requires a translation algorithm for each data format next to the mapping
language. Such an algorithm is considered for data in XML and JSON format. If
the data is in JSON, an object maps to a single row table in NRM with a column
for each field. Each column is populated with the value of the appropriate field.
Fields with scalar values do not need translation, but fields with array values
are translated to their own nested tables: if the array contains scalar or object
values, each array element becomes a row in the nested table. If the elements
are scalar values like strings as in the tags field, a default column name “values”
is provided. If a JSON document contains a JSON array at the top level, each
element is treated like a row in a database table. If the data is in XML format,
70 A. Dimou
its elements are treated like JSON objects, and its attributes and repeated child
elements as single-row nested table where each attribute is a column.
References. The column-valued term map is not limited to SQL identifiers as it
occurs in R2RML to support mapping nested columns in the NRM. A JSON
array is used to capture the column names that make up the path to a nested
column from the document root. The template-valued term map is also extended
to include columns that do not exist in the original input but are the result of
the transformations applied by the processor.
Joins. Joins are not supported because they are considered to be impractical
and require extensive planning and external support.
Execution Planning. A tag (km-dev:hasWorksheetHistory) is introduced to
capture the cleaning, transformation and modeling steps.
Data Transformations. The Nested Transformation Model can also be used to
embed transformation functions. A transformation function can create a new set
of nested tables instead of transforming the data values.
4.3 FunUL
FunUL [232] is an alternative to FnO for data transformations. FunUL allows
the definition of functions as part of the mapping language. In FunUL, functions
have a name and a body. The name needs to be unique. The body defines the
function using a standardized programming language. It has a return statement
and a call refers to a function with an optional set of parameters.
The FunUL vocabulary namespace is http://kdeg.scss.tcd.ie/ns/rrf# and the
preferred prefix is rrf.
The class rrf:Function defines a function (Listing 4.17, line 3). A function
definition has two properties defining the name (rrf:functionName, line 4), and
the function body (rrf:functionBody, line 5).
A function can be called using the property rrf:functionCall (Listing 4.17,
line 13). This property refers to a rrf:Function with the property rr:function
(line 14). Parameters are defined using rrf:parameterBindings (line 15).
1 @prefix rrf: <http://kdeg.scss.tcd.ie/ns/rrf#> .
2
3 <#SplitTransformation> a rrf:Function ;
4 rrf:functionName "splitTransformation" ;
5 rrf:functionBody
6 """function split(value, separator) {
7 str = value.split(separator).trim();
8 return str; ""; } """ ; .
9
10 <#FemalePoleVault> rr:predicateObjectMap [
11 rr:predicate ex:record;
12 rr:objectMap [
13 rrf:functionCall [
14 rrf:function <#SplitTransformation> ;
15 rrf:parameterBindings (
Chapter 4 Creation of Knowledge Graphs 71
16 [ rml:reference "notes" ]
17 [ rml:reference "," ] ); ];
Listing 4.17. A Function Call aligns FunUL with RML
5 Conclusions
A lack of in-depth understanding of the complexity of generating knowledge
graphs and the many degrees of freedom in modeling and representing knowledge
prevents human and software agents from profiting of the Semantic Web potential. This chapter identified the different approaches that were proposed in recent
years for generating knowledge graphs from heterogeneous data sources. Then,
the chapter focused on approaches that distinguish mapping rules definition
from their execution. Two types of mapping languages prevailed, dedicated mapping languages and repurposed mapping languages. The chapter further focused
on dedicated mapping languages because they follow the W3C-recommended
R2RML.
This chapter presents the author’s view on knowledge graph generation. It
serves as an introductory chapter to knowledge graphs, which are considered in
greater detail in the following chapters. The next two chapters will explain how
to perform federated querying and reasoning over knowledge graphs (Table 3).
Table 3. Mapping Languages comparison with respect to data retrieval
R2RML RML xR2RML KR2RML
Extends – R2RML R2RML & RML R2RML
Data source rr:LogicalTable rml:LogicalSource xrr:LogicalSource rr:LogicalTable
Data
references
– Reference formulation xrr:format –
Reference rr:column
rr:template
rml:reference
rr:template
xrr:reference
rr:template
rr:column rr:template
Reference
formulation
SQL SQL/XPath/
JSONPath acc.
Reference formulation
SQL/XPath/
JSONPath
acc. xrr:format
SQL/XPath/ JSONPath
Join rr:join rr:join (extended) rr:join (extended) Not supported
Declarative
iterator
No Yes Yes No
Iterator – rml:iterator xrr:iterator –
Query rr:sqlQuery rml:query xrr:query rr:sqlQuery
Lists – – xrr:RdfList –
72 A. Dimou
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.
Chapter 5
Federated Query Processing
Kemele M. Endris1,2(B) , Maria-Esther Vidal1 , and Damien Graux3
1 TIB Leibniz Information Centre For Science and Technology, Hannover, Germany 2 L3S Research Center, Hannover, Germany
endris@L3S.de 3 ADAPT SFI Research Centre, Trinity College, Dublin, Ireland
Abstract. Big data plays a relevant role in promoting both manufacturing and scientific development through industrial digitization and emerging interdisciplinary research. Semantic web technologies have also experienced great progress, and scientific communities and practitioners have
contributed to the problem of big data management with ontological models, controlled vocabularies, linked datasets, data models, query languages,
as well as tools for transforming big data into knowledge from which decisions can be made. Despite the significant impact of big data and semantic web technologies, we are entering into a new era where domains like
genomics are projected to grow very rapidly in the next decade. In this
next era, integrating big data demands novel and scalable tools for enabling
not only big data ingestion and curation but also efficient large-scale exploration and discovery. Federated query processing techniques provide a solution to scale up to large volumes of data distributed across multiple data
sources. Federated query processing techniques resort to source descriptions to identify relevant data sources for a query, as well as to find efficient
execution plans that minimize the total execution time of a query and maximize the completeness of the answers. This chapter summarizes the main
characteristics of a federated query engine, reviews the current state of the
field, and outlines the problems that still remain open and represent grand
challenges for the area.
1 Introduction
The number and variety of data collections have grown exponentially over recent
decades and a similar growth rate is expected in the coming years. In order to
transform the enormous amount of disparate data into knowledge from where
actions can be taken, fundamental problems, such as data integration and query
processing, must be solved. Data integration requires the effective identification of
entities that, albeit described differently, correspond to the same real-world entity.
Moreover, data is usually ingested in myriad unstructured formats and may suffer reduced quality due to biases, ambiguities, and noise. These issues impact on
the complexity of the solutions for data integration. Semantic integration of big
data entails variety by enabling the resolution of several interoperability conflicts
c The Author(s) 2020
V. Janev et al. (Eds.): Knowledge Graphs and Big Data Processing, LNCS 12072, pp. 73–86, 2020.
https://doi.org/10.1007/978-3-030-53199-7_5
74 K. M. Endris et al.
[159,446], e.g., structuredness, schematic, representation, completeness, granularity, and entity matching conflicts. Conflicts arise because data sources may have
different data models or none, follow various schemes for data representation, and
contain complementary information. Furthermore, a real-world entity may be represented using diverse properties or at various levels of detail. Thus, techniques
able to solve interoperability issues while addressing data complexity challenges
imposed by big data characteristics are required [402].
Existing solutions to the problem of query processing over heterogeneous
datasets rely on a unified interface for overcoming interoperability issues, usually based on metamodels [224]. Different approaches have been proposed, mainly
with a focus on data ingestion and metadata extraction and management.
Exemplary approaches include GEMMS [365], PolyWeb [244], BigDAWG [119],
Ontario [125], and Constance [179]. These systems collect metadata about the
main characteristics of the heterogeneous data collections, e.g., formats and
query capabilities. Additionally, they resort to a global ontology to describe
contextual information and relationships among data sets. Rich descriptions of
the properties and capabilities of the data have shown to be crucial for enabling
these systems to effectively perform query processing.
In the context of the Semantic Web, the problem of federated query processing has also been actively studied. As a result, diverse federated SPARQL query
engines have been defined that enable users to execute queries over a federation
of SPARQL endpoints. State-of-the-art techniques include FedX [389], ANAPSID [6], and MULDER [124]. FedX implements adaptive techniques to identify
relevant sources to evaluate a query. It is able to contact SPARQL endpoints
on the fly to decide the subqueries of the original query that can be executed
over the endpoints of the federation. ANAPSID makes use of metadata about the
vocabularies used on the RDF datasets to perform source selection. Based on the
selected sources, ANAPSID decomposes original queries and finds efficient plans
to collect the answers incrementally. Finally, MULDER resorts to description of
the RDF datasets based on the classes and relations of the dataset vocabularies.
MULDER proposes the concept of the RDF Molecule Templates (RDF-MTs)
to describe the datasets and efficiently perform source selection and query planning. The rich repertoire of federated query engines just reveals the importance
of query processing against the RDF dataset, as well as the attention that the
problem has received from the database and semantic web communities.
The contributions of the work are summarized as follows:
– A description of the concept of the data integration system and an analysis
of the different parameters that impact on the complexity of a system.
– A characterization of the challenges addressed by federated query engines and
analysis of the current state of the federated query processing field.
– A discussion of the analysis of the grand challenges in this area and future
directions.
The remainder of the chapter is structured as follows: Sect. 2 presents an
overview of the data integration system and the roles that they play in the
problem of accessing and processing queries over heterogeneous data sources.
Chapter 5 Federated Query Processing 75
Section 3 describes the problem of federated query processing, the main challenges to be addressed by a federated query engine, and the state of the art.
Finally, grand challenges and future directions are outlined in Sect. 4.
2 Data Integration Systems
An enormous amount of data is being published on the web [379]. In addition,
different data sources are being generated and stored within enterprises as well
due to technological advances in data collection, generation, and storage. These
data sources are created independently of each other and might belong to different
administrative entities. Hence, they have different data representation formats as
well as access interfaces. Such properties of the data sources hinder the usage of the
information available in them. Data integration is the process of providing uniform
access to a set of distributed (or decentralised), autonomous, and heterogeneous
data sources [114]. Data integration systems provide a global schema (also known
as mediated schema) to provide a reconciled view of all data available in different
data sources. Mapping between the global schema and source schema should be
established to combine data residing in data sources considered in the integration
process. Generally, data integration system is formally defined as follows [280]:
Definition 1 (Data Integration System). A data integration system, I, is
defined as a triple <G, S, M>, where:
– G is the global schema, expressed in a language LG over an alphabet AG. The
alphabet comprises a symbol for each element of G.
– S is the source schema, expressed in a language LS over an alphabet AS. The
alphabet AS includes a symbol for each element of the sources.
– M is the mapping between G and S, constituted by a set of assertions of
the forms: qS → qG, qG → qS; where qS and qG are two queries of the same
arity, respectively over the source schema S, and over the global schema G. An
assertion specifies the connection between the elements of the global schema
and those of the source schema.
Defining schema mapping is one of the main tasks in a data integration system.
Schema mapping is the specification of correspondences between the data at the
sources and the global schema. The mappings determine how the queries posed
by the user using the global schema are answered by translating to the schema
of the source that stores the data. Two basic approaches for specifying such
mappings have been proposed in the literature for data integration systems are
Global-as-View (GAV) [140,180] and Local-as-View (LAV) [282,433].
Rules defined using the Global-as-View (GAV) approach define concepts in
the global schema as a set of views over the data sources. Using the GAV approach, the mapping rules in M define the concepts of the schema in the sources,
S, with each element in the global schema. A query posed over the global schema,
G, needs to be reformulated by rewriting the query with the views defined in,
M. Such rewriting is also known as query unfolding – the process of rewriting
76 K. M. Endris et al.
the query defined over global schema to a query that only refers to the source
schema. Conceptually, GAV mappings specify directly how to compute tuples
of the global schema relations from tuples in the sources. This characteristics of
GAV mappings makes them easier for query unfolding strategy. However, adding
and removing sources in the GAV approach may involve updating all the mappings in the global schema, which requires knowledge of all the sources. Mappings
specified using the Local-as-View (LAV) approach describe the data sources as
views over the global schema, contrary to the GAV approach that defines the
global schema as views over the data sources. Using the LAV approach, the mapping rules in M associates a query defined over the global schema, G, to each
elements of source schema, S. Adding and removing sources in LAV is easier than
GAV, as data sources are described independently of each other. In addition, it
allows for expressing incomplete information as the global schema represents a
database whose tuples are unknown, i.e., the mapping M defined by LAV approach might not contain all the corresponding sources for all the elements in the
global schema, G. As a result, query answering in LAV may consist of querying
incomplete information, which is computationally more expensive [114].
In this chapter, we define a source description model, RDF Molecule Template
(RDF-MT), an abstract description of entities that share the same characteristics,
based on the GAV approach. The global schema is defined as a consolidation of
RDF-MTs extracted from each data source in the federation. Rule-based mappings, such as RML, are used to define the GAV mappings of heterogeneous data
sources. RDF-MTs are merged based on their semantic descriptions defined by
the ontology, e.g., in RDFS.
2.1 Classification of Data Integration Systems
Data integration systems can be classified with respect to the following three
dimensions: autonomy, distribution, and heterogeneity [338], Fig. 1. Autonomy
dimension characterizes the degree to which the integration system allows each
data source in the integration to operate independently. Data sources have autonomy over choice of their data model, schema, and evolution. Furthermore, sources
Fig. 1. Dimensions of data integration systems
Chapter 5 Federated Query Processing 77
also have autonomy to join or leave the integration system at any time as well
as to select which fragments of data to be accessible by the integration system and its users. Distribution dimension specifies the data that is physically
distributed across computer networks. Such distribution (or decentralization)
can be achieved by controlled distribution or by the autonomous decision of the
data providers. Finally, heterogeneity may occur due to the fact that autonomous
development of systems yields different solutions, for reasons such as different
understanding and modeling of the same real-world concepts, the technical environment, and particular requirements of the application [338]. Though there are
different types of heterogeneity of data sources, the important ones with respect
to data interoperability are related to data model, semantic, and interface heterogeneity. Data model heterogeneity captures the heterogeneity created by various modeling techniques such that each data model has different expressive
power and limitations, e.g., relational tables, property graph, and RDF. Semantic heterogeneity concerns the semantics of data and schema in each source. The
semantics of the data stored in each source are defined through the explicit definition of their meanings in the schema element. Finally, interface heterogeneity
exists if data sources in the integration system are accessible via different query
languages, e.g., SQL, Cypher, SPARQL, and API call.
Fig. 2. Classification of data integration systems
Figure 2 shows different classifications of data integration systems with
respect to distribution and heterogeneity dimensions. The first type of data
integration systems, Fig. 2.(1), loads heterogeneous data from data sources to
a centralized storage after transforming them to a common data representation
format. The second type of data integration systems, Fig. 2.(2), supports data
distributed across networks; however, they only support if the data sources in
78 K. M. Endris et al.
the system are homogeneous in terms of data model and access methods. The
third type of data integration systems, Fig. 2.(3), supports data heterogeneity
among data sources in the integration system. However, these data integration
systems are managed in a centralized way and data is stored in a distributed
file system (DFS), such as Hadoop1. Finally, the fourth type of data integration systems, Fig. 2.(4), supports data distributed across networks as well as
heterogeneity of data sources. Such integration systems utilize special software
components to extract data from the data sources using native query language
and access mechanism. They can also transform data extracted from the sources
to data representation defined by the integration system. Data sources in the
integration system might also be autonomous. Such types of system are different
from the third type by how data is distributed and stored. While the fourth
type supports any storage management, including DFS, the third type of data
integration systems supports only DFS in a centralized way. Mostly the distribution task is handled by the file system. For instance, data might be stored in
a multi-modal data management system or in Data Lake storage based only on
a distributed file system (DFS). In the third type of data integration system,
data is loaded from the original source to the centralized storage for further processing. Federated query processing systems fall in the second and fourth type
of integration system when the data sources are autonomous.
Data integration systems also have to make sure that data that is current (fresh) is accessed and integrated. Especially, for DFS-based Data Lakes,
Fig. 2.(2), and the centralized, Fig. 2.(4), integration systems, updates of the original data sources should be propagated to guarantee the freshness of data. Furthermore, when accessing an original data source from the provider is restricted,
or management of data in a local replica is preferred, integration systems
Fig. 2.(1) and (3), need to guarantee data freshness by propagating changes.
2.2 Data Integration in the Era of Big Data
In the era of big data, a large amount of structured, semi-structured, and unstructured data is being generated at a faster rate than ever before. Big data systems
that integrate different data sources need to handle such characteristics of data
efficiently and effectively. Generally, big data is defined as data whose volume,
acquisition speed, data representation, veracity, and potential value overcome
the capacity of traditional data management systems [77]. Big data is characterized by the 5Vs model: Volume denotes that generation and collection of data are
produced at increasingly big scales. Velocity represents that data is generated
and collected rapidly. Variety indicates heterogeneity in data types, formats,
structuredness, and data generation scale. Veracity refers to noise and quality
issues in the data. Finally, Value denotes the benefit and usefulness that can be
obtained from processing and mining big data.
1 https://hadoop.apache.org/.
Chapter 5 Federated Query Processing 79
There are two data access strategies for data integration: schema-on-write
and schema-on-read. In the schema-on-write strategy, data is cleansed, organized, and transformed according to a pre-defined schema before loading to the
repository. In schema-on-read strategy, raw data is loaded to the repository as-is
and schema is defined only when the data is needed for processing [27]. Data
warehouses provide a common schema and require data cleansing, aggregation,
and transformation in advance, hence, following the schema-on-write strategy.
To provide scalable and flexible data discovery, analysis, and reporting, Data
Lakes have been proposed. Unlike data warehouses, where data is loaded to the
repository after it is transformed to a target schema and data representation,
Data Lakes store data in its original format, i.e., the schema-on-read strategy.
Data Lakes provide a central repository for raw data that is made available
to the user immediately and defer any aggregation or transformation tasks to
the data analysis phase, thus addressing the problem of disconnected information silos, which is the result of non-integrated heterogeneous data sources in
isolated repositories with diverse schema and query languages. Such a central
repository may include different data management systems, such as distributed
file systems, relational database management systems, graph data management
systems, as well as triple stores for specialized data model and storage.
3 Federated Query Processing
A federated query processing system2, provides a unified access interface to a set
of autonomous, distributed, and heterogeneous data sources. While distributed
query processing systems have control over each dataset, federated query processing engines have no control over datasets in the federation and data providers
can join or leave the federation at any time and modify their datasets independently. Query processing in the context of data sources in a federation is more
difficult than in centralized systems because of the different parameters involved
that affect the performance of the query processing engine [114]. Data sources
in a federation might contain fragments of data about an entity, have different
processing capabilities, support different access patterns, access methods, and
operators. The role of a federated query engine is to transform a query expressed
in terms of the global schema, i.e., the federated query, into an equivalent query
expressed in the schema of the data sources, i.e., local query. The local query
represents the actual execution plan of the federated query by the data sources
of the federation. The transformation of the federated query to a local query
needs to be both effective and efficient. Query transformations are effective if
the generated query is equivalent to the original one, i.e., both the original and
the transformed queries produce same results. On the other hand, query transformations are efficient if the execution strategy of the transformed query makes
use of minimum computational resources and communication cost. Producing
2 We use the terms federated query processing system, federated query engine, and
federated query processing system interchangeably.
80 K. M. Endris et al.
Fig. 3. Federated query processing basic components
an efficient execution strategy is difficult as many equivalent and correct transformations can be produced and each equivalent execution strategy leads to
different consumption of resources [338]. The main objective of federated query
processing is to transform a query posed on a federation of data sources into a
query composed of the union of subqueries over individual data sources of the
federation. Further, a query plan is generated in order to speed up the processing
of each individual subquery over the selected sources, as well as the gathering of
the results into the query answer. An important part of query processing in the
context of federated data sources is query optimization as many execution plans
are correct transformations of the same federated query. The one that optimizes
(minimizes) resource consumption should be retained. Query processing performance can be measured by the total cost that will be used in query processing
and the response time of the query, i.e., time elapsed for executing the query.
As an RDF data model continues gaining popularity, publicly available RDF
datasets are growing in number and size. One of the challenges emerging from
this trend is how to efficiently and effectively execute queries over a set of
autonomous RDF datasets. Saleem et al. [380] study federated RDF query
engines with web access interfaces. Based on their survey results, the authors
divide federation approaches into three main categories: Query Federation over
SPARQL endpoints, Query Federation over Linked Data (via URI lookups), and
Query Federation on top of Distributed Hash Tables. Moreover, Acosta et al. [5]
classified federated RDF query processing engines based on the type of data
sources they support into three categories: Federation of SPARQL endpoints,
Federation of RDF Documents, and Federation of Triple Pattern Fragments.
Conceptually, federated query processing involves four main sub-problems
(components): (i) data source description, (ii) query decomposition and source
selection, (iii) query planning and optimization, and (iv) query execution. Federated query engines also include two additional sub-problems: query parsing and
Chapter 5 Federated Query Processing 81
result conciliation. Query parsing and result conciliation sub-problems deal with
syntactic issues of the given query and formatting the results returned from the
query execution, respectively. Below we provide an overview of the data source
description, query decomposition and source selection, query planning and optimization as well as query execution sub-problems.
3.1 Data Source Description
The data source description sub-problem deals with describing the data available
in data sources and managing catalogs about data sources that are participating
in the federation. Data source descriptions encode information about available
data sources in the federation, types of data in each data source, access method
of data sources, and privacy and access policies of these data sources [114]. The
specification of what data exist in data sources and how the terms used in data
sources are related to the global schema are specified by the schema mapping.
Schema mappings also represent privacy and access control restrictions as well
as statistics on the available data in each data source. Federated query engines
rely on the description of data sources in the federation to select relevant sources
that may contribute to answer a query. Data source descriptions are utilized by
source selection, query decomposition, and query optimization sub-problems.
A catalog of data source descriptions can be collected offline or during query
running-time. Based on the employed catalog of source descriptions, SPARQL
federation approaches can be divided into three categories [380]: pre-computed
catalog assisted, on-the-fly catalog assisted, and hybrid (uses both pre-computed
and on-the-fly) solutions. Pre-computed catalog-assisted federated SPARQL
query engines use three types of catalogs: service descriptions, VoID (Vocabulary of Interlinked Datasets) description, and list of predicates [335]. The first
two catalogs are computed and published by the data source providers that
contains descriptions about the set of vocabularies used, a list of classes and
predicates, as well as some statistics about the instances such as number of
triples per predicate, or class. Specifically in VoID descriptions, there is information about external linksets that indicate the existence of owl:sameAs and
other linking properties. The third type of catalog, i.e., a list of predicates, is
generated by contacting the data source endpoints and issuing SPARQL queries
and extracting predicates from the other two types of catalog.
FedX [389] does not require a catalog of source descriptions computed beforehand but uses triple pattern-wise ASK queries sent to data sources at query
time. Triple pattern-wise ASK queries are SPARQL ASK queries which contain
only one triple pattern in the graph expression of the given query. Lusail [4], like
FedX, uses an on-the-fly catalog solution for source selection and decomposition.
Unlike FedX, Lusail takes an additional step to check if pairs of triple patterns
can be evaluated as one subquery over a specific endpoint; this knowledge is
exploited by Lusail during query decomposition and optimization. Posting too
many SPARQL ASK queries can be a burden for data sources that have limited compute resources, which may result in DoS. Pre-computed catalog of data
source descriptions can be used to reduce the number of requests sent to the
82 K. M. Endris et al.
data sources. ANAPSID [6] is a federated query processing engine that employs
a hybrid solution and collects a list of RDF predicates of the triple patterns
that can be answered by the data sources and sends ASK queries when required
during query time. Publicly available dataset metadata are utilized by some
federated query processing engines as catalogs of source descriptions. SPLENDID [160] relies on instance-level metadata available as Vocabulary of Interlinked
Datasets (VoID) [10] for describing the sources in a federation. SPLENDID provides a hybrid solution by combining VoID descriptions for data source selection
along with SPARQL ASK queries submitted to each dataset at run-time for
verification. Statistical information for each predicate and types in the dataset
are organized as inverted indices, which will be used for data source selection
and join order optimization. Similarly, Semagrow [75] implements a hybrid solution, like SPLENDID, and triple pattern-wise source selection method which
uses VoID descriptions (if available) and SPARQL ASK queries.
MULDER [124] and Ontario [125] federated query engine employs source
description computed based on the concept of RDF molecules; a set of triples
that share the same subject values are called RDF Molecules. RDF Molecule
Templates (RDF-MTs) encode an abstract description of a set of RDF molecules
that share similar characteristics such as semantic type of entities. RDF Molecule
Template-based source descriptions leverage the semantics encoded in data
sources. It is composed of a semantic concept shared by RDF molecules, a
set of mapping rules, a list of properties that a molecule can have, and a list
of intra- and inter-connections between other RDF molecule templates. Such
description models provide a holistic view over the set of entities and their relationships within the data sources in the federation. For instance, Fig. 4 shows
RDF-MT based descriptions of the FedBench benchmark composed on 10 RDF
data sources.
3.2 Query Decomposition and Source Selection
Selecting the relevant data sources for a given query is one of the sub-problems
in federated query processing. Given a federated query parsed with no syntactic
problems, the query is first checked if it is semantically correct with respect to
the global schema. This step eliminates an incorrect query that yields no results
early on. The query is then simplified by, for example, removing redundant predicates. The task of source selection is to select the actual implementation of subqueries in the federation at specific data sources. The sources schema and global
schema are given by the data source descriptions as input to this sub-problem.
The query decomposition and source selection sub-problem decomposes the federated query into subqueries associated with data sources in the federation that
are selected for executing the subqueries. The number of data sources considered
for selection are bounded by the data source description given to the federated
query processing engine. Each sub-query may be associated to zero or more data
source, thus, if the query contains at least one sub-query without data source(s)
associated with it, then the global query can be rejected. Source selection task is
a critical part of query optimization. Failure to select correct data sources might
Chapter 5 Federated Query Processing 83
Fig. 4. RDF-MT-based description of FedBench. The graph comprises 387 RDFMTs and 6, 317 intra- and inter-dataset links. The dots in each circle represent RDFMTs. A line between dots in the same circle shows intra-dataset links, while a line
between dots in different circles corresponds to inter-dataset links. In numbers, there
is only one RDF-MT in ChEBI, 234 in DBpedia, six in Drugbank, one in Geonames,
11 in Jamendo, four in KEGG, 53 in LinkedMDB, two in NYTimes, and 80 in SWDF
dataset. Four of these RDF-MTs belong to at least two FedBench datasets, modeled
as separate circular dots.
lead to incomplete answers as well as high response time and resource consumption. The output of this component is a decomposed query into subqueries that
are associated with the selected data sources in the federation. Identifying the
relevant sources of a query not only leads to a complete answer but also faster
execution time.
3.3 Query Planning and Optimization
The goal of query planning is to generate an execution plan that represent the
steps on how the query is executed and which algorithms (operators) are used.
The task of query plan generation produces query execution plans, e.g., a treebased plan where the leaf of the tree corresponds to the sub-queries to be executed in selected data sources and the internal nodes corresponds to the physical
(algebraic) operators, such as join, union, project, and filter, that perform algebraic operations by the federated query processing engine. Many semantically
equivalent execution plans can be found by permuting the order of operators
and subqueries. However, the cost of executing different ordering of a query is
not always the same. In a federated setting, the number of intermediate results
as well as the communication costs impacts the performance of query execution. Federated query processing engines should use an optimization techniques
to select an optimal execution plan that reduces execution time and resource
usage, such as memory, communication, etc. Optimization of the query execution plan starts from selecting only relevant sources, decomposition and finally
84 K. M. Endris et al.
making decisions on the selection of appropriate implementation of join operations. These optimization techniques include making decisions on selection of
the join methods, ordering, and adapting to the condition of the sources. The
objective of the planning and optimization sub-problem is to find an execution
plan that minimizes the cost of processing the given query, i.e., finding the “best”
ordering of operators in the query, which is close to optimal solution. Finding
an optimal solution is computationally intractable [210]. Assuming a simplified
cost function, it is proven that the minimization of this cost function for a query
with many joins is NP-Complete. To select the ordering of operators, it is necessary to estimate execution costs of alternative candidate orderings. There are
two type of query optimization in the literature: cost-based and heuristics-based
query optimization. In cost-based optimization techniques, estimating the cost
of the generated plans, i.e., candidate orderings, requires collecting statistics on
each of the data sources either before query executions, static optimization or
during query execution, dynamic optimization. In federated settings, where data
sources are autonomous, collecting such statistics might not always be possible.
Cost-based approaches are often not possible because the data source descriptions do not have the needed statistics. Heuristic-based optimization techniques
can be used to estimate the execution cost using minimum information collected
from sources as well as the properties of the operators in the query, such as type
of predicates, operators, etc. The output of the query planning and optimization
is an optimized query, i.e., query execution plan, with operations (join, union)
between subqueries.
3.4 Query Execution
Query execution is performed by data sources that are involved in answering
sub-query(s) of the given query. Each sub-query executed in each data source is
then optimized using the local schema and index (if available) of the data source
and executed. The physical operator (and algorithms) to perform the relational
operators (join, union, filter) may be chosen. Five different join methods are used
in federated query engines: nested loop join, bound-join, hash join, symmetric
join, and multiple join [335]. In nested-loop join (NLJ) the inner sub-query is
executed for every binding of the intermediate results from the outer sub-query of
the join. The bindings that satisfy the join condition are then included in the join
results. Bound-join, like NLJ, executes inner sub-query for the set of bindings,
unlike NLJ which executes the inner sub-query for every single binding of the
intermediate results from the outer sub-query. This set of bindings can be sent as
a UNION or FILTER SPARQL operators can be used to send multiple bindings
to the inner sub-query. In the hash-join method, each sub-query (operands of the
join operation) is executed in parallel and the join is performed locally using a
single hash table at the query engine. The fourth type of join method, symmetric
Chapter 5 Federated Query Processing 85
(hash) join, is a non-blocking hash-based join that pipelines parallel execution
of the operands and generates output of the join operation as early as possible.
Several extended versions of this method are available, such as XJoin [436],
agjoin [6], and adjoin [6]. Finally, the multiple (hash) join method uses multiple
hash tables to join more than two sub-queries running at the same time.
4 Grand Challenges and Future Work
In this section, we analyze the grand challenges to be addressed in the definition
and implementation of federated query engines against distributed sources of big
data. These challenges can be summarized as follows:
– Definition of formal models able to describe not only the properties and relationships among data sources, but also represent and explain causality relations, bias, and trustworthiness.
– Adaptive query processing techniques able to adjust query processing schedules according to the availability of the data, as well as to the validity and
trustworthiness of the published data.
– Machine learning models able to predict the cost of integrating different
sources, and the benefits that the fusion of new data sources adds to the
accuracy, validity, and trustworthiness of query processing.
– Hybrid approaches that combine computational methods with human knowledge with the aim to enhance, certify, and explain the outcomes of the main
data-driven tasks, e.g., schema matching, and data curation and integration.
– Query processing able to interoperate during query execution. Furthermore,
data quality assessment and bias detection methods are required in order to
produce answers that ensure validity and trustworthiness.
– Methods capable of tracing data consumed from the selected sources, and
explainable federated systems able to justify all the decisions made to produce
the answer of a query over a federation of data sources.
The diversity of the problems that remain open presents enormous opportunities both in research and development. Advancement in this area will contribute
not only more efficient tools but also solutions that users can trust and understand. As a result, we expect a paradigm shift in the area of big data integration
and processing towards explainability and trustworthiness – issues that have
thus far prevented global adoption of data-driven tools.
86 K. M. Endris et al.
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.
Chapter 6
Reasoning in Knowledge Graphs:
An Embeddings Spotlight
Luigi Bellomarini1, Emanuel Sallinger2,3(B) , and Sahar Vahdati3
1 Banca d’Italia, Rome, Italy 2 TU Wien, Vienna, Austria
sallinger@dbai.tuwien.ac.at 3 University of Oxford, Oxford, UK
Abstract. In this chapter we introduce the aspect of reasoning in
Knowledge Graphs. As in Chap. 2, we will give a broad overview focusing
on the multitude of reasoning techniques: spanning logic-based reasoning, embedding-based reasoning, neural network-based reasoning, etc. In
particular, we will discuss three dimensions of reasoning in Knowledge
Graphs. Complementing these dimensions, we will structure our exploration based on a pragmatic view of reasoning tasks and families of reasoning tasks: reasoning for knowledge integration, knowledge discovery
and application services.
1 Introduction
The notion of intelligence is closely intertwined with the ability to reason. In
turn, this ability to reason plays a central role in AI algorithms. This is the
case not only for the AI of today but for any form of knowledge representation,
understanding and discovery, as stated by Leibniz in 1677: “It is obvious that
if we could find characters or signs suited for expressing all our thoughts as
clearly and as exactly as arithmetic expresses numbers or geometry expresses
lines, we could do in all matters insofar as they are subject to reasoning all that
we can do in arithmetic and geometry. For all investigations which depend on
reasoning would be carried out by transposing these characters and by a species
of calculus” [279].
Research in reasoning was carried out by mathematicians and logicians, and
naturally adopted and also carried out by computer scientists later on. Concrete
references of having knowledgeable machines date back to at least the 1940s – V.
Bush talked about a machine able to think like a human in his influential essay in
1945 “As We May Think” [65]. Later in 1950, with Alan Turing’s seminal work
[432], the idea behind Artificial Intelligence and impressing thinking power to
machines began with mathematically employed reasoning. The developments of
symbolic reasoning continued towards providing mathematical semantics of logic

20
Chapter 2
MapReduce and the New
Software Stack
Modern data-mining applications, often called “big-data” analysis, require us
to manage immense amounts of data quickly. In many of these applications, the
data is extremely regular, and there is ample opportunity to exploit parallelism.
Important examples are:
1. The ranking of Web pages by importance, which involves an iterated
matrix-vector multiplication where the dimension is many billions. This
application, called “PageRank,” is the subject of Chapter 5.
2. Searches in “friends” networks at social-networking sites, which involve
graphs with hundreds of millions of nodes and many billions of edges.
Operations on graphs of this type are covered in Chapter 10.
To deal with applications such as these, a new software stack has evolved.
These programming systems are designed to get their parallelism not from a
“supercomputer,” but from “computing clusters” – large collections of commodity hardware, including conventional processors (“compute nodes”) connected
by Ethernet cables or inexpensive switches. The software stack begins with
a new form of file system, called a “distributed file system,” which features
much larger units than the disk blocks in a conventional operating system. Distributed file systems also provide replication of data or redundancy to protect
against the frequent media failures that occur when data is distributed over
thousands of low-cost compute nodes.
On top of these file systems, many different higher-level programming systems have been developed. Central to the new software stack is a programming
system called MapReduce. Implementations of MapReduce enable many of the
most common calculations on large-scale data to be performed on computing
clusters efficiently and in a way that is tolerant of hardware failures during the
computation.
21
22 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
MapReduce systems are evolving and extending rapidly. Today, it is common for MapReduce programs to be created from still higher-level programming
systems, often an implementation of SQL. Further, MapReduce turns out to be
a useful, but simple, case of more general and powerful ideas. We include
in this chapter a discussion of generalizations of MapReduce, first to systems
that support acyclic workflows and then to systems that implement recursive
algorithms.
Our last topic for this chapter is the design of good MapReduce algorithms,
a subject that often differs significantly from the matter of designing good
parallel algorithms to be run on a supercomputer. When designing MapReduce
algorithms, we often find that the greatest cost is in the communication. We
thus investigate communication cost and what it tells us about the most efficient
MapReduce algorithms. For several common applications of MapReduce we are
able to give families of algorithms that optimally trade the communication cost
against the degree of parallelism.
2.1 Distributed File Systems
Most computing is done on a single processor, with its main memory, cache, and
local disk (a compute node). In the past, applications that called for parallel
processing, such as large scientific calculations, were done on special-purpose
parallel computers with many processors and specialized hardware. However,
the prevalence of large-scale Web services has caused more and more computing
to be done on installations with thousands of compute nodes operating more
or less independently. In these installations, the compute nodes are commodity
hardware, which greatly reduces the cost compared with special-purpose parallel
machines.
These new computing facilities have given rise to a new generation of programming systems. These systems take advantage of the power of parallelism
and at the same time avoid the reliability problems that arise when the computing hardware consists of thousands of independent components, any of which
could fail at any time. In this section, we discuss both the characteristics of
these computing installations and the specialized file systems that have been
developed to take advantage of them.
2.1.1 Physical Organization of Compute Nodes
The new parallel-computing architecture, sometimes called cluster computing,
is organized as follows. Compute nodes are stored on racks, perhaps 8–64
on a rack. The nodes on a single rack are connected by a network, typically
gigabit Ethernet. There can be many racks of compute nodes, and racks are
connected by another level of network or a switch. The bandwidth of inter-rack
communication is somewhat greater than the intrarack Ethernet, but given the
number of pairs of nodes that might need to communicate between racks, this
2.1. DISTRIBUTED FILE SYSTEMS 23
bandwidth may be essential. Figure 2.1 suggests the architecture of a largescale computing system. However, there may be many more racks and many
more compute nodes per rack.
Switch
Racks of compute nodes
Figure 2.1: Compute nodes are organized into racks, and racks are interconnected by a switch
It is a fact of life that components fail, and the more components, such as
compute nodes and communication links, a system has, the more frequently
something in the system will not be working at any given time. For systems
such as Fig. 2.1, the principal failure modes are the loss of a single node (e.g.,
the disk at that node crashes) and the loss of an entire rack (e.g., the network
connecting its nodes to each other and to the outside world fails).
Some important calculations take minutes or even hours on thousands of
compute nodes. If we had to abort and restart the computation every time
one component failed, then the computation might never complete successfully.
The solution to this problem takes two forms:
1. Files must be stored redundantly. If we did not duplicate the file at several
compute nodes, then if one node failed, all its files would be unavailable
until the node is replaced. If we did not back up the files at all, and the
disk crashes, the files would be lost forever. We discuss file management
in Section 2.1.2.
2. Computations must be divided into tasks, such that if any one task fails
to execute to completion, it can be restarted without affecting other tasks.
This strategy is followed by the MapReduce programming system that we
introduce in Section 2.2.
24 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
DFS Implementations
There are several distributed file systems of the type we have described
that are used in practice. Among these:
1. The Google File System (GFS), the original of the class.
2. Hadoop Distributed File System (HDFS), an open-source DFS used
with Hadoop, an implementation of MapReduce (see Section 2.2)
and distributed by the Apache Software Foundation.
3. Colossus is an improved version of GFS, about which little has been
published. However, a goal of Colossus is to provide real-time file
service.
2.1.2 Large-Scale File-System Organization
To exploit cluster computing, files must look and behave somewhat differently
from the conventional file systems found on single computers. This new file
system, often called a distributed file system or DFS (although this term has
had other meanings in the past), is typically used as follows.
• Files can be enormous, possibly a terabyte in size. If you have only small
files, there is no point using a DFS for them.
• Files are rarely updated. Rather, they are read as data for some calculation, and possibly additional data is appended to files from time to time.
For example, an airline reservation system would not be suitable for a
DFS, even if the data were very large, because the data is changed so
frequently.
Files are divided into chunks, which are typically 64 megabytes in size.
Chunks are replicated, perhaps three times, at three different compute nodes.
Moreover, the nodes holding copies of one chunk should be located on different
racks, so we don’t lose all copies due to a rack failure. Typically, a rack “fails”
because the interconnect among the compute nodes on the rack fails, and the
rack can no longer communicate with anything outside itself. Normally, both
the chunk size and the degree of replication can be decided by the user.
To find the chunks of a file, there is another small file called the master node
or name node for that file. The master node is itself replicated, and a directory
for the file system as a whole knows where to find its copies. The directory itself
can be replicated, and all participants using the DFS know where the directory
copies are.
2.2. MAPREDUCE 25
2.2 MapReduce
MapReduce is a style of computing that has been implemented in several systems, including Google’s internal implementation (simply called MapReduce)
and the popular open-source implementation Hadoop which can be obtained,
along with the HDFS file system from the Apache Foundation. You can use
an implementation of MapReduce to manage many large-scale, parallel computations in a way that is tolerant of hardware faults. All you need to write
are two functions, called Map and Reduce. The system manages the parallel
execution and coordination of tasks that execute Map or Reduce. The system
also deals with the possibility that one of these tasks will fail to execute. In
brief, a MapReduce computation executes as follows:
1. Some number of Map tasks each are given one or more chunks from a
distributed file system. These Map tasks turn the chunk into a sequence
of key-value pairs. The way key-value pairs are produced from the input
data is determined by the code written by the user for the Map function.
2. The key-value pairs from each Map task are collected by a master controller and sorted by key. The keys are divided among all the Reduce
tasks, so all key-value pairs with the same key wind up at the same Reduce task.
3. The Reduce tasks work on one key at a time, and combine all the values associated with that key in some way. The manner of combination
of values is determined by the code written by the user for the Reduce
function.
Figure 2.2 suggests this computation.
2.2.1 The Map Tasks
We view input files for a Map task as consisting of elements, which can be
any type: a tuple or a document, for example. A chunk is a collection of
elements, and no element is stored across two chunks. Technically, all inputs
to Map tasks and outputs from Reduce tasks are of the key-value-pair form,
but normally the keys of input elements are not relevant and we shall tend to
ignore them. Insisting on this form for inputs and outputs is motivated by the
desire to allow composition of several MapReduce processes.
The Map function takes an input element as its argument and produces
zero or more key-value pairs. The types of keys and values are each arbitrary.
Further, keys are not “keys” in the usual sense; they do not have to be unique.
Rather a Map task can produce several key-value pairs with the same key, even
from the same element.
Example 2.1 : We shall illustrate a MapReduce computation with what has
become the standard example application: counting the number of occurrences
26 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
Input
chunks
Group
by keys
Key−value
(k,v)
pairs
their values
Keys with all
output
Combined
Map
tasks
Reduce
tasks
(k, [v, w,...])
Figure 2.2: Schematic of a MapReduce computation
for each word in a collection of documents. In this example, the input file is a
repository of documents, and each document is an element. The Map function
for this example uses keys that are of type String (the words) and values that
are integers. The Map task reads a document and breaks it into its sequence
of words w1, w2, . . . , wn. It then emits a sequence of key-value pairs where the
value is always 1. That is, the output of the Map task for this document is the
sequence of key-value pairs:
(w1, 1), (w2, 1), . . . ,(wn, 1)
Note that a single Map task will typically process many documents – all
the documents in one or more chunks. Thus, its output will be more than the
sequence for the one document suggested above. Note also that if a word w
appears m times among all the documents assigned to that task, then there
will be m key-value pairs (w, 1) among its output. An option, which we discuss
in Section 2.2.4, is for this Map task to combine these m pairs into a single
pair (w, m), but we can only do that because, as we shall see, the Reduce tasks
apply an associative and commutative operation, addition, to the values. ✷
2.2.2 Grouping by Key
As soon as the Map tasks have all completed successfully, the key-value pairs are
grouped by key, and the values associated with each key are formed into a single
list of values for that key. The grouping is performed by the system, regardless
2.2. MAPREDUCE 27
of what the Map and Reduce tasks do. The master controller process knows
how many Reduce tasks there will be, say r such tasks. The user typically tells
the MapReduce system what r should be. Then the master controller picks a
hash function that takes a key as argument and produces a bucket number from
0 to r − 1. Each key that is output by a Map task is hashed and its key-value
pair is put in one of r local files. Each file is destined for one of the r Reduce
tasks.1
To perform the grouping by key and distribution to the Reduce tasks, the
master controller merges the files from each Map task that are destined for
a particular Reduce task and feeds the merged file to that process as a sequence of key/list-of-values pairs. That is, for each key k, the input to the
Reduce task that handles key k is a pair of the form (k, [v1, v2, . . . , vn]), where
(k, v1), (k, v2), . . . ,(k, vn) are all the key-value pairs with key k coming from
all the Map tasks.
2.2.3 The Reduce Tasks
The Reduce function’s argument is a pair consisting of a key and its list of
associated values. The output of the Reduce function is a sequence of zero or
more key-value pairs. These key-value pairs can be of a type different from
those sent from Map tasks to Reduce tasks, but often they are the same type.
We shall refer to the application of the Reduce function to a single key and its
associated list of values as a reducer.
A Reduce task receives one or more keys and their associated value lists.
That is, a Reduce task executes one or more reducers. The outputs from all
the Reduce tasks are merged into a single file.
Example 2.2 : Let us continue with the word-count example of Example 2.1.
The Reduce function simply adds up all the values. The output of a reducer
consists of the word and the sum. Thus, the output of all the Reduce tasks is a
sequence of (w, m) pairs, where w is a word that appears at least once among
all the input documents and m is the total number of occurrences of w among
those documents. ✷
2.2.4 Combiners
Sometimes, a Reduce function is associative and commutative. That is, the
values to be combined can be combined in any order, with the same result.
The addition performed in Example 2.2 is an example of an associative and
commutative operation. It doesn’t matter how we order or group a list of
numbers v1, v2, . . . , vn; the sum will be the same.
When the Reduce function is associative and commutative, we can push
some of what the reducers do to the Map tasks. For example, instead of each
1Optionally, users can specify their own hash function or other method for assigning keys
to Reduce tasks. However, whatever algorithm is used, each key is assigned to one and only
one Reduce task.
28 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
Reducers, Reduce Tasks, Compute Nodes, and Skew
If we want maximum parallelism, then we could use one Reduce task
to execute each reducer, i.e., a single key and its associated value list.
Further, we could execute each Reduce task at a different compute node,
so they would all execute in parallel. This plan is not usually the best. One
problem is that there is overhead associated with each task we create, so
we might want to keep the number of Reduce tasks lower than the number
of different keys. Moreover, often there are far more keys than there are
compute nodes available, so we would get no benefit from a huge number
of Reduce tasks.
Second, there is often significant variation in the lengths of the value
lists for different keys, so different reducers take different amounts of time.
If we make each reducer a separate Reduce task, then the tasks themselves
will exhibit skew – a significant difference in the amount of time each
takes. We can reduce the impact of skew by using fewer Reduce tasks
than there are reducers. If keys are sent randomly to Reduce tasks, we
can expect that there will be some averaging of the total time required by
the different Reduce tasks. We can further reduce the skew by using more
Reduce tasks than there are compute nodes. In that way, long Reduce
tasks might occupy a compute node fully, while several shorter Reduce
tasks might run sequentially at a single compute node.
Map task in Example 2.1 producing many pairs (w, 1), (w, 1), . . ., we could
apply the Reduce function within each Map task, before the outputs of the
Map tasks are subject to grouping and aggregation. These key-value pairs
would thus be replaced by one pair with key w and value equal to the sum of
all the 1’s in all those pairs. That is, the pairs with key w generated by a single
Map task would be replaced by a pair (w, m), where m is the number of times
that w appears among the documents handled by this Map task. Note that
it is still necessary to do grouping and aggregation and to pass the result to
the Reduce tasks, since there will typically be one key-value pair with key w
coming from each of the Map tasks.
2.2.5 Details of MapReduce Execution
Let us now consider in more detail how a program using MapReduce is executed.
Figure 2.3 offers an outline of how processes, tasks, and files interact. Taking
advantage of a library provided by a MapReduce system such as Hadoop, the
user program forks a Master controller process and some number of Worker
processes at different compute nodes. Normally, a Worker handles either Map
tasks (a Map worker) or Reduce tasks (a Reduce worker), but not both.
The Master has many responsibilities. One is to create some number of
2.2. MAPREDUCE 29
Program
User
Master
Worker
Worker
Worker
Worker
Worker
Data
Input
File
Output
fork fork
fork
Map
assign assign
Reduce
Intermediate
Files
Figure 2.3: Overview of the execution of a MapReduce program
Map tasks and some number of Reduce tasks, these numbers being selected
by the user program. These tasks will be assigned to Worker processes by the
Master. It is reasonable to create one Map task for every chunk of the input
file(s), but we may wish to create fewer Reduce tasks. The reason for limiting
the number of Reduce tasks is that it is necessary for each Map task to create
an intermediate file for each Reduce task, and if there are too many Reduce
tasks the number of intermediate files explodes.
The Master keeps track of the status of each Map and Reduce task (idle,
executing at a particular Worker, or completed). A Worker process reports to
the Master when it finishes a task, and a new task is scheduled by the Master
for that Worker process.
Each Map task is assigned one or more chunks of the input file(s) and
executes on it the code written by the user. The Map task creates a file for
each Reduce task on the local disk of the Worker that executes the Map task.
The Master is informed of the location and sizes of each of these files, and the
Reduce task for which each is destined. When a Reduce task is assigned by the
Master to a Worker process, that task is given all the files that form its input.
The Reduce task executes code written by the user and writes its output to a
file that is part of the surrounding distributed file system.
30 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
2.2.6 Coping With Node Failures
The worst thing that can happen is that the compute node at which the Master
is executing fails. In this case, the entire MapReduce job must be restarted.
But only this one node can bring the entire process down; other failures will be
managed by the Master, and the MapReduce job will complete eventually.
Suppose the compute node at which a Map worker resides fails. This failure will be detected by the Master, because it periodically pings the Worker
processes. All the Map tasks that were assigned to this Worker will have to
be redone, even if they had completed. The reason for redoing completed Map
tasks is that their output destined for the Reduce tasks resides at that compute
node, and is now unavailable to the Reduce tasks. The Master sets the status
of each of these Map tasks to idle and will schedule them on a Worker when
one becomes available. The Master must also inform each Reduce task that the
location of its input from that Map task has changed.
Dealing with a failure at the node of a Reduce worker is simpler. The Master
simply sets the status of its currently executing Reduce tasks to idle. These
will be rescheduled on another Reduce worker later.
2.2.7 Exercises for Section 2.2
Exercise 2.2.1 : Suppose we execute the word-count MapReduce program described in this section on a large repository such as a copy of the Web. We shall
use 100 Map tasks and some number of Reduce tasks.
(a) Suppose we do not use a combiner at the Map tasks. Do you expect there
to be significant skew in the times taken by the various reducers to process
their value list? Why or why not?
(b) If we combine the reducers into a small number of Reduce tasks, say 10
tasks, at random, do you expect the skew to be significant? What if we
instead combine the reducers into 10,000 Reduce tasks?
! (c) Suppose we do use a combiner at the 100 Map tasks. Do you expect skew
to be significant? Why or why not?
2.3 Algorithms Using MapReduce
MapReduce is not a solution to every problem, not even every problem that
profitably can use many compute nodes operating in parallel. As we mentioned
in Section 2.1.2, the entire distributed-file-system milieu makes sense only when
files are very large and are rarely updated in place. For instance, we would not
expect to use either a DFS or an implementation of MapReduce for managing
on-line retail sales, even though a large on-line retailer such as Amazon.com uses
thousands of compute nodes when processing requests over the Web. The reason
is that the principal operations on Amazon data involve responding to searches
2.3. ALGORITHMS USING MAPREDUCE 31
for products, recording sales, and so on, processes that involve relatively little
calculation and that change the database.2 On the other hand, Amazon might
use MapReduce to perform certain analytic queries on large amounts of data,
such as finding for each user those users whose buying patterns were most
similar.
The original purpose for which the Google implementation of MapReduce
was created was to execute very large matrix-vector multiplications as are
needed in the calculation of PageRank (See Chapter 5). We shall see that
matrix-vector and matrix-matrix calculations fit nicely into the MapReduce
style of computing. Another important class of operations that can use MapReduce effectively are the relational-algebra operations. We shall examine the
MapReduce execution of these operations as well.
2.3.1 Matrix-Vector Multiplication by MapReduce
Suppose we have an n × n matrix M, whose element in row i and column j is
denoted mij . Suppose we also have a vector v of length n, whose jth element
is vj . Then the matrix-vector product is the vector x of length n, whose ith
element xi
is given by
xi =
Xn
j=1
mijvj
If n = 100, we do not want to use a DFS or MapReduce for this calculation.
But this sort of calculation is at the heart of the ranking of Web pages that
goes on at search engines, and there, n is on the order of trillions.3 Let us
first assume that n is large, but not so large that vector v cannot fit in main
memory and thus be available to every Map task.
The matrix M and the vector v each will be stored in a file of the DFS. We
assume that the row-column coordinates of each matrix element will be discoverable, either from its position in the file, or because it is stored with explicit
coordinates, as a triple (i, j, mij ). We also assume the position of element vj in
the vector v will be discoverable in the analogous way.
The Map Function: The Map function is written to apply to one element of
M. However, if v is not already read into main memory at the compute node
executing a Map task, then v is first read, in its entirety, and subsequently will
be available to all applications of the Map function performed at this Map task.
Each Map task will operate on a chunk of the matrix M. From each matrix
element mij it produces the key-value pair (i, mijvj ). Thus, all terms of the
sum that make up the component xi of the matrix-vector product will get the
same key, i.
2Recall that even looking at a product you don’t buy causes Amazon to remember that
you looked at it.
3The matrix is sparse, with on the average of 10 to 15 nonzero elements per row, since the
matrix represents the links in the Web, with mij nonzero if and only if there is a link from
page j to page i. Note that there is no way we could store a dense matrix whose side was
1012, since it would have 1024 elements.
32 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
The Reduce Function: The Reduce function simply sums all the values associated with a given key i. The result will be a pair (i, xi).
2.3.2 If the Vector v Cannot Fit in Main Memory
However, it is possible that the vector v is so large that it will not fit in its
entirety in main memory. It is not required that v fit in main memory at a
compute node, but if it does not then there will be a very large number of
disk accesses as we move pieces of the vector into main memory to multiply
components by elements of the matrix. Thus, as an alternative, we can divide
the matrix into vertical stripes of equal width and divide the vector into an equal
number of horizontal stripes, of the same height. Our goal is to use enough
stripes so that the portion of the vector in one stripe can fit conveniently into
main memory at a compute node. Figure 2.4 suggests what the partition looks
like if the matrix and vector are each divided into five stripes.
Matrix Vector M v
Figure 2.4: Division of a matrix and vector into five stripes
The ith stripe of the matrix multiplies only components from the ith stripe
of the vector. Thus, we can divide the matrix into one file for each stripe, and
do the same for the vector. Each Map task is assigned a chunk from one of the
stripes of the matrix and gets the entire corresponding stripe of the vector. The
Map and Reduce tasks can then act exactly as was described in Section 2.3.1
for the case where Map tasks get the entire vector.
We shall take up matrix-vector multiplication using MapReduce again in
Section 5.2. There, because of the particular application (PageRank calculation), we have an additional constraint that the result vector should be partitioned in the same way as the input vector, so the output may become the
input for another iteration of the matrix-vector multiplication. We shall see
there that the best strategy involves partitioning the matrix M into square
blocks, rather than stripes.
2.3. ALGORITHMS USING MAPREDUCE 33
2.3.3 Relational-Algebra Operations
There are a number of operations on large-scale data that are used in database
queries. Many traditional database applications involve retrieval of small amounts of data, even though the database itself may be large. For example, a
query may ask for the bank balance of one particular account. Such queries are
not useful applications of MapReduce.
However, there are many operations on data that can be described easily in
terms of the common database-query primitives, even if the queries themselves
are not executed within a database management system. Thus, a good starting
point for exploring applications of MapReduce is by considering the standard
operations on relations. We assume you are familiar with database systems,
the query language SQL, and the relational model, but to review, a relation is
a table with column headers called attributes. Rows of the relation are called
tuples. The set of attributes of a relation is called its schema. We often write
an expression like R(A1, A2, . . . , An) to say that the relation name is R and its
attributes are A1, A2, . . . , An.
From To
url1 url2
url1 url3
url2 url3
url2 url4
· · · · · ·
Figure 2.5: Relation Links consists of the set of pairs of URL’s such that the
first has one or more links to the second
Example 2.3 : In Fig. 2.5 we see part of the relation Links that describes
the structure of the Web. There are two attributes, From and To. A row, or
tuple, of the relation is a pair of URL’s such that there is at least one link from
the first URL to the second. For instance, the first row of Fig. 2.5 is the pair
(url1, url2). This tuple says the Web page url1 has a link to page url2. While
we have shown only four tuples, the real relation of the Web, or the portion of
it that would be stored by a typical search engine, has trillions of tuples. ✷
A relation, however large, can be stored as a file in a distributed file system.
The elements of this file are the tuples of the relation.
There are several standard operations on relations, often referred to as relational algebra, that are used to implement queries. The queries themselves
usually are written in SQL. The relational-algebra operations we shall discuss
are:
1. Selection: Apply a condition C to each tuple in the relation and produce
as output only those tuples that satisfy C. The result of this selection is
denoted σC (R).
34 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
2. Projection: For some subset S of the attributes of the relation, produce
from each tuple only the components for the attributes in S. The result
of this projection is denoted πS(R).
3. Union, Intersection, and Difference: These well-known set operations
apply to the sets of tuples in two relations that have the same schema.
There are also bag (multiset) versions of the operations in SQL, with
somewhat unintuitive definitions, but we shall not go into the bag versions
of these operations here.
4. Natural Join: Given two relations, compare each pair of tuples, one from
each relation. If the tuples agree on all the attributes that are common
to the two schemas, then produce a tuple that has components for each
of the attributes in either schema and agrees with the two tuples on each
attribute. If the tuples disagree on one or more shared attributes, then
produce nothing from this pair of tuples. The natural join of relations R
and S is denoted R ⊲⊳ S. While we shall discuss executing only the natural join with MapReduce, all equijoins (joins where the tuple-agreement
condition involves equality of attributes from the two relations that do not
necessarily have the same name) can be executed in the same manner. We
shall give an illustration in Example 2.4.
5. Grouping and Aggregation:
4 Given a relation R, partition its tuples according to their values in one set of attributes G, called the grouping
attributes. Then, for each group, aggregate the values in certain other attributes. The normally permitted aggregations are SUM, COUNT, AVG,
MIN, and MAX, with the obvious meanings. Note that MIN and MAX
require that the aggregated attributes have a type that can be compared,
e.g., numbers or strings, while SUM and AVG require that the type allow
arithmetic operations, typically only numbers. COUNT can be performed
on data of any type. We denote a grouping-and-aggregation operation on
a relation R by γX(R), where X is a list of elements that are each either
(a) A grouping attribute, or
(b) An expression θ(A), where θ is one of the five aggregation operations such as SUM, and A is an attribute not among the grouping
attributes.
The result of this operation is one tuple for each group. That tuple has
a component for each of the grouping attributes, with the value common
to tuples of that group. It also has a component for each aggregation,
with the aggregated value for that group. We shall see an illustration in
Example 2.5.
4Some descriptions of relational algebra do not include these operations, and indeed they
were not part of the original definition of this algebra. However, these operations are so
important in SQL, that modern treatments of relational algebra include them.
2.3. ALGORITHMS USING MAPREDUCE 35
Example 2.4 : Let us try to find the paths of length two in the Web, using
the relation Links of Fig. 2.5. That is, we want to find the triples of URL’s
(u, v, w) such that there is a link from u to v and a link from v to w. We
essentially want to take the natural join of Links with itself, but we first need
to imagine that it is two relations, with different schemas, so we can describe the
desired connection as a natural join. Thus, imagine that there are two copies
of Links, namely L1(U1, U2) and L2(U2, U3). Now, if we compute L1 ⊲⊳ L2,
we shall have exactly what we want. That is, for each tuple t1 of L1 (i.e.,
each tuple of Links) and each tuple t2 of L2 (another tuple of Links, possibly
even the same tuple), see if their U2 components are the same. Note that
these components are the second component of t1 and the first component of
t2. If these two components agree, then produce a tuple for the result, with
schema (U1, U2, U3). This tuple consists of the first component of t1, the
second component of t1 (which must equal the first component of t2), and the
second component of t2.
We may not want the entire path of length two, but only want the pairs
(u, w) of URL’s such that there is at least one path from u to w of length two. If
so, we can project out the middle components by computing πU1,U3(L1 ⊲⊳ L2).
✷
Example 2.5 : Imagine that a social-networking site has a relation
Friends(User, Friend)
This relation has tuples that are pairs (a, b) such that b is a friend of a. The site
might want to develop statistics about the number of friends members have.
Their first step would be to compute a count of the number of friends of each
user. This operation can be done by grouping and aggregation, specifically
γUser,COUNT(Friend)(Friends)
This operation groups all the tuples by the value in their first component, so
there is one group for each user. Then, for each group the count of the number
of friends of that user is made. The result will be one tuple for each group, and
a typical tuple would look like (Sally, 300), if user “Sally” has 300 friends. ✷
2.3.4 Computing Selections by MapReduce
Selections really do not need the full power of MapReduce. They can be done
most conveniently in the map portion alone, although they could also be done
in the reduce portion alone. Here is a MapReduce implementation of selection
σC (R).
The Map Function: For each tuple t in R, test if it satisfies C. If so, produce
the key-value pair (t, t). That is, both the key and value are t. If t does not
satisfy C, then the mapper for t produces nothing.
36 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
The Reduce Function: The Reduce function is the identity. It simply passes
each key-value pair to the output.
Note that the output is not exactly a relation, because it has key-value pairs.
However, a relation can be obtained by using only the value components (or
only the key components) of the output.
2.3.5 Computing Projections by MapReduce
Projection is performed similarly to selection. However, because projection
may cause the same tuple to appear several times, the Reduce function must
eliminate duplicates. We may compute πS (R) as follows.
The Map Function: For each tuple t in R, construct a tuple t
′ by eliminating
from t those components whose attributes are not in S. Output the key-value
pair (t
′
, t′
).
The Reduce Function: For each key t
′ produced by any of the Map tasks,
there will be one or more key-value pairs (t
′
, t′
). After the system groups keyvalue pairs by key, the Reduce function turns (t
′
, [t
′
, t′
, . . . , t′
]) into (t
′
, t′
), so it
produces exactly one pair (t
′
, t′
) for this key t
′
.
Observe that the Reduce operation is duplicate elimination. This operation
is associative and commutative, so a combiner associated with each Map task
can eliminate whatever duplicates are produced locally. However, the Reduce
tasks are still needed to eliminate two identical tuples coming from different
Map tasks.
2.3.6 Union, Intersection, and Difference by MapReduce
First, consider the union of two relations. Suppose relations R and S have the
same schema. Map tasks will be assigned chunks from either R or S; it doesn’t
matter which. The Map tasks don’t really do anything except pass their input
tuples as key-value pairs to the Reduce tasks. The latter need only eliminate
duplicates as for projection.
The Map Function: Turn each input tuple t into a key-value pair (t, t).
The Reduce Function: Associated with each key t there will be either one or
two values. Produce output (t, t) in either case.
To compute the intersection, we can use the same Map function. However,
the Reduce function must produce a tuple only if both relations have the tuple.
If the key t has a list of two values [t, t] associated with it, then the Reduce
task for t should produce (t, t). However, if the value-list associated with key
t is just [t], then one of R and S is missing t, so we don’t want to produce a
tuple for the intersection.
The Map Function: Turn each tuple t into a key-value pair (t, t).
The Reduce Function: If key t has value list [t, t], then produce (t, t). Otherwise, produce nothing.
2.3. ALGORITHMS USING MAPREDUCE 37
The Difference R − S requires a bit more thought. The only way a tuple
t can appear in the output is if it is in R but not in S. The Map function
can pass tuples from R and S through, but must inform the Reduce function
whether the tuple came from R or S. We shall thus use the relation as the
value associated with the key t. Here is a specification for the two functions.
The Map Function: For a tuple t in R, produce key-value pair (t, R), and
for a tuple t in S, produce key-value pair (t, S). Note that the intent is that
the value is the name of R or S (or better, a single bit indicating whether the
relation is R or S), not the entire relation.
The Reduce Function: For each key t, if the associated value list is [R], then
produce (t, t). Otherwise, produce nothing.
2.3.7 Computing Natural Join by MapReduce
The idea behind implementing natural join via MapReduce can be seen if we
look at the specific case of joining R(A, B) with S(B, C).5 We must find tuples
that agree on their B components, that is the second component from tuples
of R and the first component of tuples of S. We shall use the B-value of tuples
from either relation as the key. The value will be the other component and the
name of the relation, so the Reduce function can know where each tuple came
from.
The Map Function: For each tuple (a, b) of R, produce the key-value pair

b,(R, a)

. For each tuple (b, c) of S, produce the key-value pair
b,(S, c)

.
The Reduce Function: Each key value b will be associated with a list of pairs
that are either of the form (R, a) or (S, c). Construct all pairs consisting of one
with first component R and the other with first component S, say (R, a) and
(S, c). The output from this key and value list is a sequence of key-value pairs.
The key is irrelevant. Each value is one of the triples (a, b, c) such that (R, a)
and (S, c) are on the input list of values for key b.
The same algorithm works if the relations have more than two attributes.
You can think of A as representing all those attributes in the schema of R but
not S. B represents the attributes in both schemas, and C represents attributes
only in the schema of S. The key for a tuple of R or S is the list of values in all
the attributes that are in the schemas of both R and S. The value for a tuple
of R is the name R together with the values of all the attributes belonging to
R but not to S, and the value for a tuple of S is the name S together with the
values of the attributes belonging to S but not R.
The Reduce function looks at all the key-value pairs with a given key and
combines those values from R with those values of S in all possible ways. From
each pairing, the tuple produced has the values from R, the key values, and the
values from S.
5
If you are familiar with database implementation, you will recognize the MapReduce
implementation of join as the classic parallel hash joi
38 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
2.3.8 Grouping and Aggregation by MapReduce
As we did for the join, we shall discuss here only the minimal example of grouping and aggregation, where there is one grouping attribute (A), one aggregated
attribute (B), and one attribute (C) that is neither grouped nor aggregated.
Let R(A, B, C) be a relation to which we apply the operator γA,θ(B)(R). Map
will perform the grouping, while Reduce does the aggregation.
The Map Function: For each tuple (a, b, c) produce the key-value pair (a, b).
The Reduce Function: Each key a represents a group. Apply the aggregation
operator θ to the list [b1, b2, . . . , bn] of B-values associated with key a. The
output is the pair (a, x), where x is the result of applying θ to the list. For
example, if θ is SUM, then x = b1 + b2 + · · · + bn, and if θ is MAX, then x is
the largest of b1, b2, . . . , bn.
If there are several grouping attributes, then the key is the list of the values
of a tuple for all these attributes. If there is more than one aggregation, then
the Reduce function applies each of them to the list of values associated with
a given key and produces a tuple consisting of the key, including components
for all grouping attributes if there is more than one, followed by the results of
each of the aggregations.
2.3.9 Matrix Multiplication
If M is a matrix with element mij in row i and column j, and N is a matrix
with element njk in row j and column k, then the product P = MN is the
matrix P with element pik in row i and column k, where
pik =
X
j
mijnjk
It is required that the number of columns of M equals the number of rows of
N, so the sum over j makes sense.
We can think of a matrix as a relation with three attributes: the row number,
the column number, and the value in that row and column. Thus, we could view
matrix M as a relation M(I, J, V ), with tuples (i, j, mij ), and we could view
matrix N as a relation N(J, K, W), with tuples (j, k, njk). As large matrices are
often sparse (mostly 0’s), and since we can omit the tuples for matrix elements
that are 0, this relational representation is often a very good one for a large
matrix. However, it is possible that i, j, and k are implicit in the position of a
matrix element in the file that represents it, rather than written explicitly with
the element itself. In that case, the Map function will have to be designed to
construct the I, J, and K components of tuples from the position of the data.
The product MN is almost a natural join followed by grouping and aggregation. That is, the natural join of M(I, J, V ) and N(J, K, W), having
only attribute J in common, would produce tuples (i, j, k, v, w) from each tuple
(i, j, v) in M and tuple (j, k, w) in N. This five-component tuple represents the
2.3. ALGORITHMS USING MAPREDUCE 39
pair of matrix elements (mij , njk). What we want instead is the product of
these elements, that is, the four-component tuple (i, j, k, v × w), because that
represents the product mijnjk. Once we have this relation as the result of one
MapReduce operation, we can perform grouping and aggregation, with I and
K as the grouping attributes and the sum of V × W as the aggregation. That
is, we can implement matrix multiplication as the cascade of two MapReduce
operations, as follows. First:
The Map Function: For each matrix element mij , produce the key value pair

j,(M, i, mij )

. Likewise, for each matrix element njk, produce the key value
pair
j,(N, k, njk)

. Note that M and N in the values are not the matrices
themselves. Rather they are names of the matrices or, more precisely, a single
bit that indicates whether the element comes from M or N (as we mentioned
regarding the similar Map function we used for the natural join):.
The Reduce Function: For each key j, examine its list of associated values.
For each value that comes from M, say (M, i, mij ), and each value that comes
from N, say (N, k, njk), produce a key-value pair with key equal to (i, k) and
value equal to the product of these elements, mijnjk.
Now, we perform a grouping and aggregation by another MapReduce operation
applied to the output of the first MapReduce operation.
The Map Function: This function is just the identity. That is, for every input
element with key (i, k) and value v, produce exactly this key-value pair.
The Reduce Function: For each key (i, k), produce the sum of the list of
values associated with this key. The result is a pair
(i, k), v
, where v is the
value of the element in row i and column k of the matrix P = MN.
2.3.10 Matrix Multiplication with One MapReduce Step
There often is more than one way to use MapReduce to solve a problem. You
may wish to use only a single MapReduce pass to perform matrix multiplication
P = MN.
6
It is possible to do so if we put more work into the two functions.
Start by using the Map function to create the sets of matrix elements that are
needed to compute each element of the answer P. Notice that an element of
M or N contributes to many elements of the result, so one input element will
be turned into many key-value pairs. The keys will be pairs (i, k), where i is a
row of M and k is a column of N. Here is a synopsis of the Map and Reduce
functions.
The Map Function: For each element mij of M, produce all the key-value
pairs
(i, k), (M, j, mij )

for k = 1, 2, . . . up to the number of columns of
N. Similarly, for each element njk of N, produce all the key-value pairs

(i, k), (N, j, njk)

for i = 1, 2, . . . up to the number of rows of M. As before, M and N are really bits to tell which of the two matrices a value comes
from.
6However, we show in Section 2.6.7 that two passes of MapReduce are usually better than
one for matrix multiplica
40 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
The Reduce Function: Each key (i, k) will have an associated list with all
the values (M, j, mij ) and (N, j, njk), for all possible values of j. The Reduce
function needs to connect the two values on the list that have the same value of
j, for each j. An easy way to do this step is to sort by j the values that begin
with M and sort by j the values that begin with N, in separate lists. The jth
values on each list must have their third components, mij and njk extracted
and multiplied. Then, these products are summed and the result is paired with
(i, k) in the output of the Reduce function.
You may notice that if a row of the matrix M or a column of the matrix N
is so large that it will not fit in main memory, then the Reduce tasks will be
forced to use an external sort to order the values associated with a given key
(i, k). However, in that case, the matrices themselves are so large, perhaps 1020
elements, that it is unlikely we would attempt this calculation if the matrices
were dense. If they are sparse, then we would expect many fewer values to be
associated with any one key, and it would be feasible to do the sum of products
in main memory.
2.3.11 Exercises for Section 2.3
Exercise 2.3.1 : Design MapReduce algorithms to take a very large file of
integers and produce as output:
(a) The largest integer.
(b) The average of all the integers.
(c) The same set of integers, but with each integer appearing only once.
! (d) The count of the number of distinct integers in the input.
In each part, you can assume that the key of each output pair will be ignored
or dropped.
Exercise 2.3.2 : Our formulation of matrix-vector multiplication assumed that
the matrix M was square. Generalize the algorithm to the case where M is an
r-by-c matrix for some number of rows r and columns c.
! Exercise 2.3.3 : In the form of relational algebra implemented in SQL, relations are not sets, but bags; that is, tuples are allowed to appear more than
once. There are extended definitions of union, intersection, and difference for
bags, which we shall define below. Write MapReduce algorithms for computing
the following operations on bags R and S:
(a) Bag Union, defined to be the bag of tuples in which tuple t appears the
sum of the numbers of times it appears in R and S.
(b) Bag Intersection, defined to be the bag of tuples in which tuple t appears
the minimum of the numbers of times it appears in R and S.
2.4. EXTENSIONS TO MAPREDUCE 41
(c) Bag Difference, defined to be the bag of tuples in which the number of
times a tuple t appears is equal to the number of times it appears in R
minus the number of times it appears in S. A tuple that appears more
times in S than in R does not appear in the difference.
! Exercise 2.3.4 : Selection can also be performed on bags. Give a MapReduce
implementation that produces the proper number of copies of each tuple t that
passes the selection condition. That is, produce key-value pairs from which the
correct result of the selection can be obtained easily from the values.
Exercise 2.3.5 : The relational-algebra operation R(A, B) ⊲⊳ B<C S(C, D)
produces all tuples (a, b, c, d) such that tuple (a, b) is in relation R, tuple (c, d) is
in S, and b < c. Give a MapReduce implementation of this operation, assuming
R and S are sets.
! Exercise 2.3.6 : In Section 2.3.5 we claimed that duplicate elimination is an
associative and commutative operation. Prove this fact.
2.4 Extensions to MapReduce
MapReduce proved so influential that it spawned a number of extensions and
modifications. These systems typically share a number of characteristics with
MapReduce systems:
1. They are built on a distributed file system.
2. They manage very large numbers of tasks that are instantiations of a
small number of user-written functions.
3. They incorporate a method for dealing with most of the failures that
occur during the execution of a large job, without having to restart that
job from the beginning.
We begin this section with a discussion of “workflow” systems, which extend MapReduce by supporting acyclic networks of functions, each function
implemented by a collection of tasks. While many such systems have been
implemented (see the bibliographic notes for this chapter), an increasingly popular choice is UC Berkeley’s Spark. Also gaining in importance is Google’s
TensorFlow. The latter, while not generally recognized as a workflow system
because of its very specific targeting of machine-learning applications, in fact
has a workflow architecture at heart.
Another family of systems uses a graph model of data. Computation occurs
at the nodes of the graph, and messages are sent from any node to any adjacent
node. The original system of this type was Google’s Pregel, which has its
own unique way of dealing with failures. But it has now become common to
implement a graph-model facility on top of a workflow system and use the
latter’s file system and failure-management facility.
42 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
2.4.1 Workflow Systems
Workflow systems extend MapReduce from the simple two-step workflow (the
Map function feeds the Reduce function) to any collection of functions, with an
acyclic graph representing workflow among the functions. That is, there is an
acyclic flow graph whose arcs a → b represent the fact that function a’s output
is an input to function b.
The data passed from one function to the next is a file of elements of one
type. If a function has a single input, then that function is applied to each
input independently, just as Map and Reduce functions are applied to their
input elements individually. The output of the function is a file collected from
the result of applying the function to each input. If a function has inputs from
more than one file, elements from each of the files can be combined in various
ways. But the function itself is applied to combinations of input elements, at
most one from each input file. We shall see examples of such combinations when
we discuss the implementation of union and the relational join in Section 2.4.2.
h
f g
i
j
Figure 2.6: An example of a workflow that is more complex than Map feeding
Reduce
Example 2.6 : A suggestion of what a workflow might look like is in Fig. 2.6.
There, five functions, f through j, pass data from left to right in specific ways,
so the flow of data is acyclic and no task needs to provide data out before
its entire input is available. For instance, function h takes its input from a
preexisting file of the distributed file system. Each of h’s output elements is
passed to the functions i and j, while i takes the outputs of both f and h as
inputs. The output of j is either stored in the distributed file system or is
passed to an application that invoked this dataflow. ✷
In analogy to Map and Reduce functions, each function of a workflow can
be executed by many tasks, each of which is assigned a portion of the input to
the function. A master controller is responsible for dividing the work among
the tasks that implement a function, possibly by hashing the input elements to
decide on the proper task to receive an element. Thus, like Map tasks, each task
implementing a function f has an output file of data destined for each of the
tasks that implement the successor function(s) of f. These files are delivered
2.4. EXTENSIONS TO MAPREDUCE 43
by the master controller at the appropriate time – after the task has completed
its work.
The functions of a workflow, and therefore the tasks, share with MapReduce
tasks an important property: the blocking property, in that they only deliver
output after they complete. As a result, if a task fails, it has not delivered
output to any of its successors in the flow graph.7 A master controller can
therefore restart the failed task at another compute node, without worrying
that the output of the restarted task will duplicate output that previously was
passed to some other task.
Some applications of workflow systems are effectively cascades of MapReduce jobs. An example would be the join of three relations, where one MapReduce job joins the first two relations, and a second MapReduce job joins the
third relation with the result of joining the first two relations. Both jobs would
use an algorithm like that of Section 2.3.7.
There is an advantage to implementing such cascades as a single workflow.
For example, the flow of data among tasks, and its replication, can be managed
by the master controller, without need to store the temporary file that is output of one MapReduce job in the distributed file system. By locating tasks at
compute nodes that have a copy of their input, we can avoid much of the communication that would be necessary if we stored the result of one MapReduce
job and then initiated a second MapReduce job (although Hadoop and other
MapReduce systems also try to locate Map tasks where a copy of their input is
already present).
2.4.2 Spark
Spark is, at its heart, a workflow system. However, it is an advance over the
early workflow systems in several ways, including:
1. A more efficient way of coping with failures.
2. A more efficient way of grouping tasks among compute nodes and scheduling execution of functions.
3. Integration of programming language features such as looping (which technically takes it out of the acyclic workflow class of systems) and function
libraries.
The central data abstraction of Spark is called the Resilient Distributed
Dataset, or RDD. An RDD is a file of objects of one type. The primary example
of an RDD that we have seen so far is the files of key-value pairs that are used
in MapReduce systems. They are also the files that get passed among functions
that we talked about in connection with Fig. 2.6. RDD’s are “distributed” in
the sense that an RDD is normally broken into chunks that may be held at
7As we shall discuss in Section 2.4.5, the blocking property only holds for acyclic workflows,
and systems that support recursion cannot use it to manage failures.
44 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
different compute nodes. They are “resilient” in the sense that we expect to be
able to recover from the loss of any or all chunks of an RDD. However, unlike
the key-value-pair abstraction of MapReduce, there is no restriction on the type
of the elements that comprise an RDD.
A Spark program consists of a sequence of steps, each of which typically
applies some function to an RDD to produce another RDD. Such operations
are called transformations. It is also possible to take data from the surrounding
file system, such as HDFS, and turn it into an RDD, and to take an RDD and
return it to the surrounding file system or to produce a result that is passed back
to an application that called a Spark program. The latter kinds of operations
are called actions.
We shall not try to list all the available transformations and actions that
are available. Neither shall we fix on the dictions of a particular programming
language, since the Spark operations are designed to be expressable in a number
of different programming languages. However, here are some of the commonly
used operations.
Map, Flatmap, and Filter
The Map transformation takes a parameter that is a function, and it applies that
function to every element of an RDD, producing another RDD. This operation
should remind us of the Map of MapReduce, but it is not exactly the same.
First of all, in MapReduce, a Map function can only apply to a key-value pair.
Second, in MapReduce, a Map function produces a set of key-value pairs, and
each key-value pair is considered an independent element of the output of the
Map function. In Spark, a Map function can apply to any object type, but it
produces exactly one object as a result. The type of the resulting object can
be a set, but that is not the same as producing many objects from one input
object. If you want to produce a set of objects from a single object, Spark
provides for you another transformation called Flatmap, which is analogous to
Map of MapReduce, but without the requirement that all types be key-value
pairs.
Example 2.7 : Suppose our input RDD is a file of documents, as in the “wordcount” of Example 2.1. We could write a Spark Map function that takes one
document and produces one set of pairs, with each pair of the form (w, 1), where
w is one of the words in the document. However, if we do so, then the output
RDD is a list of sets, each set consisting of all the words of one document,
each word paired with the integer 1. If we want to duplicate the Map function
described in Example 2.1, then we need to use Spark’s Flatmap transformation.
That operation applied to the RDD of documents will produce another RDD,
each of whose elements is a single pair (w, 1). ✷
Spark also provides an operation similar to a limited form of Map, called
Filter. Instead of a function as a parameter, the Filter transformation takes a
predicate that applies to the type of objects in the input RDD. The predicate
2.4. EXTENSIONS TO MAPREDUCE 45
returns true or false for each object, and the output RDD of a Filter transformation consists of only those objects in the input RDD for which the filter
function returns true.
Example 2.8 : Continuing Example 2.7, suppose we want to avoid counting
stop words: the most common words like “the” or “and.” We could write a
filter function that has built into it the list of words we want to eliminate.
When applied to a pair (w, 1), this function returns true if and only if w is not
on the list. We can then write a Spark program that first applies Flatmap to
the RDD of documents, producing an RDD R1 consisting of a pair (w, 1) for
each occurrence of the word w in any of the documents. The program then
applies the stop-word-eliminating Filter to R1, producing another RDD, R2.
The latter RDD consists of a pair (w, 1) for each occurrence of word w in any
of the documents, but only if w is not a stop word. ✷
Reduce
In Spark, the Reduce operation is an action, not a transformation. That is,
the operation Reduce applies to an RDD but returns a value and not another
RDD. Reduce takes a parameter that is a function which takes two elements of
some particular type T and returns another element of the same type T . When
applied to an RDD whose elements are of type T , Reduce is applied repeatedly
to each pair of consecutive elements, reducing them to a single element. When
only one element remains, that becomes the result of the Reduce operation.
For example, if the parameter is the addition function, and this instance
of Reduce is applied to an RDD whose elements are integers, then the result
will be a single integer that is the sum of all the integers in the RDD. As long
as the function parameter is an associative and commutative function, such as
addition, it does not matter in which order elements of the input RDD are
combined. However, it is also possible to use an arbitrary function, as long as
we are satisfied with combination of elements in any order.
Relational Database Operations
There are a number of built-in Spark operations that behave like relationalalgebra operators on relations that are represented by RDD’s. That is, think
of the elements of the RDD’s as tuples of a relation. The transformation Join
takes two RDD’s, each representing one of the relations. The type of each RDD
must be a key-value pair, and the key types of both relations must be the same.
The Join transformation then looks for two objects, one from each of its input
RDD’s, such that the key values are the same, say (k, x) and (k, y). For each
such pair found, Join produces the key-value pair
k,(x, y)

, and the output
RDD consists of all such objects.
The group-by operation of SQL is also implemented in Spark by the transformation GroupByKey. This transformation takes as input an RDD whose
type is key-value pairs. The output RDD is also a set of key-value pairs wit
46 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
the same key type. The value type for the output is a list of values of the input
type. GroupByKey sorts its input RDD by key and for each key k produces
the pair (k, [v1, v2, . . . , vn]) such that the vi
’s are all the values associated with
key k in the input RDD. Notice that GroupByKey is exactly the operation that
is performed behind the scenes by MapReduce in order to group the output of
the Map function by key.
2.4.3 Spark Implementation
There are a number of ways that Spark implementation differs from Hadoop or
other MapReduce implementations. We shall discuss two important improvements: lazy evaluation of RDD’s and lineage for RDD’s. Before we do, we
should mention one way in which Spark is similar to MapReduce: the way
large RDD’s are managed.
Recall that when applying Map to a large file, MapReduce divides that file
into chunks and creates a Map task for each chunk or group of chunks. The
chunks and their tasks are typically distributed among many different compute
nodes. Likewise, many Reduce tasks can run in parallel on different compute
nodes, and each of these tasks takes a portion of the entire set of key-value pairs
that are passed from Map to Reduce. Spark also allows any RDD to be divided
into chunks, which it calls splits. Each split can be given to a different compute
node, and the transformation on that RDD can be performed in parallel on
each of the splits.
Lazy Evaluation
As mentioned in Section 2.4.1, it is common for workflow systems to exploit
the blocking property for error handling. To do so, a function is applied to a
single intermediate file (analogous to an RDD) and the output of that function
is made available to consumers of that output only after the function completes.
However, Spark does not actually apply transformations to RDD’s until it is
required to do so, typically because it must apply some action, e.g., storing
a computed RDD in the surrounding file system or returning a result to an
application.
The benefit of this strategy of lazy evaluation is that many RDD’s are not
constructed all at once. When one split of an RDD is created at a node, it may
be used immediately at the same compute node to apply another transformation. The benefit of this startegy is that this RDD is never stored on disk and
never transmitted to another compute nodes, thus saving orders of magnitude
in running time in some cases.
Example 2.9 : Consider the situation suggested in Example 2.8, where Flatmap is applied to one RDD, which we shall refer to as R0. Note that RDD R0
is created by converting the external file of documents into an RDD. As R0 is
a large file, we shall want to divide it into splits and operate on the splits in
parallel.
2.4. EXTENSIONS TO MAPREDUCE 47
The first transformation on R0 applies Flatmap to create a set of pairs
(w, 1) for each word. For each split of R0, a split of the resulting RDD, which
we called R1 in Example 2.8, is created at the same compute node. This split
of R1 is then passed to the transformation Filter, which eliminates pairs whose
first component is a stop word. When this Filter is applied to the split, the
result is a split of the RDD R2, located at the same compute node.
However, neither the Flatmap nor Filter transformations occur unless an
action is applied to R2. For example, the Spark program may store R2 in the
surrounding file system or perform a Reduce operation that counts occurrences
of the words. Only when the program reaches this action does Spark apply the
Flatmap and Filter transformations to R0, running these transformations at
each of the compute nodes that holds a split of R0, in parallel. Thus, the splits
of R1 and R2 exist only locally at the compute node that created them, and
unless the programmer explicitly calls for them to be maintained, these splits
are dropped as soon as they are used locally. ✷
Resilience of RDD’s
One may naturally ask what happens in Example 2.9 if a compute node fails
after creating a split of R1 and before transforming that split into a split of
R2. Since R1 is not backed up to the file system, is it not lost forever? Spark’s
substitute for redundant storage of intermediate values is to record the lineage
of every RDD it creates. The lineage tells the Spark system how to recreate
the RDD, or a split of the RDD, if that is needed.
Example 2.10 : Considering again the situation described in Example 2.9,
the lineage for R2 would say that it is created by applying to R1 the particular
Filter operation that eliminates stop words. In turn, R1 is created from R0 by
the Flatmap operation that turns words of a document into (w, 1) pairs. And
R0 was created from a particular file of the surrounding file system.
For instance, if we lose a split of R2, we know we can reconstruct it from
the corresponding split of R1. But since that split exists at the same compute
node, we’ve probably lost that split also. If so, we could reconstruct it from
the corresponding split of R0, which is also probably lost if this compute node
has failed. But we know that we can reconstruct the split of R0 from the
surrounding file system, which is presumably redundant and will not be lost.
Thus, Spark will find another compute node, reconstruct the lost split of R0
from the file system there, and then apply the known transformations needed
to reconstruct the corresponding splits of R1 and R2. ✷
As we can see from Example 2.10, recovery from a node failure can be more
complex in Spark than in MapReduce or in workflow systems that store intermediate values redundantly. However, the tradeoff of more complex recovery
when things go wrong against greater speed when things go right is generally a
good one. The faster a Spark program runs, the less chance there is of a node
failure while running.
48 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
We should contrast Spark’s need to be able to execute a program in the face
of failures with the need for redundant storage of files that are expected to exist
for a long period. Over a long period, failures are almost certain, so we are very
likely to lose pieces of a file if we do not store it redundantly. But over a short
period – minutes or even hours – there is a good chance of avoiding failures.
Thus, it is reasonable to be willing to pay more when there is a failure in this
case.
2.4.4 TensorFlow
TensorFlow is an open-source system developed initially at Google to support
machine-learning applications. Like Spark, TensorFlow provides a programming interface in which one writes a sequence of steps. Programs are typically
acyclic, although like Spark it is possible to iterate blocks of code.
One major difference between Spark and TensorFlow is the type of data
that is passed between steps of the program. In place of the RDD, TensorFlow
uses tensors; a tensor is simply a multidimensional matrix.
Example 2.11 : A constant, e.g. 3.14159, is regarded as a 0-dimensional tensor. A vector is a 1-dimensional tensor. For instance, the vector (1, 2, 3) can
be written in tensorFlow as [1., 2., 3.]. A matrix is a 2-dimensional tensor. For
example, the matrix
1 2 3 4
5 6 7 8
9 10 11 12
is expressed as [[1., 2., 3., 4.], [5., 6., 7., 8.], [9., 10., 11., 12.]].
Higher-dimensional arrays are possible as well. For instance, a 2-by-2-by-2
cube of 0’s is represented as [[[0., 0.], [0., 0.]], [[0., 0.], [0., 0.]]]. ✷
Although tensors are in fact a restricted form of RDD, the power of TensorFlow comes from its selection of built-in operations. Linear algebra operations
are available as functions. For example, if you want matrix C to be the product
of matrices A and B, you can write
C = tensorflow.matmul(A,B)
Even more powerful are the common approaches to machine learning that
are built in as operations. a single statement in the TensorFlow language can
cause a model that is a tensor to be constructed from training data, which is
also represented as a tensor, using a method like gradient descent. (We discuss
gradient descent in Sections 9.4.5 and 12.3.4).
2.4. EXTENSIONS TO MAPREDUCE 49
2.4.5 Recursive Extensions to MapReduce
Many large-scale computations are really recursions. An important example is
PageRank, which is the subject of Chapter 5. That computation is, in simple terms, the computation of the fixedpoint of a matrix-vector multiplication.
It is computed under MapReduce systems by the iterated application of the
matrix-vector multiplication algorithm described in Section 2.3.1, or by a more
complex strategy that we shall introduce in Section 5.2. The iteration typically continues for an unknown number of steps, each step being a MapReduce
job, until the results of two consecutive iterations are sufficiently close that
we believe convergence has occurred. A second important example of a recursive algorithm on massive data is gradient descent, which we just mentioned in
connection with TensorFlow.
Recursions present a problem for failure recovery. Recursive tasks inherently
lack the blocking property necessary for independent restart of failed tasks. It
is impossible for a collection of mutually recursive tasks, each of which has an
output that is input to at least some of the other tasks, to produce output only
at the end of the task. If they all followed that policy, no task would ever receive
any input, and nothing could be accomplished. As a result, some mechanism
other than simple restart of failed tasks must be implemented in a system that
handles recursive workflows (flow graphs that are not acyclic). We shall start
by studying an example of a recursion implemented as a workflow, and then
discuss approaches to dealing with task failures.
Example 2.12 : Suppose we have a directed graph whose arcs are represented
by the relation E(X, Y ), meaning that there is an arc from node X to node Y .
We wish to compute the paths relation P(X, Y ), meaning that there is a path
of length 1 or more from node X to node Y . That is, P is the transitive closure
of E. A simple recursive algorithm to do so is:
1. Start with P(X, Y ) = E(X, Y ).
2. While changes to the relation P occur, add to P all tuples in
πX,Y
P(X, Z) ⊲⊳ P(Z, Y )

That is, find pairs of nodes X and Y such that for some node Z there is
known to be a path from X to Z and also a known path from Z to Y .
Figure 2.7 suggests how we could organize recursive tasks to perform this
computation. There are two kinds of tasks: Join tasks and Dup-elim tasks.
There are n Join tasks, for some n, and each corresponds to a bucket of a hash
function h. A path tuple P(a, b), when it is discovered, becomes input to two
Join tasks: those numbered h(a) and h(b). The job of the ith Join task, when
it receives input tuple P(a, b), is to find certain other tuples seen previously
(and stored locally by that task).
1. Store P(a, b) locally
50 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
Join
task
0
Join
task
1
Join
task
i
task
0
Dup−elim
task
1
Dup−elim
task
j
Dup−elim
.
.
.
.
.
.
.
.
.
.
.
.
P(a,b) if
h(a) = i or
h(b) = i
P(c,d) if
g(c,d) = j
P(c,d) if never
seen before
To join task h(d)
To join task h(c)
Figure 2.7: Implementation of transitive closure by a collection of recursive
tasks
2. If h(a) = i then look for tuples P(x, a) and produce output tuple P(x, b).
3. If h(b) = i then look for tuples P(b, y) and produce output tuple P(a, y).
Note that in rare cases, we have h(a) = h(b), so both (2) and (3) are executed.
But generally, only one of these needs to be executed for a given tuple.
There are also m Dup-elim tasks, and each corresponds to a bucket of a
hash function g that takes two arguments. If P(c, d) is an output of some Join
task, then it is sent to Dup-elim task j = g(c, d). On receiving this tuple, the
jth Dup-elim task checks that it has not received this tuple before, since its job
is duplicate elimination. If previously received, the tuple is ignored. But if this
tuple is new, it is stored locally and sent to two Join tasks, those numbered
h(c) and h(d).
Every Join task has m output files – one for each Dup-elim task – and every
Dup-elim task has n output files – one for each Join task. These files may be
distributed according to any of several strategies. Initially, the E(a, b) tuples
representing the arcs of the graph are distributed to the Dup-elim tasks, with
E(a, b) being sent as P(a, b) to Dup-elim task g(a, b). The master controller
2.4. EXTENSIONS TO MAPREDUCE 51
waits until each Join task has processed its entire input for a round. Then,
all output files are distributed to the Dup-elim tasks, which create their own
output. That output is distributed to the Join tasks and becomes their input
for the next round. ✷
In Example 2.12 it is not essential to have two kinds of tasks. Rather, Join
tasks could eliminate duplicates as they are received, since they must store
their previously received inputs anyway. However, this arrangement has an
advantage when we must recover from a task failure. If each task stores all
the output files it has ever created, and we place Join tasks on different racks
from the Dup-elim tasks, then we can deal with any single compute node or
single rack failure. That is, a Join task needing to be restarted can get all the
previously generated inputs that it needs from the Dup-elim tasks, and vice
versa.
In the particular case of computing transitive closure, it is not necessary to
prevent a restarted task from generating outputs that the original task generated previously. In the computation of the transitive closure, the rediscovery of
a path does not influence the eventual answer. However, many computations
cannot tolerate a situation where both the original and restarted versions of a
task pass the same output to another task. For example, if the final step of the
computation were an aggregation, say a count of the number of nodes reached
by each node in the graph, then we would get the wrong answer if we counted
a path twice.
There are at least three different approaches that have been used to deal
with failures while executing a recursive program.
1. Iterated MapReduce: Write the recursion as repeated execution of a MapReduce job or of a sequence of MapReduce jobs. We can then rely on the
failure mechanism of the MapReduce implementation to handle failures
at any step. The first example of such a system was HaLoop (see the
bibliographic notes for this chapter).
2. The Spark Approach: The Spark language actually includes iterative
statements, such as for-loops that allow the implementation of recursions.
Here, failure management is implemented using the lazy-evaluation and
lineage mechanisms of Spark. In addition, the Spark programmer has
options to store intermediate states of the recursion.
3. Bulk-Synchronous Systems: These systems use a graph-based model of
computation that we shall describe next. They typically use another
resilience approach: periodic checkpointing.
2.4.6 Bulk-Synchronous Systems
Another approach to implementing recursive algorithms on a computing cluster
is represented by the Google’s Pregel system, which was the first example of a
52 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
graph-based, bulk-synchronous system for processing massive amounts of data.
Such a system views its data as a graph. Each node of the graph corresponds
roughly to a task (although in practice many nodes of a large graph would be
bundled into a single task, as in the Join tasks of Example 2.12). Each graph
node generates output messages that are destined for other nodes of the graph,
and each graph node processes the inputs it receives from other nodes.
Example 2.13 : Suppose our data is a collection of weighted arcs of a graph,
and we want to find, for each node of the graph, the length of the shortest path
to each of the other nodes. As the algorithm executes, each node a will store
a set of pairs (b, w), where w is the length of the shortest path from node a to
node b that is currently known.
Initially, each graph node a stores the set of pairs (b, w) such that there
is an arc from a to b of weight w. These facts are sent to all other nodes, as
triples (a, b, w), with the intended meaning that node a knows about a path of
length w to node b.
8 When the node a receives a triple (c, d, w), it must decide
whether this fact implies a shorter path than a already knows about from itself
to node d. Node a looks up its current distance to c; that is, it finds the pair
(c, v) stored locally, if there is one. It also finds the pair (d, u) if there is one.
If w + v < u, then the pair (d, u) is replaced by (d, w + v), and if there was
no pair (d, u), then the pair (d, w + v) is stored at the node a. Also, the other
nodes are sent the message (a, d, w + v) in either of these two cases. ✷
Computations in Pregel are organized into supersteps. In one superstep, all
the messages that were received by any of the nodes at the previous superstep
(or initially, if it is the first superstep) are processed, and then all the messages
generated by those nodes are sent to their destination. It is this packaging of
many messages into one that gives this approach the name “bulk-synchronous.”
There is a very important advantage to grouping messages in this way.
Communication over a network generally requires a large amount of overhead
to send any message, however short. Suppose that in Example 2.13 we sent
a single new shortest-distance fact to the relevant node every time one was
discovered. The number of messages sent would be enormous if the graph was
large, and it would not be realistic to implement such an algorithm. However,
in a bulk-synchonous system, a task that has the responsibility for managing
many nodes of the graph can bundle together all the messages being sent by
its nodes to any of the nodes being managed by another task. That choice
typically saves orders of magnitude in the time required to send all the needed
messages.
Failure Management in Pregel
In case of a compute-node failure, there is no attempt to restart the failed tasks
at that compute node. Rather, Pregel checkpoints its entire computation after
8This algorithm uses much too much communication, but it will serve as a simple example
of the Pregel computation model.
2.5. THE COMMUNICATION-COST MODEL 53
some of the supersteps. A checkpoint consists of making a copy of the entire
state of each task, so it can be restarted from that point if necessary. If any
compute node fails, the entire job is restarted from the most recent checkpoint.
Although this recovery strategy causes many tasks that have not failed to
redo their work, it is satisfactory in many situations. Recall that the reason
MapReduce systems support restart of only the failed tasks is that we want
assurance that the expected time to complete the entire job in the face of failures is not too much greater than the time to run the job with no failures.
Any failure-management system will have that property as long as the time
to recover from a failure is much less than the average time between failures.
Thus, it is only necessary that Pregel checkpoints its computation after a number of supersteps such that the probability of a failure during that number of
supersteps is low.
2.4.7 Exercises for Section 2.4
! Exercise 2.4.1 : Suppose a job consists of n tasks, each of which takes time t
seconds. Thus, if there are no failures, the sum over all compute nodes of the
time taken to execute tasks at that node is nt. Suppose also that the probability
of a task failing is p per job per second, and when a task fails, the overhead of
management of the restart is such that it adds 10t seconds to the total execution
time of the job. What is the total expected execution time of the job?
! Exercise 2.4.2 : Suppose a Pregel job has a probability p of a failure during
any superstep. Suppose also that the execution time (summed over all compute
nodes) of taking a checkpoint is c times the time it takes to execute a superstep.
To minimize the expected execution time of the job, how many supersteps
should elapse between checkpoints?
2.5 The Communication-Cost Model
In this section we shall introduce a model for measuring the quality of algorithms
implemented on a computing cluster of the type so far discussed in this chapter.
We assume the computation is described by an acyclic workflow, as discussed
in Section 2.4.1. For many applications, the bottleneck is moving data among
tasks, such as transporting the outputs of Map tasks to their proper Reduce
tasks. As an example, we explore the computation of multiway joins as single
MapReduce jobs, and we see that in some situations, this approach is more
efficient than the straightforward cascade of 2-way joins.
2.5.1 Communication Cost for Task Networks
Imagine that an algorithm is implemented by an acyclic network of tasks. These
tasks could be Map tasks feeding Reduce tasks, as in a standard MapReduce
algorithm, or they could be several MapReduce jobs cascaded, or a more general
54 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
workflow structure, such as a collection of tasks each of which implements the
workflow of Fig. 2.6.9 The communication cost of a task is the size of the input
to the task. This size can be measured in bytes. However, since we shall be
using relational database operations as examples, we shall often use the number
of tuples as a measure of size.
The communication cost of an algorithm is the sum of the communication
cost of all the tasks implementing that algorithm. We shall focus on the communication cost as the way to measure the efficiency of an algorithm. In particular,
we do not consider the amount of time it takes each task to execute when estimating the running time of an algorithm. While there are exceptions, where
execution time of tasks dominates, these exceptions are rare in practice. We
can explain and justify the importance of communication cost as follows.
• The algorithm executed by each task tends to be very simple, often linear
in the size of its input.
• The typical interconnect speed for a computing cluster is one gigabit per
second. That may seem like a lot, but it is slow compared with the speed
at which a processor executes instructions. Moreover, in many cluster
architectures, there is competition for the interconnect when several compute nodes need to communicate at the same time. As a result, the
compute node can do a lot of work on a received input element in the
time it takes to deliver that element.
• Even if a task executes at a compute node that has a copy of the chunk(s)
on which the task operates, that chunk normally will be stored on disk,
and the time taken to move the data into main memory may exceed the
time needed to operate on the data once it is available in memory.
Assuming that communication cost is the dominant cost, we might still ask
why we count only input size, and not output size. The answer to this question
involves two points:
1. If the output of one task τ is input to another task, then the size of τ’s
output will be accounted for when measuring the input size for the receiving task. Thus, there is no reason to count the size of any output except
for those tasks whose output forms the result of the entire algorithm.
2. But in practice, the algorithm output is rarely large compared with the
input or the intermediate data produced by the algorithm. The reason
is that massive outputs cannot be used unless they are summarized or
aggregated in some way. For example, although we talked in Example 2.12
of computing the entire transitive closure of a graph, in practice we would
want something much simpler, such as the count of the number of nodes
9Recall that this figure represented functions, not tasks. As a network of tasks, there
would be, for example, many tasks implementing function f, each of which feeds data to each
of the tasks for function g and each of the tasks for function i.
2.5. THE COMMUNICATION-COST MODEL 55
reachable from each node, or the set of nodes reachable from a single
node.
Example 2.14 : Let us evaluate the communication cost for the join algorithm
from Section 2.3.7. Suppose we are joining R(A, B) ⊲⊳ S(B, C), and the sizes
of relations R and S are r and s, respectively. Each chunk of the files holding
R and S is fed to one Map task, so the sum of the communication costs for all
the Map tasks is r + s. Note that in a typical execution, the Map tasks will
each be executed at a compute node holding a copy of the chunk to which it
applies. Thus, no internode communication is needed for the Map tasks, but
they still must read their data from disk. Since all the Map tasks do is make a
simple transformation of each input tuple into a key-value pair, we expect that
the computation cost will be small compared with the communication cost,
regardless of whether the input is local to the task or must be transported to
its compute node.
The sum of the outputs of the Map tasks is roughly as large as their inputs. Each output key-value pair is sent to exactly one Reduce task, and it is
unlikely that this Reduce task will execute at the same compute node. Therefore, communication from Map tasks to Reduce tasks is likely to be across the
interconnect of the cluster, rather than memory-to-disk. This communication
is O(r + s), so the communication cost of the join algorithm is O(r + s).
The Reduce tasks execute the reducer (application of the Reduce function
to a key and its associated value list) for one or more values of attribute B.
Each reducer takes the inputs it receives and divides them between tuples that
came from R and those that came from S. Each tuple from R pairs with each
tuple from S to produce one output. The output size for the join can be either
larger or smaller than r + s, depending on how likely it is that a given R-tuple
joins with a given S-tuple. For example, if there are many different B-values,
we would expect the output to be small, while if there are few B-values, a large
output is likely.
If the output is large, then the computation cost of generating all the outputs
from a reducer could be much larger than O(r+s). However, we shall rely on our
supposition that if the output of the join is large, then there is probably some
aggregation being done to reduce the size of the output. It will be necessary to
communicate the result of the join to another collection of tasks that perform
this aggregation, and thus the communication cost will be at least proportional
to the computation needed to produce the output of the join. ✷
2.5.2 Wall-Clock Time
While communication cost often influences our choice of algorithm to use in
a cluster-computing environment, we must also be aware of the importance of
wall-clock time, the time it takes a parallel algorithm to finish. Using careless
reasoning, one could minimize total communication cost by assigning all the
work to one task, and thereby minimize total communication. However, the
56 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
wall-clock time of such an algorithm would be quite high. The algorithms we
suggest, or have suggested so far, have the property that the work is divided
fairly among the tasks. Therefore, the wall-clock time would be approximately
as small as it could be, given the number of compute nodes available.
2.5.3 Multiway Joins
To see how analyzing the communication cost can help us choose an algorithm
in the cluster-computing environment, we shall examine carefully the case of a
multiway join. There is a general theory in which we:
1. Select certain attributes of the relations involved in the natural join of
three or more relations to have their values hashed, each to some number
of buckets.
2. Select the number of buckets for each of these attributes, subject to the
constraint that the product of the numbers of buckets for each attribute
is k, the number of reducers that will be used.
3. Identify each of the k reducers with a vector of bucket numbers. These
vectors have one component for each of the attributes selected at step (1).
4. Send tuples of each relation to all those reducers where it might find tuples
to join with. That is, the given tuple t will have values for some of the
attributes selected at step (1), so we can apply the hash function(s) to
those values to determine certain components of the vector that identifies
the reducers. Other components of the vector are unknown, so t must
be sent to reducers for all vectors having any value in these unknown
components.
Some examples of this general technique appear in the exercises. Here,
we shall look only at the join R(A, B) ⊲⊳ S(B, C) ⊲⊳ T (C, D) as an example.
Suppose that the relations R, S, and T have sizes r, s, and t, respectively, and
for simplicity, suppose p is the probability that
1. An R-tuple and and S-tuple agree on B, and also the probability that
2. An S-tuple and a T -tuple agree on C.
If we join R and S first, using the MapReduce algorithm of Section 2.3.7,
then the communication cost is O(r + s), and the size of the intermediate join
R ⊲⊳ S is prs. When we join this result with T , the communication of this
second MapReduce job is O(t + prs). Thus, the entire communication cost of
the algorithm consisting of two 2-way joins is O(r + s + t + prs). If we instead
join S and T first, and then join R with the result, we get another algorithm
whose communication cost is O(r + s + t + pst).
A third way to take this join is to use a single MapReduce job that joins
the three relations at once. Suppose that we plan to use k reducers for this
2.5. THE COMMUNICATION-COST MODEL 57
job. Pick numbers b and c representing the number of buckets into which we
shall hash B- and C-values, respectively. Let h be a hash function that sends
B-values into b buckets, and let g be another hash function that sends C-values
into c buckets. We require that bc = k; that is, each reducer corresponds to
a pair of buckets, one for the B-value and one for the C-value. The reducer
corresponding to bucket pair (i, j) is responsible for joining the tuples R(u, v),
S(v, w), and T (w, x) whenever h(v) = i and g(w) = j.
As a result, the Map tasks that send tuples of R, S, and T to the reducers
that need them must send R- and T -tuples to more than one reducer. For an
S-tuple S(v, w), we know the B- and C-values, so we can send this tuple only to
the reducer for
h(v), g(w)

. However, consider an R-tuple R(u, v). We know
it only needs to go to reducers that correspond to
h(v), y
, for some y. But
we don’t know y; the value of C could be anything as far as we know. Thus,
we must send R(u, v) to c reducers, since y could be any of the c buckets for
C-values. Similarly, we must send the T -tuple T (w, x) to each of the reducers

z, g(w)

for any z. There are b such reducers.
0 1 2 3
2
1
0
g(T.C) = 1
g(C) = h(S.B) = 2 and g(S.C) = 1
h(B) =
h(R.B) = 2
3
Figure 2.8: Sixteen reducers together perform a 3-way join
Example 2.15 : Suppose that b = c = 4, so k = 16. The sixteen reducers
can be thought of as arranged in a rectangle, as suggested by Fig. 2.8. There,
we see a hypothetical S-tuple S(v, w) for which h(v) = 2 and g(w) = 1. This
tuple is sent by its Map task only to the reducer for key (2, 1). We also see
an R-tuple R(u, v). Since h(v) = 2, this tuple is sent to all reducers (2, y), for
y = 1, 2, 3, 4. Finally, we see a T -tuple T (w, x). Since g(w) = 1, this tuple is
sent to all reducers (z, 1) for z = 1, 2, 3, 4. Notice that these three tuples join,
and they meet at exactly one reducer, the reducer for key (2, 1). ✷
Now, suppose that the sizes of R, S, and T are different; recall we use r,
s, and t, respectively, for those sizes. If we hash B-values to b buckets
58 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
Computation Cost of the 3-Way Join
Each of the reducers must compute the join of parts of the three relations,
and it is reasonable to ask whether this join can be taken in time that is
as small as possible: linear in the sum of the sizes of the input and output
for that Reduce task. While more complex joins might not be computable
in linear time, the join of our running example can be executed at each
Reduce process efficiently. First, create an index on R.B, to organize
the R-tuples received. Likewise, create an index on T.C for the T -tuples.
Then, consider each received S-tuple, S(v, w). Use the index on R.B to
find all R-tuples with R.B = v and use the index on T.C to find all
T -tuples with T.C = w.
C-values to c buckets, where bc = k, then the total communication cost for
moving the tuples to the proper reducers is the sum of:
1. s to send each tuple S(v, w) to the reducer
h(v), g(w)

.
2. cr to send each tuple R(u, v) to the c reducers
h(v), y
for each of the c
possible values of y.
3. bt to send each tuple T (w, x) to the b reducers
z, g(w)

for each of the b
possible values of z.
There is also a cost r + s + t to make each tuple of each relation be input to
one of the Map tasks. This cost is fixed, independent of b, c, and k.
We must select b and c, subject to the constraint bc = k, to minimize
s + cr + bt. We shall use the technique of Lagrangean multipliers to find the
place where the function s + cr + bt − λ(bc − k) has its derivatives with respect
to b and c equal to 0. That is, we must solve the equations r − λb = 0 and
t − λc = 0. Since r = λb and t = λc, we may multiply corresponding sides of
these equations to get rt = λ
2
bc. Since bc = k, we get rt = λ
2k, or λ =
p
rt/k.
Thus, the minimum communication cost is obtained when c = t/λ =
p
kt/r,
and b = r/λ =
p
kr/t.
If we substitute these values into the formula s + cr + bt, we get s + 2√
krt.
That is the communication cost for the Reduce tasks, to which we must add
the cost s + r + t for the communication cost of the Map tasks. The total
communication cost is thus r + 2s + t + 2√
krt. In most circumstances, we can
neglect r + t, because it will be less than 2√
krt, usually by a factor of O(
√
k).
Example 2.16 : Let us see under what circumstances the 3-way join has lower
communication cost than the cascade of two 2-way joins. To make matters
simple, let us assume that R, S, and T are all the same relation R, which
represents the “friends” relation in a social network like Facebook. There
2.5. THE COMMUNICATION-COST MODEL 59
roughly a billion subscribers on Facebook, with an average of 300 friends each, so
relation R has r = 3 × 1011 tuples. Suppose we want to compute R ⊲⊳ R ⊲⊳ R,
perhaps as part of a calculation to find the number of friends of friends of
friends each subscriber has, or perhaps just the person with the largest number
of friends of friends of friends.10 The communication cost of the 3-way join of R
with itself is 4r + 2r
√
k; 3r represents the cost of the Map tasks, and r + 2√
kr2
is the cost of the Reduce tasks. Since we assume r = 3 × 1011, this cost is
1.2 × 1012 + 6 × 1011√
k.
Now consider the communication cost of joining R with itself, and then
joining the result with R again. The Map and Reduce tasks for the first join each
have a cost of 2r, so the first join only has communication cost 4r = 1.2 × 1012
.
But the size of R ⊲⊳ R is large. We cannot say exactly how large, since friends
tend to fall into cliques, and therefore a person with 300 friends will have many
fewer than the maximum possible number of friends of friends, which is 90,000.
Let us estimate conservatively that the size of R ⊲⊳ R is not 300r, but only
30r, or 9 × 1012. The communication cost for the second join of (R ⊲⊳ R) ⊲⊳ R
is thus 1.8 × 1013 + 6 × 1011. The total cost of the two joins is therefore
1.2 × 1012 + 1.8 × 1013 + 6 × 1011 = 1.98 × 1013
.
We must ask whether the cost of the 3-way join, which is
1.2 × 1012 + 6 × 1011√
k
is less than 1.98 × 1013. That is so, provided 6 × 1011√
k < 1.86 × 1013
√
, or
k < 31. That is, the 3-way join will be preferable provided we use no more
than 312 = 961 reducers. ✷
2.5.4 Exercises for Section 2.5
Exercise 2.5.1 : What is the communication cost of each of the following
algorithms, as a function of the size of the relations, matrices, or vectors to
which they are applied?
(a) The matrix-vector multiplication algorithm of Section 2.3.2.
(b) The union algorithm of Section 2.3.6.
(c) The aggregation algorithm of Section 2.3.8.
(d) The matrix-multiplication algorithm of Section 2.3.10.
! Exercise 2.5.2 : Suppose relations R, S, and T have sizes r, s, and t, respectively, and we want to take the 3-way join R(A, B) ⊲⊳ S(B, C) ⊲⊳ T (A, C),
using k reducers. We shall hash values of attributes A, B, and C to a, b, and c
buckets, respectively, where abc = k. Each reducer is associated with a vector
10This person, or more generally, people with large extended circles of friends, are good
people to use to start a marketing campaign by giving them free samples.
60 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
Star Joins
A common structure for data mining of commercial data is the star join.
For example, a chain store like Walmart keeps a fact table whose tuples each represent a single sale. This relation looks like F(A1, A2, . . .),
where each attribute Ai
is a key representing one of the important components of the sale, such as the purchaser, the item purchased, the store
branch, or the date. For each key attribute there is a dimension table
giving information about the participant. For instance, the dimension table D(A1, B11, B12, . . .) might represent purchasers. A1 is the purchaser
ID, the key for this relation. The B1i
’s might give the purchaser’s name,
address, phone, and so on. Typically, the fact table is much larger than
the dimension tables. For instance, there might be a fact table of a billion
tuples and ten dimension tables of a million tuples each.
Analysts mine this data by asking analytic queries that typically join
the fact table with several of the dimension tables (a “star join”) and then
aggregate the result into a useful form. For instance, an analyst might ask
“give me a table of sales of pants, broken down by region and color, for
each month of 2016.” Under the communication-cost model of this section,
joining the fact table and dimension tables by a multiway join is almost
certain to be more efficient than joining the relations in pairs. In fact, it
may make sense to store the fact table over however many compute nodes
are available, and replicate the dimension tables permanently in exactly
the same way as we would replicate them should we take the join of the
fact table and all the dimension tables. In this special case, only the
key attributes (the A’s above) are hashed to buckets, and the number of
buckets for each key attribute is proportional to the size of its dimension
table.
of buckets, one for each of the three hash functions. Find, as a function of r, s,
t, and k, the values of a, b, and c that minimize the communication cost of the
algorithm.
! Exercise 2.5.3 : Suppose we take a star join of a fact table F(A1, A2, . . . , Am)
with dimension tables Di(Ai
, Bi) for i = 1, 2, . . . , m. Let there be k reducers,
each associated with a vector of buckets, one for each of the key attributes
A1, A2, . . . , Am. Suppose the number of buckets into which we hash Ai
is ai
.
Naturally, a1a2 · · · am = k. Finally, suppose each dimension table Di has size
di
, and the size of the fact table is much larger than any of these sizes. Find
the values of the ai
’s that minimize the cost of taking the star join as one
MapReduce operation.
2.6. COMPLEXITY THEORY FOR MAPREDUCE 61
2.6 Complexity Theory for MapReduce
Now, we shall explore the design of MapReduce algorithms in more detail.
Section 2.5 introduced the idea that communication between the Map and Reduce tasks is often the bottleneck when performing a MapReduce computation.
Here, we shall look at how the communication cost relates to other desiderata
for MapReduce algorithms, in particular our desire to shrink the wall-clock time
and to execute each reducer in main memory. Recall that a “reducer” is the
execution of the Reduce function on a single key and its associated value list.
The point of the exploration in this section is that for many problems there is a
spectrum of MapReduce algorithms requiring different amounts of communication. Moreover, the less communication an algorithm uses, the worse it may be
in other respects, including wall-clock time and the amount of main memory it
requires.
2.6.1 Reducer Size and Replication Rate
Let us now introduce the two parameters that characterize families of MapReduce algorithms. The first is the reducer size, which we denote by q. This
parameter is the upper bound on the number of values that are allowed to appear in the list associated with a single key. Reducer size can be selected with
at least two goals in mind.
1. By making the reducer size small, we can force there to be many reducers,
i.e., many different keys according to which the problem input is divided
by the Map tasks. If we also create many Reduce tasks – even one for
each reducer – then there will be a high degree of parallelism, and we can
expect a low wall-clock time.
2. We can choose a reducer size sufficiently small that we are certain the
computation associated with a single reducer can be executed entirely in
the main memory of the compute node where its Reduce task is located.
Regardless of the computation done by the reducers, the running time
will be greatly reduced if we can avoid having to move data repeatedly
between main memory and disk.
The second parameter is the replication rate, denoted r. We define r to
be the number of key-value pairs produced by all the Map tasks on all the
inputs, divided by the number of inputs. That is, the replication rate is the
average communication from Map tasks to Reduce tasks (measured by counting
key-value pairs) per input.
Example 2.17 : Let us consider the one-pass matrix-multiplication algorithm
of Section 2.3.10. Suppose that all the matrices involved are n × n matrices.
Then the replication rate r is equal to n. That fact is easy to see, since for
each element mij , there are n key-value pairs produced; these have all keys of
62 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
the form (i, k), for 1 ≤ k ≤ n. Likewise, for each element of the other matrix,
say njk, we produce n key-value pairs, each having one of the keys (i, k), for
1 ≤ i ≤ n. In this case, not only is n the average number of key-value pairs
produced for an input element, but each input produces exactly this number of
pairs.
We also see that q, the required reducer size, is 2n. That is, for each key
(i, k), there are n key-value pairs representing elements mij of the first matrix
and another n key-value pairs derived from the elements njk of the second
matrix. While this pair of values represents only one particular algorithm for
one-pass matrix multiplication, we shall see that it is part of a spectrum of
algorithms, and in fact represents an extreme point, where q is as small as can
be, and r is at its maximum. More generally, there is a tradeoff between r and
q, that can be expressed as qr ≥ 2n
2
. ✷
2.6.2 An Example: Similarity Joins
To see the tradeoff between r and q in a realistic situation, we shall examine a
problem known as similarity join. In this problem, we are given a large set of
elements X and a similarity measure s(x, y) that tells how similar two elements
x and y of set X are. In Chapter 3 we shall learn about the most important
notions of similarity and also learn some tricks that let us find similar pairs
quickly. But here, we shall consider only the raw form of the problem, where
we have to look at each pair of elements of X and determine their similarity by
applying the function s. We assume that s is symmetric, so s(x, y) = s(y, x),
but we assume nothing else about s. The output of the algorithm is those pairs
whose similarity exceeds a given threshold t.
For example, let us suppose we have a collection of one million images, each
of size one megabyte. Thus, the dataset has size one terabyte. We shall not
try to describe the similarity function s, but it might, say, involve giving higher
values when images have roughly the same distribution of colors or when images
have corresponding regions with the same distribution of colors. The goal would
be to discover pairs of images that show the same type of object or scene. This
problem is extremely hard, but classifying by color distribution is generally of
some help toward that goal.
Let us look at how we might do the computation using MapReduce to exploit
the natural parallelism found in this problem. The input is key-value pairs
(i, Pi), where i is an ID for the picture and Pi
is the picture itself. We want
to compare each pair of pictures, so let us use one key for each set of two ID’s
{i, j}. There are approximately 5 × 1011 pairs of two ID’s. We want each
key {i, j} to be associated with the two values Pi and Pj , so the input to the
corresponding reducer will be ({i, j}, [Pi
, Pj ]). Then, the Reduce function can
simply apply the similarity function s to the two pictures on its value list, that
is, compute s(Pi
, Pj ), and decide whether the similarity of the two pictures is
above threshold. The pair would be output if so.
Alas, this algorithm will fail completely. The reducer size is small, since no
2.6. COMPLEXITY THEORY FOR MAPREDUCE 63
list has more than two values, or a total of 2MB of input. Although we don’t
know exactly how the similarity function s operates, we can reasonably expect
that it will not require more than the available main memory. However, the
replication rate is 999,999, since for each picture we generate that number of keyvalue pairs, one for each of the other pictures in the dataset. The total number
of bytes communicated from Map tasks to Reduce tasks is 1,000,000 (for the
pictures) times 999,999 (for the replication), times 1,000,000 (for the size of each
picture). That’s approximately 1018 bytes, or one exabyte. To communicate
this amount of data over gigabit Ethernet would take 1010 seconds, or about
300 years.11
Fortunately, this algorithm is only the extreme point in a spectrum of possible algorithms. We can characterize these algorithms by grouping pictures into
g groups, each of 106/g pictures.
The Map Function: Take an input element (i, Pi) and generate g − 1 keyvalue pairs. For each, the key is one of the sets {u, v}, where u is the group to
which picture i belongs, and v is one of the other groups. The associated value
is the pair (i, Pi).
The Reduce Function: Consider the key {u, v}. The associated value list
will have the 2 × 106/g elements (j, Pj ), where j belongs to either group u or
group v. The Reduce function takes each (i, Pi) and (j, Pj ) on this list, where
i and j belong to different groups, and applies the similarity function s(Pi
, Pj ).
In addition, we need to compare the pictures that belong to the same group,
but we don’t want to do the same comparison at each of the g − 1 reducers
whose key contains a given group number. There are many ways to handle this
problem, but one way is as follows. Compare the members of group u at the
reducer {u, u + 1}, where the “+1” is taken in the end-around sense. That is,
if u = g (i.e., u is the last group), then u + 1 is group 1. Otherwise, u + 1 is the
group whose number is one greater than u.
We can compute the replication rate and reducer size as a function of the
number of groups g. Each input element is turned into g − 1 key-value pairs.
That is, the replication rate is g − 1, or approximately r = g, since we suppose
that the number of groups is still fairly large. The reducer size is 2×106/g, since
that is the number of values on the list for each reducer. Each value is about a
megabyte, so the number of bytes needed to store the input is 2 × 1012/g.
Example 2.18 : If g is 1000, then the input consumes about 2GB. That’s
enough to hold everything in a typical main memory. Moreover, the total
number of bytes communicated is now 106 × 999 × 106
, or about 1015 bytes.
While that is still a huge amount of data to communicate, it is 1000 times
less than that of the brute-force algorithm discussed first. Moreover, there are
11In a typical cluster, there are many switches connecting subsets of the compute nodes, so
all the data does not need to go across a single gigabit switch. However, the total available
communication is still small enough that it is not feasible to implement this algorithm for the
scale of data we have hypothesized.
64 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
still about half a million reducers. Since we are unlikely to have available that
many compute nodes, we can divide all the reducers into a smaller number of
Reduce tasks and still keep all the compute nodes busy; i.e., we can get as much
parallelism as our computing cluster offers us. ✷
The computation cost for algorithms in this family is independent of the
number of groups g, as long as the input to each reducer fits in main memory.
The reason is that the bulk of the computation is the application of function s
to the pairs of pictures. No matter what value g has, s is applied to each pair
once and only once. Thus, although the work of algorithms in the family may
be divided among reducers in widely different ways, all members of the family
do the same computation.
2.6.3 A Graph Model for MapReduce Problems
In this section, we begin the study of a technique that will enable us to prove
lower bounds on the replication rate, as a function of reducer size, for a number
of problems. Our first step is to introduce a graph model of problems. This
graph describes how the outputs of the problem depend on the inputs. The
key idea to be exploited is that, since reducers operate independently, for each
output there must be some reducer that gets all of the inputs needed to compute
that output. For each problem solvable by a MapReduce algorithm there is:
1. A set of inputs.
2. A set of outputs.
3. A many-many relationship between the inputs and outputs, which describes which inputs are necessary to produce which outputs.
Example 2.19 : Figure 2.9 shows the graph for the similarity-join problem
discussed in Section 2.6.2, if there were four pictures rather than a million. The
inputs are the pictures, and the outputs are the six possible pairs of pictures.
Each output is related to the two inputs that are members of its pair. This
form of problem, where the outputs are all the pairs of inputs, is common, and
we shall refer to it as the all-pairs problem. ✷
Example 2.20 : Matrix multiplication presents a more complex graph. If we
multiply n × n matrices M and N to get matrix P, then there are 2n
2
inputs,
mij and njk, and there are n
2 outputs pik. Each output pik is related to 2n
inputs: mi1, mi2, . . . , min and n1k, n2k, . . . , nnk. Moreover, each input is related
to n outputs. For example, mij is related to pi1, pi2, . . . , pin. Figure 2.10 shows
the input-output relationship for matrix multiplication for the simple case of
2 × 2 matrices, specifically

a b
c d   e f
g h 
=

i j
k l 
✷
2.6. COMPLEXITY THEORY FOR MAPREDUCE 65
P1
P2
P3
P4
P1
P1
P1
P2
P3
P4
P2
P2
P3
P4
P3 P4
{ , }
{ , }
{ , }
{ , }
{ , }
{ , }
Figure 2.9: Input-output relationship for a similarity join
In the problems of Examples 2.19 and 2.20, the inputs and outputs were
clearly all present. However, there are other problems where the inputs and/or
outputs may not all be present in any instance of the problem. An example
of such a problem is the natural join of R(A, B) and S(B, C) discussed in
Section 2.3.7. We assume the attributes A, B, and C each have a finite domain,
so there are only a finite number of possible inputs and outputs. The inputs are
all possible R-tuples, those consisting of a value from the domain of A paired
with a value from the domain of B, and all possible S-tuples – pairs from the
domains of B and C. The outputs are all possible triples, with components from
the domains of A, B, and C in that order. The output (a, b, c) is connected to
two inputs, namely R(a, b) and S(b, c).
But in an instance of the join computation, only some of the possible inputs
will be present, and therefore only some of the possible outputs will be produced.
That fact does not influence the graph for the problem. We still need to know
how every possible output relates to inputs, whether or not that output is
produced in a given instance.
2.6.4 Mapping Schemas
Now that we see how to represent problems addressable by MapReduce as
graphs, we can define the requirements for a MapReduce algorithm to solve
a given problem. Each such algorithm must have a mapping schema, which
expresses how outputs are produced by the various reducers used by the algorithm. That is, a mapping schema for a given problem with a given reducer
size q is an assignment of inputs to one or more reducers, such that:
1. No reducer is assigned more than q inputs.
66 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
a
b
c
d
e
f
g
h
i
j
k
l
Figure 2.10: Input-output relationship for matrix multiplication
2. For every output of the problem, there is at least one reducer that is
assigned all the inputs that are related to that output. We say this reducer
covers the output.
Point (1) is simply the definition of “reducer size.” Point (2) is justified by
the fact that reducers can only see the inputs they are given. If no reducer
sees all the inputs that an output depends upon, then no reducer can correctly
produce that output, and therefore the supposed algorithm will not work. It
can be argued that the existence of a mapping schema for any reducer size is
what distinguishes problems that can be solved by a single MapReduce job from
those that cannot.
Example 2.21 : Let us reconsider the “grouping” strategy we discussed in connection with the all-pairs problem in Section 2.6.2. To generalize the problem,
suppose the input is p pictures, which we place in g equal-sized groups of p/g
inputs each. The number of outputs is
p
2

, or approximately p
2/2 outputs. A
reducer will get the inputs from two groups – that is 2p/g inputs – so the reducer
size we need is q = 2p/g. Each picture is sent to the reducers corresponding to
the pairs consisting of its group and any of the g − 1 other groups. Thus, the
replication rate is g − 1, or approximately g. If we replace g by the replication
rate r in q = 2p/g, we conclude that r = 2p/q. That is, the replication rate
is inversely proportional to the reducer size. That relationship is common; the
smaller the reducer size, the larger the replication rate, and therefore the higher
the communication.
This family of algorithms is described by a family of mapping schemas, one
for each possible q. In the mapping schema for q = 2p/g, there are
g
2

, or
approximately g
2/2 reducers. Each reducer corresponds to a pair of groups,
and an input P is assigned to all the reducers whose pair includes the group
2.6. COMPLEXITY THEORY FOR MAPREDUCE 67
P. Thus, no reducer is assigned more than 2p/g inputs; in fact each reducer
is assigned exactly that number. Moreover, every output is covered by some
reducer. Specifically, if the output is a pair from two different groups u and v,
then this output is covered by the reducer for the pair of groups {u, v}. If the
output corresponds to inputs from only one group u, then the output is covered
by several reducers – those corresponding to the set of groups {u, v} for any
v 6= u. Note that the algorithm we described has only one of these reducers
computing the output, but any of them could compute it. ✷
The fact that an output depends on a certain input means that when that
input is processed at the Map task, there will be at least one key-value pair
generated to be used when computing that output. The value might not be
exactly the input (as was the case in Example 2.21), but it is derived from that
input. What is important is that for every related input and output there is
a key-value pair that must be communicated. Note that there is technically
never a need for more than one key-value pair for a given input and output,
because the input could be transmitted to the reducer as itself, and whatever
transformations on the input were applied by the Map function could instead
be applied by the Reduce function at the reducer for that output.
2.6.5 When Not All Inputs Are Present
Example 2.21 describes a problem where we know every possible input is present, because we can define the input set to be those pictures that actually
exist in the dataset. However, as discussed at the end of Section 2.6.3, there
are problems like computing the join, where the graph of inputs and outputs
describes inputs that might exist, and outputs that are only made when at least
one of the inputs exists in the dataset. In fact, for the join, both inputs related
to an output must exist if we are to make that output.
An algorithm for a problem where outputs can be missing still needs a
mapping schema. The justification is that all inputs, or any subset of them,
might be present, so an algorithm without a mapping schema would not be
able to produce every possible output if all the inputs related to that output
happened to be present, and yet no reducer covered that output.
The only way the absence of some inputs makes a difference is that we
may wish to rethink the desired value of the reducer size q when we select an
algorithm from the family of possible algorithms. Especially, if the value of q
we select is that number such that we can be sure the input will just fit in main
memory, then we may wish to increase q to take into account that some fraction
of the inputs are not really there.
Example 2.22 : Suppose that we know we can execute the Reduce function
in main memory on a key and its associated list of q values. However, we also
know that only 5% of the possible inputs are really present in the data set.
Then a mapping schema for reducer size q will really send about q/20 of the
inputs that exist to each reducer. Put another way, we could use the algorithm
68 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
for reducer size 20q and expect that an average of q inputs will actually appear
on the list for each reducer. We can thus choose 20q as the reducer size, or since
there will be some randomness in the number of inputs actually appearing at
each reducer, we might wish to pick a slightly smaller value of reducer size, such
as 18q. ✷
2.6.6 Lower Bounds on Replication Rate
The family of similarity-join algorithms described in Example 2.21 lets us trade
off communication against the reducer size, and through reducer size to trade
communication against parallelism or against the ability to execute the Reduce
function in main memory. How do we know we are getting the best possible
tradeoff? We can only know we have the minimum possible communication if
we can prove a matching lower bound. Using existence of a mapping schema as
the starting point, we can often prove such a lower bound. Here is an outline
of the technique.
1. Prove an upper bound on how many outputs a reducer with q inputs can
cover. Call this bound g(q). This step can be difficult, but for examples
like the all-pairs problem, it is actually quite simple.
2. Determine the total number of outputs produced by the problem.
3. Suppose that there are k reducers, and the ith reducer has qi < q inputs.
Observe that Pk
i=1 g(qi) must be no less than the number of outputs
computed in step (2).
4. Manipulate the inequality from (3) to get a lower bound on Pk
i=1 qi
.
Often, the trick used at this step is to replace some factors of qi by their
upper bound q, but leave a single factor of qi
in the term for i.
5. Since Pk
i=1 qi
is the total communication from Map tasks to Reduce tasks,
divide the lower bound from (4) on this quantity by the number of inputs.
The result is a lower bound on the replication rate.
Example 2.23 : This sequence of steps may seem mysterious, but let us consider the all-pairs problem as an example that we hope will make things clear.
Recall that in Example 2.21 we gave an upper bound on the replication rate
r of r ≤ 2p/q, where p was the number of inputs and q was the reducer size.
We shall show a lower bound on r that is half that amount, which implies
that, although improvements to the algorithm are possible,12 any reduction in
communication for a given reducer size will be by a factor of 2 at most.
For step (1), observe that if a reducer gets q inputs, it cannot cover more
than
q
2

, or approximately q
2/2 outputs. For step (2), we know there are a
12In fact, an algorithm with r very close to p/q exists, for at least some values of p
2.6. COMPLEXITY THEORY FOR MAPREDUCE 69
total of
p
2

, or approximately p
2/2 outputs that each must be covered. The
inequality constructed at step (3) is thus
X
k
i=1
q
2
i
/2 ≥ p
2
/2
or, multiplying both sides by 2,
X
k
i=1
q
2
i ≥ p
2
(2.1)
Now, we must do the manipulation of step (4). Following the hint, we note
that there are two factors of qi
in each term on the left of Equation (2.1), so
we replace one factor by q and leave the other as qi
. Since q ≥ qi
, we can only
increase the left side by doing so, and thus the inequality continues to hold:
q
X
k
i=1
qi ≥ p
2
or, dividing by q:
X
k
i=1
qi ≥ p
2
/q (2.2)
The final step, which is step (5), is to divide both sides of Equation 2.2 by
p, the number of inputs. As a result, the left side, which is (Pk
i=1 qi)/p is equal
to the replication rate, and the right side becomes p/q. That is, we have proved
the lower bound on r:
r ≥ p/q
As claimed, this shows that the family of algorithms from Example 2.21 all have
a replication rate that is at most twice the lowest possible replication rate. ✷
2.6.7 Case Study: Matrix Multiplication
In this section we shall apply the lower-bound technique to one-pass matrixmultiplication algorithms. We saw one such algorithm in Section 2.3.10, but
that is only an extreme case of a family of possible algorithms. In particular, for
that algorithm, a reducer corresponds to a single element of the output matrix.
Just as we grouped inputs in the all-pairs problem to reduce the communication
at the expense of a larger reducer size, we can group rows and columns of the
two input matrices into bands. Each pair consisting of a band of rows of the
first matrix and a band of columns of the second matrix is used by one reducer
to produce a square of elements of the output matrix. An example is suggested
by Fig. 2.11
70 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
=
Figure 2.11: Dividing matrices into bands to reduce communication
In more detail, suppose we want to compute MN = P, and all three matrices
are n × n. Group the rows of M into g bands of n/g rows each, and group the
columns of N into g bands of n/g columns each. This grouping is as suggested
by Fig. 2.11. Keys correspond to two groups (bands), one from M and one
from N.
The Map Function: For each element of M, the Map function generates g
key-value pairs. The value in each case is the element itself, together with its
row and column number so it can be identified by the Reduce function. The
key is the group to which the element belongs, paired with any of the groups
of the matrix N. Similarly, for each element of N, the Map function generates
g key-value pairs. The key is the group of that element paired with any of the
groups of M, and the value is the element itself plus its row and column.
The Reduce Function: The reducer corresponding to the key (i, j), where i
is a group of M and j is a group of N, gets a value list consisting of all the
elements in the ith band of M and the jth band of N. It thus has all the
values it needs to compute the elements of P whose row is one of those rows
comprising the ith band of M and whose column is one of those comprising the
jth band of N. For instance, Fig. 2.11 suggests the third group of M and the
fourth group of N, combining to compute a square of P at the reducer (3, 4).
Each reducer gets n(n/g) elements from each of the two matrices, so q =
2n
2/g. The replication rate is g, since each element of each matrix is sent to
g reducers. That is, r = g. Combining r = g with q = 2n
2/g we can conclude
that r = 2n
2/q. That is, just as for similarity join, the replication rate varies
inversely with the reducer size.
It turns out that this upper bound on replication rate is also a lower bound.
That is, we cannot do better than the family of algorithms we described above
in a single round of MapReduce. Interestingly, we shall see that we can get a
lower total communication for the same reducer size, if we use two passes of
MapReduce as we discussed in Section 2.3.9. We shall not give the complete
proof of the lower bound, but will suggest the important elements.
2.6. COMPLEXITY THEORY FOR MAPREDUCE 71
For step (1) we need to get an upper bound on how many outputs a reducer
of size q can cover. First, notice that if a reducer gets some of the elements in
a row of M, but not all of them, then the elements of that row are useless; the
reducer cannot produce any output in that row of P. Similarly, if a reducer
receives some but not all of a column of N, these inputs are also useless. Thus,
we may assume that the best mapping schema will send to each reducer some
number of full rows of M and some number of full columns of N. This reducer
is then capable of producing output element pik if and only if it has received
the entire ith row of M and the entire kth column of N. The remainder of the
argument for step (1) is to prove that the largest number of outputs are covered
when the reducer receives the same number of rows as columns. We leave this
part as an exercise.
However, assuming a reducer receives k rows of M and k columns of N,
then q = 2nk, and k
2 outputs are covered. That is, g(q), the maximum number
of outputs covered by a reducer that receives q inputs, is q
2/4n
2
.
For step (2), we know the number of outputs is n
2
. In step (3) we observe
that if there are k reducers, with the ith reducer receiving qi ≤ q inputs, then
X
k
i=1
q
2
i /4n
2 ≥ n
2
or
X
k
i=1
q
2
i ≥ 4n
4
From this inequality, you can derive
r ≥ 2n
2
/q
We leave the algebraic manipulation, which is similar to that in Example 2.23,
as an exercise.
Now, let us consider the generalization of the two-pass matrix-multiplication
algorithm that we described in Section 2.3.9. First, notice that we could have
designed the first pass to use one reducer for each triple (i, j, k). This reducer
would get only the two elements mij and njk. We can generalize this idea to
use reducers that get larger sets of elements from each matrix; these sets of
elements form squares within their respective matrices. The idea is suggested
by Fig. 2.12. We may divide the rows and columns of both input matrices M
and N into g groups of n/g rows or columns each. The intersections of the
groups partition each matrix into g
2
squares of n
2/g2
elements each.
The square of M corresponding to set of rows I and set of columns J combines with the square of N corresponding to set of rows J and set of columns
K. These two squares compute some of the terms that are needed to produce
the square of the output matrix P that has set of rows I and set of columns K.
However, these two squares do not compute the full value of these elements of
P; rather they produce a contribution to the sum. Other pairs of squares, one
72 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
=
Figure 2.12: Partitioning matrices into squares for a two-pass MapReduce algorithm
from M and one from N, contribute to the same square of P. These contributions are suggested in Fig. 2.12. There, we see how all the squares of M with
a fixed value for set of rows I pair with all the squares of N that have a fixed
value for the set of columns K by letting the set J vary.
So in the first pass, we compute the products of the square (I, J) of M with
the square (J, K) of N, for all I, J, and K. Then, in the second pass, for each
I and K we sum the products over all possible sets J. In more detail, the first
MapReduce job does the following.
The Map Function: The keys are triples of sets of rows and/or column numbers (I, J, K). Suppose the element mij belongs to group of rows I and group
of columns J. Then from mij we generate g key-value pairs with value equal to
mij , together with its row and column numbers, i and j, to identify the matrix
element. There is one key-value pair for each key (I, J, K), where K can be any
of the g groups of columns of N. Similarly, from element njk of N, if j belongs
to group J and k to group K, the Map function generates g key-value pairs
with value consisting of njk, j, and k, and with keys (I, J, K) for any group I.
The Reduce Function: The reducer corresponding to (I, J, K) receives as
input all the elements mij where i is in I and j is in J, and it also receives all
the elements njk, where j is in J and k is in K. It computes
xiJk =
X
j in J
mijnjk
2.6. COMPLEXITY THEORY FOR MAPREDUCE 73
for all i in I and k in K.
Notice that the replication rate for the first MapReduce job is g, and the total communication is therefore 2gn2
. Also notice that each reducer gets 2n
2/g2
inputs, so q = 2n
2/g2
. Equivalently, g = n
p
2/q. Thus, the total communication 2gn2
can be written in terms of q as 2√
2n
3/
√q.
The second MapReduce job is simple; it sums up the xiJk’s over all sets J.
The Map Function: We assume that the Map tasks execute at whatever
compute nodes executed the Reduce tasks of the previous job. Thus, no communication is needed between the jobs. The Map function takes as input one
element xiJk, which we assume the previous reducers have left labeled with i
and k so we know to what element of matrix P this term contributes. One
key-value pair is generated. The key is (i, k) and the value is xiJk.
The Reduce Function: The Reduce function simply sums the values associated with key (i, k) to compute the output element Pik.
The communication between the Map and Reduce tasks of the second job is
gn2
, since there are n possible values of i, n possible values of k, and g possible
values of the set J, and each xiJk is communicated only once. If we recall from
our analysis of the first MapReduce job that g = n
p
2/q, we can write the
communication for the second job as n
2
g =
√
2n
3/
√q. This amount is exactly
half the communication for the first job, so the total communication for the
two-pass algorithm is 3√
2n
3/
√
q. Although we shall not examine this point
here, it turns out that we can do slightly better if we divide the matrices M
and N not into squares but into rectangles that are twice as long on one side
as on the other. In that case, we get the slightly smaller constant 4 in place
of 3√
2 = 4.24, and we get a two-pass algorithm with communication equal to
4n
3/
√
q.
Now, recall that the communication cost we computed for the one-pass
algorithm is 4n
4/q. We may as well assume q is less than n
2
, or else we can
just use a serial algorithm at one compute node and not use MapReduce at all.
Thus, n
3/
√q is smaller than n
4/q, and if q is close to its minimum possible
value of 2n,
13 then the two-pass algorithm beats the one-pass algorithm by a
factor of O(
√
n) in communication. Moreover, we can expect the difference
in communication to be the significant cost difference. Both algorithms do
the same O(n
3
) arithmetic operations. The two-pass method naturally has
more overhead managing tasks than does the one-job method. On the other
hand, the second pass of the two-pass algorithm applies a Reduce function
that is associative and commutative. Thus, it might be possible to save some
communication cost by using a combiner on that pass.
2.6.8 Exercises for Section 2.6
Exercise 2.6.1 : Describe the graphs that model the following problems.
13If q is less than 2n, then a reducer cannot get even one row and one column, and therefore
cannot compute any outputs at all.
74 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
(a) The multiplication of an n × n matrix by a vector of length n.
(b) The natural join of R(A, B) and S(B, C), where A, B, and C have domains of sizes a, b, and c, respectively.
(c) The grouping and aggregation on the relation R(A, B), where A is the
grouping attribute and B is aggregated by the MAX operation. Assume
A and B have domains of size a and b, respectively.
! Exercise 2.6.2 : Provide the details of the proof that a one-pass matrixmultiplication algorithm requires replication rate at least r ≥ 2n
2/q, including:
(a) The proof that, for a fixed reducer size, the maximum number of outputs
are covered by a reducer when that reducer receives an equal number of
rows of M and columns of N.
(b) The algebraic manipulation needed, starting with Pk
i=1 q
2
i ≥ 4n
4
.
!! Exercise 2.6.3 : Suppose our inputs are bit strings of length b, and the outputs
correspond to pairs of strings at Hamming distance 1.14
(a) Prove that a reducer of size q can cover at most (q/2) log2
q outputs.
(b) Use part (a) to show the lower bound on replication rate: r ≥ b/ log2
q.
(c) Show that there are algorithms with replication rate as given by part (b)
for the cases q = 2, q = 2b
, and q = 2b/2
.
!! Exercise 2.6.4 : For p that is the square of a prime, show that there is a
mapping schema for the all-pairs problem that has r ≤ 1 + p/q.
2.7 Summary of Chapter 2
✦ Cluster Computing: A common architecture for very large-scale applications is a cluster of compute nodes (processor chip, main memory, and
disk). Compute nodes are mounted in racks, and the nodes on a rack are
connected, typically by gigabit Ethernet. Racks are also connected by a
high-speed network or switch.
✦ Distributed File Systems: An architecture for very large-scale file systems has developed recently. Files are composed of chunks of about 64
megabytes, and each chunk is replicated several times, on different compute nodes or racks.
14Bit strings have Hamming distance 1 if they differ in exactly one bit position. You may
look ahead to Section 3.5.6 for the general definition.
2.7. SUMMARY OF CHAPTER 2 75
✦ MapReduce: This programming system allows one to exploit parallelism
inherent in cluster computing, and manages the hardware failures that
can occur during a long computation on many nodes. Many Map tasks
and many Reduce tasks are managed by a Master process. Tasks on a
failed compute node are rerun by the Master.
✦ The Map Function: This function is written by the user. It takes a
collection of input objects and turns each into zero or more key-value
pairs. Keys are not necessarily unique.
✦ The Reduce Function: A MapReduce programming system sorts all the
key-value pairs produced by all the Map tasks, forms all the values associated with a given key into a list and distributes key-list pairs to Reduce
tasks. Each Reduce task combines the elements on each list, by applying
the function written by the user. The results produced by all the Reduce
tasks form the output of the MapReduce process.
✦ Reducers: It is often convenient to refer to the application of the Reduce
function to a single key and its associated value list as a “reducer.”
✦ Hadoop: This programming system is an open-source implementation of a
distributed file system (HDFS, the Hadoop Distributed File System) and
MapReduce (Hadoop itself). It is available through the Apache Foundation.
✦ Managing Compute-Node Failures: MapReduce systems support restart
of tasks that fail because their compute node, or the rack containing that
node, fail. Because Map and Reduce tasks deliver their output only after
they finish (the blocking property), it is possible to restart a failed task
without concern for possible repetition of the effects of that task. It is
necessary to restart the entire job only if the node at which the Master
executes fails.
✦ Applications of MapReduce: While not all parallel algorithms are suitable
for implementation in the MapReduce framework, there are simple implementations of matrix-vector and matrix-matrix multiplication. Also,
the principal operators of relational algebra are easily implemented in
MapReduce.
✦ Workflow Systems: MapReduce has been generalized to systems that support any acyclic collection of functions, each of which can be instantiated
by any number of tasks, each responsible for executing that function on
a portion of the data.
✦ Spark: This popular workflow system introduces Resilient, Distributed
Datasets (RDD’s) and a language in which many common operations on
RDD’s can be written. Spark has a number of efficiencies, including lazy
evaluation of RDD’s to avoid secondary storage of intermediate results
76 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
and the recording of lineage for RDD’s so they can be reconstructed as
needed.
✦ TensorFlow: This workflow system is specifically designed to support
machine-learning. Data is represented as multidimensional arrays, or tensors, and built-in operations perform many powerful operations, such as
linear algebra and model training.
✦ Recursive Workflows: When implementing a recursive collection of functions, it is not always possible to preserve the ability to restart any failed
task, because recursive tasks may have produced output that was consumed by another task before the failure. A number of schemes for checkpointing parts of the computation to allow restart of single tasks, or restart
all tasks from a recent point, have been proposed.
✦ Communication-Cost: Many applications of MapReduce or similar systems do very simple things for each task. Then, the dominant cost is
usually the cost of transporting data from where it is created to where
it is used. In these cases, efficiency of a MapReduce algorithm can be
estimated by calculating the sum of the sizes of the inputs to all the
tasks.
✦ Multiway Joins: It is sometimes more efficient to replicate tuples of the
relations involved in a join and have the join of three or more relations
computed as a single MapReduce job. The technique of Lagrangean multipliers can be used to optimize the degree of replication for each of the
participating relations.
✦ Star Joins: Analytic queries often involve a very large fact table joined
with smaller dimension tables. These joins can always be done efficiently
by the multiway-join technique. An alternative is to distribute the fact
table and replicate the dimension tables permanently, using the same
strategy as would be used if we were taking the multiway join of the fact
table and every dimension table.
✦ Replication Rate and Reducer Size: It is often convenient to measure
communication by the replication rate, which is the communication per
input. Also, the reducer size is the maximum number of inputs associated
with any reducer. For many problems, it is possible to derive a lower
bound on replication rate as a function of the reducer size.
✦ Representing Problems as Graphs: It is possible to represent many problems that are amenable to MapReduce computation by a graph in which
nodes represent inputs and outputs. An output is connected to all the
inputs that are needed to compute that output.
✦ Mapping Schemas: Given the graph of a problem, and given a reducer size,
a mapping schema is an assignment of the inputs to one or more reducers
2.8. REFERENCES FOR CHAPTER 2 77
so that no reducer is assigned more inputs than the reducer size permits,
and yet for every output there is some reducer that gets all the inputs
needed to compute that output. The requirement that there be a mapping
schema for any MapReduce algorithm is a good expression of what makes
MapReduce algorithms different from general parallel computations.
✦ Matrix Multiplication by MapReduce: There is a family of one-pass MapReduce algorithms that performs multiplication of n × n matrices with
the minimum possible replication rate r = 2n
2/q, where q is the reducer
size. On the other hand, a two-pass MapReduce algorithm for the same
problem with the same reducer size can use up to a factor of n less communication.
2.8 References for Chapter 2
GFS, the Google File System, was described in [13]. The paper on Google’s
MapReduce is [10]. Information about Hadoop and HDFS can be found at [15].
More detail on relations and relational algebra can be found in [25].
Several of the earliest workflow systems were Clustera [11] at the Univ. of
Wisconsin, and Hyracks (previously called Hyrax) [6], from UC Irvine, and
Microsoft’s Dryad [17] and later DryadLINQ [26].
Flink is an open-source workflow system designed to handle streaming data
[12]. It was originally developed in the Stratosphere project [5] at TU Berlin.
Many of the innovations in Spark are described in [27]. The open-source implementation of Spark is at [21]. The information page about TensorFlow is
[24].
The iterated-MapReduce approach to implementing recursion is from Haloop [7]. For a discussion of cluster implementation of recursion, see [1].
Pregel is from [19]. There is an open-source version of Pregel called Giraph
[14]. GraphLab [18] is another notable parallel implementation system for graph
algorithms. GraphX [22] is a graph-based front-end for Spark.
There are a number of other systems built on a distributed file system and/or
MapReduce, which have not been covered here, but may be worth knowing
about. [8] describes BigTable, a Google implementation of an object store of
very large size. A somewhat different direction was taken at Yahoo! with Pnuts
[9]. The latter supports a limited form of transaction processing, for example.
PIG [20] is an implementation of relational algebra on top of Hadoop. Similarly, Hive [16] implements a restricted form of SQL on top of Hadoop. Spark
also provides a SQL-like front end [23].
The communication-cost model for MapReduce algorithms and the optimal
implementations of multiway joins is from [3]. The material on replication rate,
reducer size, and their relationship is from [2]. Solutions to Exercises 2.6.2 and
2.6.3 can be found there. The solution to Exercise 2.6.4 is in [4].
78 CHAPTER 2. MAPREDUCE AND THE NEW SOFTWARE STACK
1. F.N. Afrati, V. Borkar, M. Carey, A. Polyzotis, and J.D. Ullman, “Cluster computing, recursion, and Datalog,” to appear in Proc. Datalog 2.0
Workshop, Elsevier, 2011.
2. F.N. Afrati, A. Das Sarma, S. Salihoglu, and J.D. Ullman, “Upper and
lower bounds on the cost of a MapReduce computation.” to appear in
Proc. Intl. Conf. on Very Large Databases, 2013. Also available as CoRR,
abs/1206.4377.
3. F.N. Afrati and J.D. Ullman, “Optimizing joins in a MapReduce environment,” Proc. Thirteenth Intl. Conf. on Extending Database Technology,
2010.
4. F.N. Afrati and J.D. Ullman, “Matching bounds for the all-pairs MapReduce problem,” IDEAS 2013, pp. 3–4.
5. A. Alexandrov, R. Bergmann, S. Ewen, J.-C. Freytag, F. Hueske, A.
Heise O. Kao, M. Leich, U. Leser, V. Markl, F. Naumann, M. Peters,
A. Rheinlander, M.J. Sax, S. Schelter, M. Hoger, K. Tzoumas, and D.
Warneke, “The Stratosphere platform for big data analytics,” VLDB J.
23:6, pp. 939–964, 2014.
6. V.R. Borkar, M.J. Carey, R, Grover, N. Onose, and R. Vernica, “Hyracks:
A flexible and extensible foundation for data-intensive computing,” Intl.
Conf. on Data Engineering, pp. 1151–1162, 2011.
7. Y. Bu, B. Howe, M. Balazinska, and M. Ernst, “HaLoop: efficient iterative data processing on large clusters,” Proc. Intl. Conf. on Very Large
Databases, 2010.
8. F. Chang, J. Dean, S. Ghemawat, W.C. Hsieh, D.A. Wallach, M. Burrows,
T. Chandra, A. Fikes, and R.E. Gruber, “Bigtable: a distributed storage
system for structured data,” ACM Transactions on Computer Systems
26:2, pp. 1–26, 2008.
9. B.F. Cooper, R. Ramakrishnan, U. Srivastava, A. Silberstein, P. Bohannon, H.-A. Jacobsen, N. Puz, D. Weaver, and R. Yerneni, “Pnuts: Yahoo!’s hosted data serving platform,” PVLDB 1:2, pp. 1277–1288, 2008.
10. J. Dean and S. Ghemawat, “Mapreduce: simplified data processing on
large clusters,” Comm. ACM 51:1, pp. 107–113, 2008.
11. D.J. DeWitt, E. Paulson, E. Robinson, J.F. Naughton, J. Royalty, S.
Shankar, and A. Krioukov, “Clustera: an integrated computation and
data management system,” PVLDB 1:1, pp. 28–41, 2008.
12. flink.apache.org, Apache Foundation.
13. S. Ghemawat, H. Gobioff, and S.-T. Leung, “The Google file system,”
19th ACM Symposium on Operating Systems Principles, 2003.
2.8. REFERENCES FOR CHAPTER 2 79
14. giraph.apache.org, Apache Foundation.
15. hadoop.apache.org, Apache Foundation.
16. hadoop.apache.org/hive, Apache Foundation.
17. M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. “Dryad: distributed data-parallel programs from sequential building blocks,” Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference on Computer Systems, pp. 59–72, ACM, 2007.
18. Y. Low, D. Bickson, J. Gonzalez, C. Guestrin, A. Kyrola, and J.M. Hellerstein, “Distributed GraphLab: a framework for machine learning and data
mining in the cloud,” —em Proc. VLDB Endowment 5:8, pp. 716–727,
2012.
19. G. Malewicz, M.N. Austern, A.J.C. Sik, J.C. Denhert, H. Horn, N. Leiser,
and G. Czajkowski, “Pregel: a system for large-scale graph processing,”
Proc. ACM SIGMOD Conference, 2010.
20. C. Olston, B. Reed, U. Srivastava, R. Kumar, and A. Tomkins, “Pig latin:
a not-so-foreign language for data processing,” Proc. ACM SIGMOD Conference, pp. 1099–1110, 2008.
21. spark.apache.org, Apache Foundation.
22. spark.apache.org/graphx, Apache Foundation.
23. spark.apache.org/sql, Apache Foundation.
24. www.tensorflow.org.
25. J.D. Ullman and J. Widom, A First Course in Database Systems, Third
Edition, Prentice-Hall, Upper Saddle River, NJ, 2008.
26. Y. Yu, M. Isard, D. Fetterly, M. Budiu, I. Erlingsson, P.K. Gunda, and
J. Currey, “DryadLINQ: a system for general-purpose distributed dataparallel computing using a high-level language,” OSDI, pp. 1–14, USENIX
Association, 2008.
27. M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauley, M.J.
Franklin, S. Shenker, and I. Stoica, “Resilient distributed datasets: A
fault-tolerant abstraction for in-memory cluster computing,” Proc. 9th
USENIX conference on Networked Systems Design and Implementation,
USENIX Association, 2012.

72
Chapter 3
Finding Similar Items
A fundamental data-mining problem is to examine data for “similar” items. We
shall take up applications in Section 3.1, but an example would be looking at a
collection of Web pages and finding near-duplicate pages. These pages could be
plagiarisms, for example, or they could be mirrors that have almost the same
content but differ in information about the host and about other mirrors.
The naive approach to finding pairs of similar items requires us to look at every pair of items. When we are dealing with a large dataset, looking at all pairs
of items may be prohibitive, even given an abundance of hardware resources.
For example, even a million items gives us half a trillion pairs to examine, and
a million items is considered a “small” dataset by today’s standards.
It is therefore a pleasant surprise to learn of a family of techniques called
locality-sensitive hashing, or LSH, that allows us to focus on pairs that are likely
to be similar, without having to look at all pairs. Thus, it is possible that we
can avoid the quadratic growth in computation time that is required by the
naive algorithm. There is usually a downside to locality-sensitive hashing, due
to the presence of false negatives, that is, pairs of items that are similar, yet
are not included in the set of pairs that we examine, but by careful tuning we
can reduce the fraction of false negatives by increasing the number of pairs we
consider.
The general idea behind LSH is that we hash items using many different
hash functions. These hash functions are not the conventional sort of hash
functions. Rather, they are carefully designed to have the property that pairs
are much more likely to wind up in the same bucket of a hash function if the
items are similar than if they are not similar. We then can examine only the
candidate pairs, which are pairs of items that wind up in the same bucket for
at least one of the hash functions.
We begin our discussion of LSH with an examination of the problem of finding similar documents – those that share a lot of common text. We first show
how to convert documents into sets (Section 3.2) in a way that lets us view
textual similarity of documents as sets having a large overlap. More precisely,
73
74 CHAPTER 3. FINDING SIMILAR ITEMS
we measure the similarity of sets by their Jaccard similarity, the ratio of the
sizes of their intersection and union. A second key trick we need is minhashing
(Section 3.3), which is a way to convert large sets into much smaller representations, called signatures, that still enable us to estimate closely the Jaccard
similarity of the represented sets. Finally, in Section 3.4 we see how to apply
the bucketing idea inherent in LSH to the signatures.
In Section 3.5 we begin our study of how to apply LSH to items other than
sets. We consider the general notion of a distance measure that tells to what
degree items are similar. Then, in Section 3.6 we consider the general idea of
locality-sensitive hashing, and in Section 3.7 we see how to do LSH for some data
types other than sets. Then, Section 3.8 examines in detail several applications
of the LSH idea. Finally, we consider in Section 3.9 some techniques for finding
similar sets that can be more efficient than LSH when the degree of similarity
we want is very high.
3.1 Applications of Set Similarity
We shall focus initially on a particular notion of “similarity”: the similarity of
sets by looking at the relative size of their intersection. This notion of similarity
is called Jaccard similarity, which is introduced in Section 3.1.1. We then
examine some of the uses of finding similar sets. These include finding textually
similar documents and collaborative filtering by finding similar customers and
similar products. In order to turn the problem of textual similarity of documents
into one of set intersection, we use the technique called shingling, which is the
subject of Section 3.2.
3.1.1 Jaccard Similarity of Sets
The Jaccard similarity of sets S and T is |S ∩ T |/|S ∪ T |, that is, the ratio
of the size of the intersection of S and T to the size of their union. We shall
denote the Jaccard similarity of S and T by SIM(S, T ).
Example 3.1 : In Fig. 3.1 we see two sets S and T . There are three elements
in their intersection and a total of eight elements that appear in S or T or both.
Thus, SIM(S, T ) = 3/8. ✷
3.1.2 Similarity of Documents
An important class of problems that Jaccard similarity addresses well is that
of finding textually similar documents in a large corpus such as the Web or a
collection of news articles. We should understand that the aspect of similarity
we are looking at here is character-level similarity, not “similar meaning,” which
requires us to examine the words in the documents and their uses. That problem
is also interesting but is addressed by other techniques, which we hinted at in
3.1. APPLICATIONS OF SET SIMILARITY 75
































T
S
Figure 3.1: Two sets with Jaccard similarity 3/8
Section 1.3.1. However, textual similarity also has important uses. Many of
these involve finding duplicates or near duplicates. First, let us observe that
testing whether two documents are exact duplicates is easy; just compare the
two documents character-by-character, and if they ever differ then they are not
the same. However, in many applications, the documents are not identical, yet
they share large portions of their text. Here are some examples:
Plagiarism
Finding plagiarized documents tests our ability to find textual similarity. The
plagiarizer may extract only some parts of a document for his own. He may
alter a few words and may alter the order in which sentences of the original
appear. Yet the resulting document may still contain much of the original. No
simple process of comparing documents character by character will detect a
sophisticated plagiarism.
Mirror Pages
It is common for important or popular Web sites to be duplicated at a number
of hosts, in order to share the load. The pages of these mirror sites will be
quite similar, but are rarely identical. For instance, they might each contain
information associated with their particular host, and they might each have
links to the other mirror sites but not to themselves. A related phenomenon is
the reuse of Web pages from one academic class to another. These pages might
include class notes, assignments, and lecture slides. Similar pages might change
the name of the course, year, and make small changes from year to year. It
is important to be able to detect similar pages of these kinds, because search
engines produce better results if they avoid showing two pages that are nearly
identical within the first
76 CHAPTER 3. FINDING SIMILAR ITEMS
Articles from the Same Source
It is common for one reporter to write a news article that gets distributed,
say through the Associated Press, to many newspapers, which then publish
the article on their Web sites. Each newspaper changes the article somewhat.
They may cut out paragraphs, or even add material of their own. They most
likely will surround the article by their own logo, ads, and links to other articles
at their site. However, the core of each newspaper’s page will be the original
article. News aggregators, such as Google News, try to find all versions of such
an article, in order to show only one, and that task requires finding when two
Web pages are textually similar, although not identical.1
3.1.3 Collaborative Filtering as a Similar-Sets Problem
Another class of applications where similarity of sets is very important is called
collaborative filtering, a process whereby we recommend to users items that were
liked by other users who have exhibited similar tastes. We shall investigate
collaborative filtering in detail in Section 9.3, but for the moment let us see
some common examples.
On-Line Purchases
Amazon.com has millions of customers and sells millions of items. Its database
records which items have been bought by which customers. We can say two customers are similar if their sets of purchased items have a high Jaccard similarity.
Likewise, two items that have sets of purchasers with high Jaccard similarity
will be deemed similar. Note that, while we might expect mirror sites to have
Jaccard similarity above 90%, it is unlikely that any two customers have Jaccard similarity that high (unless they have purchased only one item). Even a
Jaccard similarity like 20% might be unusual enough to identify customers with
similar tastes. The same observation holds for items; Jaccard similarities need
not be very high to be significant.
Collaborative filtering requires several tools, in addition to finding similar
customers or items, as we discuss in Chapter 9. For example, two Amazon
customers who like science-fiction might each buy many science-fiction books,
but only a few of these will be in common. However, by combining similarityfinding with clustering (Chapter 7), we might be able to discover that sciencefiction books are mutually similar and put them in one group. Then, we can
get a more powerful notion of customer-similarity by asking whether they made
purchases within many of the same groups.
1News aggregation also involves finding articles that are about the same topic, even though
not textually similar. This problem too can yield to a similarity search, but it requires
techniques other than Jaccard similarity of sets.
3.1. APPLICATIONS OF SET SIMILARITY 77
Movie Ratings
Netflix records which movies each of its customers rented, and also the ratings
assigned to those movies by the customers. We can regard movies as similar
if they were rented or rated highly by many of the same customers, and see
customers as similar if they rented or rated highly many of the same movies.
The same observations that we made for Amazon above apply in this situation:
similarities need not be high to be significant, and clustering movies by genre
will make things easier.
When our data consists of ratings rather than binary decisions (bought/did
not buy or liked/disliked), we cannot rely simply on sets as representations of
customers or items. Some options are:
1. Ignore low-rated customer/movie pairs; that is, treat these events as if
the customer never watched the movie.
2. When comparing customers, imagine two set elements for each movie,
“liked” and “hated.” If a customer rated a movie highly, put “liked” for
that movie in the customer’s set. If they gave a low rating to a movie, put
“hated” for that movie in their set. Then, we can look for high Jaccard
similarity among these sets. We can use a similar trick when comparing
movies.
3. If ratings are 1-to-5-stars, put a movie in a customer’s set n times if
they rated the movie n-stars. Then, use Jaccard similarity for bags when
measuring the similarity of customers. The Jaccard similarity for bags
B and C is defined by counting an element n times in the intersection if
n is the minimum of the number of times the element appears in B and
C. In the union, we count the element the sum of the number of times it
appears in B and in C.
2
Example 3.2 : The bag-similarity of bags {a, a, a, b} and {a, a, b, b, c} is 1/3.
The intersection counts a twice and b once, so its size is 3. The size of the
union of two bags is always the sum of the sizes of the two bags, or 9 in this
case. Since the highest possible Jaccard similarity for bags is 1/2, the score
of 1/3 indicates the two bags are quite similar, as should be apparent from an
examination of their contents. ✷
2Although the union for bags is normally (e.g., in the SQL standard) defined to have the
sum of the number of copies in each of the two bags, this definition causes some inconsistency
with the Jaccard similarity for sets. Under this definition of bag union, the maximum Jaccard
similarity is 1/2, not 1, since the union of a set with itself has twice as many elements as the
intersection of the same set with itself. If we prefer to have the Jaccard similarity of a set
with itself be 1, we can redefine the union of bags to have each element appear the maximum
number of times it appears in either of the two bags. This change also gives a reasonable
measure of bag similarity.
78 CHAPTER 3. FINDING SIMILAR ITEMS
3.1.4 Exercises for Section 3.1
Exercise 3.1.1 : Compute the Jaccard similarities of each pair of the following
three sets: {1, 2, 3, 4}, {2, 3, 5, 7}, and {2, 4, 6}.
Exercise 3.1.2 : Compute the Jaccard bag similarity of each pair of the following three bags: {1, 1, 1, 2}, {1, 1, 2, 2, 3}, and {1, 2, 3, 4}.
!! Exercise 3.1.3 : Suppose we have a universal set U of n elements, and we
choose two subsets S and T at random, each with m of the n elements. What
is the expected value of the Jaccard similarity of S and T ?
3.2 Shingling of Documents
The most effective way to represent documents as sets, for the purpose of identifying lexically similar documents is to construct from the document the set
of short strings that appear within it. If we do so, then documents that share
pieces as short as sentences or even phrases will have many common elements
in their sets, even if those sentences appear in different orders in the two documents. In this section, we introduce the simplest and most common approach,
shingling, as well as an interesting variation.
3.2.1 k-Shingles
A document is a string of characters. Define a k-shingle for a document to be
any substring of length k found within the document. Then, we may associate
with each document the set of k-shingles that appear one or more times within
that document.
Example 3.3 : Suppose our document D is the string abcdabd, and we pick
k = 2. Then the set of 2-shingles for D is {ab, bc, cd, da, bd}.
Note that the substring ab appears twice within D, but appears only once
as a shingle. A variation of shingling produces a bag, rather than a set, so each
shingle would appear in the result as many times as it appears in the document.
However, we shall not use bags of shingles here. ✷
There are several options regarding how white space (blank, tab, newline,
etc.) is treated. It probably makes sense to replace any sequence of one or more
white-space characters by a single blank. That way, we distinguish shingles that
cover two or more words from those that do not.
Example 3.4 : If we use k = 9, but eliminate whitespace altogether, then we
would see some lexical similarity in the sentences “The plane was ready for
touch down”. and “The quarterback scored a touchdown”. However, if we
retain the blanks, then the first has shingles touch dow and ouch down, while
the second has touchdown. If we eliminated the blanks, then both would have
touchdown. ✷
3.2. SHINGLING OF DOCUMENTS 79
3.2.2 Choosing the Shingle Size
We can pick k to be any constant we like. However, if we pick k too small, then
we would expect most sequences of k characters to appear in most documents.
If so, then we could have documents whose shingle-sets had high Jaccard similarity, yet the documents had none of the same sentences or even phrases. As
an extreme example, if we use k = 1, most Web pages will have most of the
common characters and few other characters, so almost all Web pages will have
high similarity.
How large k should be depends on how long typical documents are and how
large the set of typical characters is. The important thing to remember is:
• k should be picked large enough that the probability of any given shingle
appearing in any given document is low.
Thus, if our corpus of documents is emails, picking k = 5 should be fine.
To see why, suppose that only letters and a general white-space character appear in emails (although in practice, most of the printable ASCII characters
can be expected to appear occasionally). If so, then there would be 275 =
14,348,907 possible shingles. Since the typical email is much smaller than 14
million characters long, we would expect k = 5 to work well, and indeed it does.
However, the calculation is a bit more subtle. Surely, more than 27 characters appear in emails, However, all characters do not appear with equal probability. Common letters and blanks dominate, while ”z” and other letters that
have high point-value in Scrabble are rare. Thus, even short emails will have
many 5-shingles consisting of common letters, and the chances of unrelated
emails sharing these common shingles is greater than would be implied by the
calculation in the paragraph above. A good rule of thumb is to imagine that
there are only 20 characters and estimate the number of k-shingles as 20k
. For
large documents, such as research articles, choice k = 9 is considered safe.
3.2.3 Hashing Shingles
Instead of using substrings directly as shingles, we can pick a hash function
that maps strings of length k to some number of buckets and treat the resulting
bucket number as the shingle. The set representing a document is then the
set of integers that are bucket numbers of one or more k-shingles that appear
in the document. For instance, we could construct the set of 9-shingles for a
document and then map each of those 9-shingles to a bucket number in the
range 0 to 232 − 1. Thus, each shingle is represented by four bytes instead
of nine. Not only has the data been compacted, but we can now manipulate
(hashed) shingles by single-word machine operations.
Notice that we can differentiate documents better if we use 9-shingles and
hash them down to four bytes than to use 4-shingles, even though the space used
to represent a shingle is the same. The reason was touched upon in Section 3.2.2.
If we use 4-shingles, most sequences of four bytes are unlikely or impossible to
80 CHAPTER 3. FINDING SIMILAR ITEMS
find in typical documents. Thus, the effective number of different shingles is
much less than 232 −1. If, as in Section 3.2.2, we assume only 20 characters are
frequent in English text, then the number of different 4-shingles that are likely
to occur is only (20)4 = 160,000. However, if we use 9-shingles, there are many
more than 232 likely shingles. When we hash them down to four bytes, we can
expect almost any sequence of four bytes to be possible, as was discussed in
Section 1.3.2.
3.2.4 Shingles Built from Words
An alternative form of shingle has proved effective for the problem of identifying
similar news articles, mentioned in Section 3.1.2. The exploitable distinction for
this problem is that the news articles are written in a rather different style than
are other elements that typically appear on the page with the article. News
articles, and most prose, have a lot of stop words (see Section 1.3.1), the most
common words such as “and,” “you,” “to,” and so on. In many applications,
we want to ignore stop words, since they don’t tell us anything useful about
the article, such as its topic.
However, for the problem of finding similar news articles, it was found that
defining a shingle to be a stop word followed by the next two words, regardless
of whether or not they were stop words, formed a useful set of shingles. The
advantage of this approach is that the news article would then contribute more
shingles to the set representing the Web page than would the surrounding elements. Recall that the goal of the exercise is to find pages that had the same
articles, regardless of the surrounding elements. By biasing the set of shingles
in favor of the article, pages with the same article and different surrounding
material have higher Jaccard similarity than pages with the same surrounding
material but with a different article.
Example 3.5 : An ad might have the simple text “Buy Sudzo.” However, a
news article with the same idea might read something like “A spokesperson
for the Sudzo Corporation revealed today that studies have shown it is
good for people to buy Sudzo products.” Here, we have italicized all the
likely stop words, although there is no set number of the most frequent words
that should be considered stop words. The first three shingles made from a
stop word and the next two following are:
A spokesperson for
for the Sudzo
the Sudzo Corporation
There are nine shingles from the sentence, but none from the “ad.” ✷
3.2.5 Exercises for Section 3.2
Exercise 3.2.1 : What are the first ten 3-shingles in the first sentence of Section 3.2?
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 81
Exercise 3.2.2 : If we use the stop-word-based shingles of Section 3.2.4, and
we take the stop words to be all the words of three or fewer letters, then what
are the shingles in the first sentence of Section 3.2?
Exercise 3.2.3 : What is the largest number of k-shingles a document of n
bytes can have? You may assume that the size of the alphabet is large enough
that the number of possible strings of length k is at least n.
3.3 Similarity-Preserving Summaries of Sets
Sets of shingles are large. Even if we hash them to four bytes each, the space
needed to store a set is still roughly four times the space taken by the document.
If we have millions of documents, it may well not be possible to store all the
shingle-sets in main memory.3
Our goal in this section is to replace large sets by much smaller representations called “signatures.” The important property we need for signatures is
that we can compare the signatures of two sets and estimate the Jaccard similarity of the underlying sets from the signatures alone. It is not possible that
the signatures give the exact similarity of the sets they represent, but the estimates they provide are close, and the larger the signatures the more accurate
the estimates. For example, if we replace the 200,000-byte hashed-shingle sets
that derive from 50,000-byte documents by signatures of 1000 bytes, we can
usually get within a few percent.
3.3.1 Matrix Representation of Sets
Before explaining how it is possible to construct small signatures from large
sets, it is helpful to visualize a collection of sets as their characteristic matrix.
The columns of the matrix correspond to the sets, and the rows correspond to
elements of the universal set from which elements of the sets are drawn. There
is a 1 in row r and column c if the element for row r is a member of the set for
column c. Otherwise the value in position (r, c) is 0.
Element S1 S2 S3 S4
a 1 0 0 1
b 0 0 1 0
c 0 1 0 1
d 1 0 1 1
e 0 0 1 0
Figure 3.2: A matrix representing four sets
3There is another serious concern: even if the sets fit in main memory, the number of pairs
may be too great for us to evaluate the similarity of each pair. We take up the solution to
this problem in Section 3.4.
82 CHAPTER 3. FINDING SIMILAR ITEMS
Example 3.6 : In Fig. 3.2 is an example of a matrix representing sets chosen
from the universal set {a, b, c, d, e}. Here, S1 = {a, d}, S2 = {c}, S3 = {b, d, e},
and S4 = {a, c, d}. The top row and leftmost columns are not part of the matrix,
but are present only to remind us what the rows and columns represent. ✷
It is important to remember that the characteristic matrix is unlikely to be
the way the data is stored, but it is useful as a way to visualize the data. For one
reason not to store data as a matrix, these matrices are almost always sparse
(they have many more 0’s than 1’s) in practice. It saves space to represent a
sparse matrix of 0’s and 1’s by the positions in which the 1’s appear. For another
reason, the data is usually stored in some other format for other purposes.
As an example, if rows are products, and columns are customers, represented
by the set of products they bought, then this data would really appear in a
database table of purchases. A tuple in this table would list the item, the
purchaser, and probably other details about the purchase, such as the date and
the credit card used.
3.3.2 Minhashing
The signatures we desire to construct for sets are composed of the results of a
large number of calculations, say several hundred, each of which is a “minhash”
of the characteristic matrix. In this section, we shall learn how a minhash is
computed in principle, and in later sections we shall see how a good approximation to the minhash is computed in practice.
To minhash a set represented by a column of the characteristic matrix, pick
a permutation of the rows. The minhash value of any column is the number of
the first row, in the permuted order, in which the column has a 1.
Example 3.7 : Let us suppose we pick the order of rows beadc for the matrix
of Fig. 3.2. This permutation defines a minhash function h that maps sets to
rows. Let us compute the minhash value of set S1 according to h. The first
column, which is the column for set S1, has 0 in row b, so we proceed to row e,
the second in the permuted order. There is again a 0 in the column for S1, so
we proceed to row a, where we find a 1. Thus. h(S1) = a.
Element S1 S2 S3 S4
b 0 0 1 0
e 0 0 1 0
a 1 0 0 1
d 1 0 1 1
c 0 1 0 1
Figure 3.3: A permutation of the rows of Fig. 3.2
Although it is not physically possible to permute very large characteristic
matrices, the minhash function h implicitly reorders the rows of the matrix of
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 83
Fig. 3.2 so it becomes the matrix of Fig. 3.3. In this matrix, we can read off
the values of h by scanning from the top until we come to a 1. Thus, we see
that h(S2) = c, h(S3) = b, and h(S4) = a. ✷
3.3.3 Minhashing and Jaccard Similarity
There is a remarkable connection between minhashing and Jaccard similarity
of the sets that are minhashed.
• The probability that the minhash function for a random permutation of
rows produces the same value for two sets equals the Jaccard similarity
of those sets.
To see why, we need to picture the columns for those two sets. If we restrict
ourselves to the columns for sets S1 and S2, then rows can be divided into three
classes:
1. Type X rows have 1 in both columns.
2. Type Y rows have 1 in one of the columns and 0 in the other.
3. Type Z rows have 0 in both columns.
Since the matrix is sparse, most rows are of type Z. However, it is the ratio
of the numbers of type X and type Y rows that determine both SIM(S1, S2)
and the probability that h(S1) = h(S2). Let there be x rows of type X and y
rows of type Y . Then SIM(S1, S2) = x/(x + y). The reason is that x is the size
of S1 ∩ S2 and x + y is the size of S1 ∪ S2.
Now, consider the probability that h(S1) = h(S2). If we imagine the rows
permuted randomly, and we proceed from the top, the probability that we shall
meet a type X row before we meet a type Y row is x/(x + y). But if the
first row from the top other than type Z rows is a type X row, then surely
h(S1) = h(S2). On the other hand, if the first row other than a type Z row
that we meet is a type Y row, then the set with a 1 gets that row as its minhash
value. However the set with a 0 in that row surely gets some row further down
the permuted list. Thus, we know h(S1) 6= h(S2) if we first meet a type Y row.
We conclude the probability that h(S1) = h(S2) is x/(x + y), which is also the
Jaccard similarity of S1 and S2.
3.3.4 Minhash Signatures
Again think of a collection of sets represented by their characteristic matrix M.
To represent sets, we pick at random some number n of permutations of the
rows of M. Perhaps 100 permutations or several hundred permutations will do.
Call the minhash functions determined by these permutations h1, h2, . . . , hn.
From the column representing set S, construct the minhash signature for S, the
vector [h1(S), h2(S), . . . , hn(S)]. We normally represent this list of hash-values
84 CHAPTER 3. FINDING SIMILAR ITEMS
as a column. Thus, we can form from matrix M a signature matrix, in which
the ith column of M is replaced by the minhash signature for (the set of) the
ith column.
Note that the signature matrix has the same number of columns as M but
only n rows. Even if M is not represented explicitly, but in some compressed
form suitable for a sparse matrix (e.g., by the locations of its 1’s), it is normal
for the signature matrix to be much smaller than M.
The remarkable thing about signature matrices is that we can use their
columns to estimate the Jaccard similarity of the sets that correspond to the
columns of signature matrix. By the theorem proved in Section 3.3.3, we know
that the probability that two columns have the same value in a given row of
the signature matrix equals the Jaccard similarity of the sets corresponding to
those columns. Moreover, since the permutations on which the minhash values
are based were chosen independently, we can think of each row of the signature
matrix as an independent experiment. Thus, the expected number of rows in
which two columns agree equals the Jaccard similarity of their corresponding
sets. Moreover, the more minhashings we use, i.e., the more rows in the signature matrix, the smaller the expected error in the estimate of the Jaccard
similarity will be.
3.3.5 Computing Minhash Signatures in Practice
It is not feasible to permute a large characteristic matrix explicitly. Even picking
a random permutation of millions or billions of rows is time-consuming, and
the necessary sorting of the rows would take even more time. Thus, permuted
matrices like that suggested by Fig. 3.3, while conceptually appealing, are not
implementable.
Fortunately, it is possible to simulate the effect of a random permutation by
a random hash function that maps row numbers to as many buckets as there
are rows. A hash function that maps integers 0, 1, . . . , k − 1 to bucket numbers
0 through k−1 typically will map some pairs of integers to the same bucket and
leave other buckets unfilled. However, the difference is unimportant as long as
k is large and there are not too many collisions. We can maintain the fiction
that our hash function h “permutes” row r to position h(r) in the permuted
order.
Thus, instead of picking n random permutations of rows, we pick n randomly
chosen hash functions h1, h2, . . . , hn on the rows. We construct the signature
matrix by considering each row in their given order. Let SIG(i, c) be the element
of the signature matrix for the ith hash function and column c. Initially, set
SIG(i, c) to ∞ for all i and c. We handle row r by doing the following:
1. Compute h1(r), h2(r), . . . , hn(r).
2. For each column c do the following:
(a) If c has 0 in row r, do nothing.
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 85
(b) However, if c has 1 in row r, then for each i = 1, 2, . . . , n set SIG(i, c)
to the smaller of the current value of SIG(i, c) and hi(r).
Row S1 S2 S3 S4 x + 1 mod 5 3x + 1 mod 5
0 1 0 0 1 1 1
1 0 0 1 0 2 4
2 0 1 0 1 3 2
3 1 0 1 1 4 0
4 0 0 1 0 0 3
Figure 3.4: Hash functions computed for the matrix of Fig. 3.2
Example 3.8 : Let us reconsider the characteristic matrix of Fig. 3.2, which
we reproduce with some additional data as Fig. 3.4. We have replaced the
letters naming the rows by integers 0 through 4. We have also chosen two hash
functions: h1(x) = x+1 mod 5 and h2(x) = 3x+1 mod 5. The values of these
two functions applied to the row numbers are given in the last two columns of
Fig. 3.4. Notice that these simple hash functions are true permutations of the
rows, but a true permutation is only possible because the number of rows, 5, is
a prime. In general, there will be collisions, where two rows get the same hash
value.
Now, let us simulate the algorithm for computing the signature matrix.
Initially, this matrix consists of all ∞’s:
S1 S2 S3 S4
h1 ∞ ∞ ∞ ∞
h2 ∞ ∞ ∞ ∞
First, we consider row 0 of Fig. 3.4. We see that the values of h1(0) and
h2(0) are both 1. The row numbered 0 has 1’s in the columns for sets S1 and
S4, so only these columns of the signature matrix can change. As 1 is less than
∞, we do in fact change both values in the columns for S1 and S4. The current
estimate of the signature matrix is thus:
S1 S2 S3 S4
h1 1 ∞ ∞ 1
h2 1 ∞ ∞ 1
Now, we move to the row numbered 1 in Fig. 3.4. This row has 1 only in
S3, and its hash values are h1(1) = 2 and h2(1) = 4. Thus, we set SIG(1, 3) to 2
and SIG(2, 3) to 4. All other signature entries remain as they are because their
columns have 0 in the row numbered 1. The new signature matrix:
S1 S2 S3 S4
h1 1 ∞ 2 1
h2 1 ∞ 4 1
86 CHAPTER 3. FINDING SIMILAR ITEMS
The row of Fig. 3.4 numbered 2 has 1’s in the columns for S2 and S4, and
its hash values are h1(2) = 3 and h2(2) = 2. We could change the values in the
signature for S4, but the values in this column of the signature matrix, [1, 1], are
each less than the corresponding hash values [3, 2]. However, since the column
for S2 still has ∞’s, we replace it by [3, 2], resulting in:
S1 S2 S3 S4
h1 1 3 2 1
h2 1 2 4 1
Next comes the row numbered 3 in Fig. 3.4. Here, all columns but S2 have
1, and the hash values are h1(3) = 4 and h2(3) = 0. The value 4 for h1 exceeds
what is already in the signature matrix for all the columns, so we shall not
change any values in the first row of the signature matrix. However, the value
0 for h2 is less than what is already present, so we lower SIG(2, 1), SIG(2, 3) and
SIG(2, 4) to 0. Note that we cannot lower SIG(2, 2) because the column for S2 in
Fig. 3.4 has 0 in the row we are currently considering. The resulting signature
matrix:
S1 S2 S3 S4
h1 1 3 2 1
h2 0 2 0 0
Finally, consider the row of Fig. 3.4 numbered 4. h1(4) = 0 and h2(4) = 3.
Since row 4 has 1 only in the column for S3, we only compare the current
signature column for that set, [2, 0] with the hash values [0, 3]. Since 0 < 2, we
change SIG(1, 3) to 0, but since 3 > 0 we do not change SIG(2, 3). The final
signature matrix is:
S1 S2 S3 S4
h1 1 3 0 1
h2 0 2 0 0
We can estimate the Jaccard similarities of the underlying sets from this
signature matrix. Notice that columns 1 and 4 are identical, so we guess that
SIM(S1, S4) = 1.0. If we look at Fig. 3.4, we see that the true Jaccard similarity
of S1 and S4 is 2/3. Remember that the fraction of rows that agree in the
signature matrix is only an estimate of the true Jaccard similarity, and this
example is much too small for the law of large numbers to assure that the
estimates are close. For additional examples, the signature columns for S1 and
S3 agree in half the rows (true similarity 1/4), while the signatures of S1 and
S2 estimate 0 as their Jaccard similarity (the correct value). ✷
3.3.6 Speeding Up Minhashing
The process of minhashing is time-consuming, since we need to examine the
entire k-row matrix M for each minhash function we want. Let us first return
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 87
to the model of Section 3.3.2, where we imagine rows are actually permuted.
But to compute one minhash function on all the columns, we shall not go all
the way to the end of the permutation, but only look at the first m out of k
rows. If we make m small compared with k, we reduce the work by a large
factor, k/m.
However, there is a downside to making m small. As long as each column
has at least one 1 in the first m rows in permuted order, the rows after the mth
have no effect on any minhash value and may as well not be looked at. But
what if some columns are all-0’s in the first m rows? We have no minhash value
for those columns, and will instead have to use a special symbol, for which we
shall use ∞.
When we examine the minhash signatures of two columns in order to estimate the Jaccard similarity of their underlying sets, as in Section 3.3.4, we have
to take into account the possibility that one or both columns have ∞ as their
minhash value for some components of the signature. There are three cases:
1. If neither column has ∞ in a given row, then there is no change needed.
Count this row as an example of equal values if the two values are the
same, and as an example of unequal values if not.
2. One column has ∞ and the other does not. In this case, had we used
all the rows of the original permuted matrix M, the column that has the
∞ would eventually have been given some row number, and that number
will surely not be one of the first m rows in the permuted order. But the
other column does have a value that is one of the first m rows. Thus, we
surely have an example of unequal minhash values, and we count this row
of the signature matrix as such an example.
3. Now, suppose both columns have ∞ in row. Then in the original permuted
matrix M, the first m rows of both columns were all 0’s. We thus have no
information about the Jaccard similarity of the corresponding sets; that
similarity is only a function of the last k − m rows, which we have chosen
not to look at. We therefore count this row of the signature matrix as
neither an example of equal values nor of unequal values.
As long as the third case, where both columns have ∞, is rare, we get
almost as many examples to average as there are rows in the signature matrix.
That effect will reduce the accuracy of our estimates of the Jaccard distance
somewhat, but not much. And since we are now able to compute minhash
values for all the columns much faster than if we examined all the rows of M,
we can afford the time to apply a few more minhash functions. We get even
better accuracy than originally, and we do so faster than before.
3.3.7 Speedup Using Hash Functions
As before, there are reasons not to physically permute rows in the manner
assumed in Section 3.3.6. However, the idea of true permutations makes more
88 CHAPTER 3. FINDING SIMILAR ITEMS
sense in the context of Section 3.3.6 than it did in Section 3.3.2. The reason
is that we do not need to construct a full permutation of k elements, but only
pick a small number m out of the k rows and then pick a random permutation
of those rows. Depending on the value of m and how the matrix M is stored, it
might make sense to follow the algorithm suggested by Section 3.3.6 literally.
However, it is more likely that a strategy akin to Section 3.3.5 is needed.
Now, the rows of M are fixed, and not permuted. We choose a hash function
that hashes row numbers, and compute hash values for only the first m rows.
That is, we follow the algorithm of Section 3.3.5, but only until we reach the
mth row, whereupon we stop and, and for each columns, we take the minimum
hash value seen so far as the minhash value for that column.
Since some column may have 0 in all m rows, it is possible that some of the
minhash values will be ∞. Assuming m is sufficiently large that ∞ minhash
values are rare, we still get a good estimate of the Jaccard similarity of sets by
comparing columns of the signature matrix. Suppose T is the set of elements
of the universal set that are represented by the first m rows of matrix M. Let
S1 and S2 be the sets represented by two columns of M. Then the first m rows
of M represent the sets S1 ∩ T and S2 ∩ T . If both these sets are empty (i.e.,
both columns are all-0 in their first m rows), then this minhash function will be
∞ in both columns and will be ignored when estimating the Jaccard similarity
of the columns’ underlying sets.
If at least one of the sets S1 ∩ T and S2 ∩ T is nonempty, then the probability of the two columns having equal values for this minhash function is the
Jaccard similarity of these two sets, that is
|S1 ∩ S2 ∩ T |
|(S1 ∪ S2) ∩ T |
As long as T is chosen to be a random subset of the universal set, the expected
value of this fraction will be the same as the Jaccard similarity of S1 and S2.
However, there will be some random variation, since depending on T , we could
find more or less than an average number of type X rows (1’s in both columns)
and/or type Y rows (1 in one column and 0 in the other) among the first m
rows of matrix M.
To mitigate this variation, we do not use the same set T for each minhashing
that we do. Rather, we divide the rows of M into k/m groups.4 Then for each
hash function, we compute one minhash value by examining only the first m
rows of M, a different minhash value by examining only the second m rows,
and so on. We thus get k/m minhash values from a single hash function and
a single pass over all the rows of M. In fact, if k/m is large enough, we may
get all the rows of the signature matrix that we need by a single hash function
applied to each of the subsets of rows of M.
4
In what follows, we assume m divides k evenly, for convenience. It is unimportant, as
long as k/m is large, if some rows are not included in any group because k is not an integer
multiple of m.
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 89
Moreover, by using each of the rows of M to compute one of these minhash
values, we tend to balance out the errors in estimation of the Jaccard similarity
due to any one particular subset of the rows. That is, the Jaccard similarity
of S1 and S2 determines the ratio of type X and type Y rows. All the type X
rows are distributed among the k/m sets of rows, and likewise the type Y rows.
Thus, while one set of m rows may have more of one type of row than average,
there must then be some other set of m rows with fewer than average of that
same type.
Example 3.9 : In Fig. 3.5 we see a matrix representing three sets S1, S2, and
S3, with a universal set of eight elements; i.e., k = 8. Let us pick m = 4, so one
pass through the rows yields two minhash values, one based on the first four
rows and the other on the second four rows.
S1 S2 S3
0 0 0
0 0 0
0 0 1
0 1 1
1 1 1
1 1 0
1 0 0
0 0 0
Figure 3.5: A Boolean matrix representing three sets
First, note that the Jaccard similarities of the three sets are SIM(S1, S2) =
1/2, SIM(S1, S3) = 1/5, and SIM(S2, S3) = 1/2. Now, look at the first four
rows only. Whatever hash function we use, the minhash value for S1 will be
∞, the minhash value for S2 will be the hash value of the 4th row, and the
minhash value for S3 will be the smaller of the hash values for the third and
fourth rows. Thus, the minhash values for S1 and S2 will never agree. That
makes sense, since if T is the set of elements represented by the first four rows,
then S1 ∩ T = ∅, and therefore SIM(S1 ∩ T, S2 ∩ T ) = 0. However, in the
second four rows, the Jaccard similarity of S1 and S2 restricted to the elements
represented by the last four rows is 2/3.
We conclude that if we generate signatures consisting of two minhash values
using this hash function, one based on the first four rows and the second based
on the last four rows, the expected number of matches we get between the
signatures for S1 and S2 is the average of 0 and 2/3, or 1/3. Since the actual
Jaccard similarity of S1 and S2 is 1/2, there is an error, but not too great an
error. In larger examples, where minhash values are based on far more than
four rows, the expected error will approach zero.
Similarly, we can see the effect of splitting the rows on the other two pairs
of columns. Between S1 and S3, the top half represents sets with a Jaccard
90 CHAPTER 3. FINDING SIMILAR ITEMS
similarity of 0, while the bottom half represents sets with a Jaccard similarity
1/3. The expected number of matches in the signatures of S1 and S3 is therefore
the average of these, or 1/6. That compares with the true Jaccard similarity
SIM(S1, S3) = 1/5. Finally, when we compare S2 and S3, we note that the
Jaccard similarity of these columns in the first four rows is 1/2, and so is their
Jaccard similarity in the bottom four rows. The average, 1/2, also agrees exactly
with SIM(S2, S3) = 1/2. ✷
3.3.8 Exercises for Section 3.3
Exercise 3.3.1 : Verify the theorem from Section 3.3.3, which relates the Jaccard similarity to the probability of minhashing to equal values, for the particular case of Fig. 3.2.
(a) Compute the Jaccard similarity of each of the pairs of columns in Fig. 3.2.
! (b) Compute, for each pair of columns of that figure, the fraction of the 120
permutations of the rows that make the two columns hash to the same
value.
Exercise 3.3.2 : Using the data from Fig. 3.4, add to the signatures of the
columns the values of the following hash functions:
(a) h3(x) = 2x + 4 mod 5.
(b) h4(x) = 3x − 1 mod 5.
Element S1 S2 S3 S4
0 0 1 0 1
1 0 1 0 0
2 1 0 0 1
3 0 0 1 0
4 0 0 1 1
5 1 0 0 0
Figure 3.6: Matrix for Exercise 3.3.3
Exercise 3.3.3 : In Fig. 3.6 is a matrix with six rows.
(a) Compute the minhash signature for each column if we use the following
three hash functions: h1(x) = 2x + 1 mod 6; h2(x) = 3x + 2 mod 6;
h3(x) = 5x + 2 mod 6.
(b) Which of these hash functions are true permutations?
3.4. LOCALITY-SENSITIVE HASHING FOR DOCUMENTS 91
(c) How close are the estimated Jaccard similarities for the six pairs of columns
to the true Jaccard similarities?
! Exercise 3.3.4 : Now that we know Jaccard similarity is related to the probability that two sets minhash to the same value, reconsider Exercise 3.1.3. Can
you use this relationship to simplify the problem of computing the expected
Jaccard similarity of randomly chosen sets?
! Exercise 3.3.5 : Prove that if the Jaccard similarity of two columns is 0, then
minhashing always gives a correct estimate of the Jaccard similarity.
!! Exercise 3.3.6 : One might expect that we could estimate the Jaccard similarity of columns without using all possible permutations of rows. For example,
we could only allow cyclic permutations; i.e., start at a randomly chosen row
r, which becomes the first in the order, followed by rows r + 1, r + 2, and so
on, down to the last row, and then continuing with the first row, second row,
and so on, down to row r − 1. There are only n such permutations if there are
n rows. However, these permutations are not sufficient to estimate the Jaccard
similarity correctly. Give an example of a two-column matrix where averaging
over all the cyclic permutations does not give the Jaccard similarity.
! Exercise 3.3.7 : Suppose we want to use a MapReduce framework to compute
minhash signatures. If the matrix is stored in chunks that correspond to some
columns, then it is quite easy to exploit parallelism. Each Map task gets some
of the columns and all the hash functions, and computes the minhash signatures
of its given columns. However, suppose the matrix were chunked by rows, so
that a Map task is given the hash functions and a set of rows to work on. Design
Map and Reduce functions to exploit MapReduce with data in this form.
! Exercise 3.3.8 : As we noticed in Section 3.3.6, we have problems when a
column has only 0’s. If we compute a minhash function using entire columns
(as in Section 3.3.2), then the only time we get all 0’s in a column is if that
column represents the empty set. How should we handle the empty set to make
sure no errors in Jaccard-similarity estimation are introduced?
!! Exercise 3.3.9 : In Example 3.9, each of the three estimates of Jaccard similarity we obtained was either smaller than or the same as the true Jaccard
similarity. Is it possible that for another pair of columns, the average of the
Jaccard similarities of the upper and lower halves will exceed the actual Jaccard
similarity of the columns?
3.4 Locality-Sensitive Hashing for Documents
Even though we can use minhashing to compress large documents into small
signatures and preserve the expected similarity of any pair of documents, it
still may be impossible to find the pairs with greatest similarity efficiently. The
92 CHAPTER 3. FINDING SIMILAR ITEMS
reason is that the number of pairs of documents may be too large, even if there
are not too many documents.
Example 3.10 : Suppose we have a million documents, and we use signatures
of length 250. Then we use 1000 bytes per document for the signatures, and
the entire data fits in a gigabyte – less than a typical main memory of a laptop.
However, there are
1,000,000
2

or half a trillion pairs of documents. If it takes a
microsecond to compute the similarity of two signatures, then it takes almost
six days to compute all the similarities on that laptop. ✷
If our goal is to compute the similarity of every pair, there is nothing we
can do to reduce the work, although parallelism can reduce the elapsed time.
However, often we want only the most similar pairs or all pairs that are above
some lower bound in similarity. If so, then we need to focus our attention only
on pairs that are likely to be similar, without investigating every pair. There is
a general theory of how to provide such focus, called locality-sensitive hashing
(LSH) or near-neighbor search. In this section we shall consider a specific form
of LSH, designed for the particular problem we have been studying: documents,
represented by shingle-sets, then minhashed to short signatures. In Section 3.6
we present the general theory of locality-sensitive hashing and a number of
applications and related techniques.
3.4.1 LSH for Minhash Signatures
One general approach to LSH is to “hash” items several times, in such a way that
similar items are more likely to be hashed to the same bucket than dissimilar
items are. We then consider any pair that hashed to the same bucket for any
of the hashings to be a candidate pair. We check only the candidate pairs for
similarity. The hope is that most of the dissimilar pairs will never hash to the
same bucket, and therefore will never be checked. Those dissimilar pairs that
do hash to the same bucket are false positives; we hope these will be only a
small fraction of all pairs. We also hope that most of the truly similar pairs
will hash to the same bucket under at least one of the hash functions. Those
that do not are false negatives; we hope these will be only a small fraction of
the truly similar pairs.
If we have minhash signatures for the items, an effective way to choose the
hashings is to divide the signature matrix into b bands consisting of r rows
each. For each band, there is a hash function that takes vectors of r integers
(the portion of one column within that band) and hashes them to some large
number of buckets. We can use the same hash function for all the bands, but
we use a separate bucket array for each band, so columns with the same vector
in different bands will not hash to the same bucket.
Example 3.11 : Figure 3.7 shows part of a signature matrix of 12 rows divided
into four bands of three rows each. The second and fourth of the explicitly
shown columns each have the column vector [0, 2, 1] in the first band, so the
3.4. LOCALITY-SENSITIVE HASHING FOR DOCUMENTS 93
1 0 0 0 2
3 2 1 2 2
0 1 3 1 1
. . . . . . band 1
band 2
band 3
band 4
Figure 3.7: Dividing a signature matrix into four bands of three rows per band
will definitely hash to the same bucket in the hashing for the first band. Thus,
regardless of what those columns look like in the other three bands, this pair
of columns will be a candidate pair. It is possible that other columns, such as
the first two shown explicitly, will also hash to the same bucket according to
the hashing of the first band. However, since their column vectors are different,
[1, 3, 0] and [0, 2, 1], and there are many buckets for each hashing, we expect the
chances of an accidental collision to be very small. We shall normally assume
that two vectors hash to the same bucket if and only if they are identical.
Two columns that do not agree in band 1 have three other chances to become
a candidate pair; they might be identical in any one of these other bands.
However, observe that the more similar two columns are, the more likely it is
that they will be identical in some band. Thus, intuitively the banding strategy
makes similar columns much more likely to be candidate pairs than dissimilar
pairs. ✷
3.4.2 Analysis of the Banding Technique
Suppose we use b bands of r rows each, and suppose that a particular pair of
documents have Jaccard similarity s. Recall from Section 3.3.3 that the probability the minhash signatures for these documents agree in any one particular
row of the signature matrix is s. We can calculate the probability that these
documents (or rather their signatures) become a candidate pair as follows:
1. The probability that the signatures agree in all rows of one particular
band is s
r
.
2. The probability that the signatures disagree in at least one row of a particular band is 1 − s
r
.
3. The probability that the signatures disagree in at least one row of each
of the bands is (1 − s
r
)
b
.
94 CHAPTER 3. FINDING SIMILAR ITEMS
4. The probability that the signatures agree in all the rows of at least one
band, and therefore become a candidate pair, is 1 − (1 − s
r
)
b
.
0 1
 of documents
Jaccard similarity
Probability
of becoming
a candidate
Figure 3.8: The S-curve
It may not be obvious, but regardless of the chosen constants b and r, this
function has the form of an S-curve, as suggested in Fig. 3.8. The threshold,
that is, the value of similarity s at which the probability of becoming a candidate is 1/2, is a function of b and r. The threshold is roughly where the rise is
the steepest, and for large b and r we find that pairs with similarity above the
threshold are very likely to become candidates, while those below the threshold
are unlikely to become candidates – exactly the situation we want. An approximation to the threshold is (1/b)
1/r. For example, if b = 16 and r = 4, then the
threshold is approximately at s = 1/2, since the 4th root of 1/16 is 1/2.
Example 3.12 : Let us consider the case b = 20 and r = 5. That is, we suppose
we have signatures of length 100, divided into twenty bands of five rows each.
Figure 3.9 tabulates some of the values of the function 1 − (1 − s
5
)
20. Notice
that the threshold, the value of s at which the curve has risen halfway, is just
slightly more than 0.5. Also notice that the curve is not exactly the ideal step
function that jumps from 0 to 1 at the threshold, but the slope of the curve
in the middle is significant. For example, it rises by more than 0.6 going from
s = 0.4 to s = 0.6, so the slope in the middle is greater than 3.
For example, at s = 0.8, 1 − (0.8)5
is about 0.672. If you raise this number
to the 20th power, you get about 0.00035. Subtracting this fraction from 1
yields 0.99965. That is, if we consider two documents with 80% similarity, then
in any one band, they have only about a 33% chance of agreeing in all five rows
and thus becoming a candidate pair. However, there are 20 bands and thus 20
3.4. LOCALITY-SENSITIVE HASHING FOR DOCUMENTS 95
s 1 − (1 − s
r
)
b
.2 .006
.3 .047
.4 .186
.5 .470
.6 .802
.7 .975
.8 .9996
Figure 3.9: Values of the S-curve for b = 20 and r = 5
chances to become a candidate. Only roughly one in 3000 pairs that are as high
as 80% similar will fail to become a candidate pair and thus be a false negative.
✷
3.4.3 Combining the Techniques
We can now give an approach to finding the set of candidate pairs for similar
documents and then discovering the truly similar documents among them. It
must be emphasized that this approach can produce false negatives – pairs of
similar documents that are not identified as such because they never become
a candidate pair. There will also be false positives – candidate pairs that are
evaluated, but are found not to be sufficiently similar.
1. Pick a value of k and construct from each document the set of k-shingles.
Optionally, hash the k-shingles to shorter bucket numbers.
2. Sort the document-shingle pairs to order them by shingle.
3. Pick a length n for the minhash signatures. Feed the sorted list to the
algorithm of Section 3.3.5 to compute the minhash signatures for all the
documents.
4. Choose a threshold t that defines how similar documents have to be in
order for them to be regarded as a desired “similar pair.” Pick a number
of bands b and a number of rows r such that br = n, and the threshold
t is approximately (1/b)
1/r. If avoidance of false negatives is important,
you may wish to select b and r to produce a threshold lower than t; if
speed is important and you wish to limit false positives, select b and r to
produce a higher threshold.
5. Construct candidate pairs by applying the LSH technique of Section 3.4.1.
6. Examine each candidate pair’s signatures and determine whether the fraction of components in which they agree is at least t.
96 CHAPTER 3. FINDING SIMILAR ITEMS
7. Optionally, if the signatures are sufficiently similar, go to the documents
themselves and check that they are truly similar, rather than documents
that, by luck, had similar signatures.
3.4.4 Exercises for Section 3.4
Exercise 3.4.1 : Evaluate the S-curve 1 − (1 − s
r
)
b
for s = 0.1, 0.2, . . . , 0.9, for
the following values of r and b:
• r = 3 and b = 10.
• r = 6 and b = 20.
• r = 5 and b = 50.
! Exercise 3.4.2 : For each of the (r, b) pairs in Exercise 3.4.1, compute the
threshold, that is, the value of s for which the value of 1−(1−s
r
)
b
is exactly 1/2.
How does this value compare with the estimate of (1/b)
1/r that was suggested
in Section 3.4.2?
! Exercise 3.4.3 : Use the techniques explained in Section 1.3.5 to approximate
the S-curve 1 − (1 − s
r
)
b when s
r
is very small.
! Exercise 3.4.4 : Suppose we wish to implement LSH by MapReduce. Specifically, assume chunks of the signature matrix consist of columns, and elements
are key-value pairs where the key is the column number and the value is the
signature itself (i.e., a vector of values).
(a) Show how to produce the buckets for all the bands as output of a single
MapReduce process. Hint: Remember that a Map function can produce
several key-value pairs from a single element.
(b) Show how another MapReduce process can convert the output of (a) to
a list of pairs that need to be compared. Specifically, for each column i,
there should be a list of those columns j > i with which i needs to be
compared.
3.5 Distance Measures
We now take a short detour to study the general notion of distance measures.
The Jaccard similarity is a measure of how close sets are, although it is not
really a distance measure. That is, the closer sets are, the higher the Jaccard
similarity. Rather, 1 minus the Jaccard similarity is a distance measure, as we
shall see; it is called the Jaccard distance.
However, Jaccard distance is not the only measure of closeness that makes
sense. We shall examine in this section some other distance measures that have
applications. Then, in Section 3.6 we see how some of these distance measures
3.5. DISTANCE MEASURES 97
also have an LSH technique that allows us to focus on nearby points without
comparing all points. Other applications of distance measures will appear when
we study clustering in Chapter 7.
3.5.1 Definition of a Distance Measure
Suppose we have a set of points, called a space. A distance measure on this
space is a function d(x, y) that takes two points in the space as arguments and
produces a real number, and satisfies the following axioms:
1. d(x, y) ≥ 0 (no negative distances).
2. d(x, y) = 0 if and only if x = y (distances are positive, except for the
distance from a point to itself).
3. d(x, y) = d(y, x) (distance is symmetric).
4. d(x, y) ≤ d(x, z) + d(z, y) (the triangle inequality).
The triangle inequality is the most complex condition. It says, intuitively, that
to travel from x to y, we cannot obtain any benefit if we are forced to travel via
some particular third point z. The triangle-inequality axiom is what makes all
distance measures behave as if distance describes the length of a shortest path
from one point to another.
3.5.2 Euclidean Distances
The most familiar distance measure is the one we normally think of as “distance.” An n-dimensional Euclidean space is one where points are vectors of n
real numbers. The conventional distance measure in this space, which we shall
refer to as the L2-norm, is defined:
d([x1, x2, . . . , xn], [y1, y2, . . . , yn]) =
vuutXn
i=1
(xi − yi)
2
That is, we square the distance in each dimension, sum the squares, and take
the positive square root.
It is easy to verify the first three requirements for a distance measure are
satisfied. The Euclidean distance between two points cannot be negative, because the positive square root is intended. Since all squares of real numbers are
nonnegative, any i such that xi 6= yi forces the distance to be strictly positive.
On the other hand, if xi = yi for all i, then the distance is clearly 0. Symmetry
follows because (xi − yi)
2 = (yi − xi)
2
. The triangle inequality requires a good
deal of algebra to verify. However, it is well understood to be a property of
Euclidean space: the sum of the lengths of any two sides of a triangle is no less
than the length of the third side.
98 CHAPTER 3. FINDING SIMILAR ITEMS
There are other distance measures that have been used for Euclidean spaces.
For any constant r, we can define the Lr-norm to be the distance measure d
defined by:
d([x1, x2, . . . , xn], [y1, y2, . . . , yn]) = (Xn
i=1
|xi − yi
|
r
)
1/r
The case r = 2 is the usual L2-norm just mentioned. Another common distance
measure is the L1-norm, or Manhattan distance. There, the distance between
two points is the sum of the magnitudes of the differences in each dimension.
It is called “Manhattan distance” because it is the distance one would have to
travel between points if one were constrained to travel along grid lines, as on
the streets of a city such as Manhattan.
Another interesting distance measure is the L∞-norm, which is the limit
as r approaches infinity of the Lr-norm. As r gets larger, only the dimension
with the largest difference matters, so formally, the L∞-norm is defined as the
maximum of |xi − yi
| over all dimensions i.
Example 3.13 : Consider the two-dimensional Euclidean space (the customary plane) and the points (2, 7) and (6, 4). The L2-norm gives a distance
of p
(2 − 6)2 + (7 − 4)2 =
√
4
2 + 32 = 5. The L1-norm gives a distance of
|2 − 6| + |7 − 4| = 4 + 3 = 7. The L∞-norm gives a distance of
max(|2 − 6|, |7 − 4|) = max(4, 3) = 4
✷
3.5.3 Jaccard Distance
As mentioned at the beginning of the section, we define the Jaccard distance
of sets by d(x, y) = 1 − SIM(x, y). That is, the Jaccard distance is 1 minus the
ratio of the sizes of the intersection and union of sets x and y. We must verify
that this function is a distance measure.
1. d(x, y) is nonnegative because the size of the intersection cannot exceed
the size of the union.
2. d(x, y) = 0 if x = y, because x ∪ x = x ∩ x = x. However, if x 6= y, then
the size of x ∩ y is strictly less than the size of x ∪ y, so d(x, y) is strictly
positive.
3. d(x, y) = d(y, x) because both union and intersection are symmetric; i.e.,
x ∪ y = y ∪ x and x ∩ y = y ∩ x.
4. For the triangle inequality, recall from Section 3.3.3 that SIM(x, y) is the
probability a random minhash function maps x and y to the same value.
Thus, the Jaccard distance d(x, y) is the probability that a random minhash function does not send x and y to the same value. We can therefore
3.5. DISTANCE MEASURES 99
translate the condition d(x, y) ≤ d(x, z) + d(z, y) to the statement that if
h is a random minhash function, then the probability that h(x) 6= h(y)
is no greater than the sum of the probability that h(x) 6= h(z) and the
probability that h(z) 6= h(y). However, this statement is true because
whenever h(x) 6= h(y), at least one of h(x) and h(y) must be different
from h(z). They could not both be h(z), because then h(x) and h(y)
would be the same.
.
3.5.4 Cosine Distance
The cosine distance makes sense in spaces that have dimensions, including Euclidean spaces and discrete versions of Euclidean spaces, such as spaces where
points are vectors with integer components or Boolean (0 or 1) components. In
such a space, points may be thought of as directions. We do not distinguish between a vector and a multiple of that vector. Then the cosine distance between
two points is the angle that the vectors to those points make. This angle will
be in the range 0 to 180 degrees, regardless of how many dimensions the space
has.
We can calculate the cosine distance by first computing the cosine of the
angle, and then applying the arc-cosine function to translate to an angle in the
0-180 degree range. Given two vectors x and y, the cosine of the angle between
them is the dot product x.y divided by the L2-norms of x and y (i.e., their
Euclidean distances from the origin). Recall that the dot product of vectors
[x1, x2, . . . , xn].[y1, y2, . . . , yn] is Pn
i=1 xiyi
.
Example 3.14 : Let our two vectors be x = [1, 2, −1] and = [2, 1, 1]. The dot
√
product x.y is 1 × 2 + 2 × 1 + (−1) × 1 = 3. The L2-norm of both vectors is
6. For example, x has L2-norm p
1
2 + 22 + (−1)2 =
√
6. Thus, the cosine of
the angle between x and y is 3/(
√
6
√
6) or 1/2. The angle whose cosine is 1/2
is 60 degrees, so that is the cosine distance between x and y. ✷
We must show that the cosine distance is indeed a distance measure. We
have defined it so the values are in the range 0 to 180, so no negative distances
are possible. Two vectors have angle 0 if and only if they are the same direction.5
Symmetry is obvious: the angle between x and y is the same as the angle
between y and x. The triangle inequality is best argued by physical reasoning.
One way to rotate from x to y is to rotate to z and thence to y. The sum of
those two rotations cannot be less than the rotation directly from x to y.
5Notice that to satisfy the second axiom, we have to treat vectors that are multiples of
one another, e.g. [1, 2] and [3, 6], as the same direction, which they are. If we regarded these
as different vectors, we would give them distance 0 and thus violate the condition that only
d(x, x) is 0.
100 CHAPTER 3. FINDING SIMILAR ITEMS
3.5.5 Edit Distance
This distance makes sense when points are strings. The distance between two
strings x = x1x2 · · · xn and y = y1y2 · · · ym is the smallest number of insertions
and deletions of single characters that will convert x to y.
Example 3.15 : The edit distance between the strings x = abcde and y =
acfdeg is 3. To convert x to y:
1. Delete b.
2. Insert f after c.
3. Insert g after e.
No sequence of fewer than three insertions and/or deletions will convert x to y.
Thus, d(x, y) = 3. ✷
Another way to define and calculate the edit distance d(x, y) is to compute
a longest common subsequence (LCS) of x and y. An LCS of x and y is a
string that is constructed by deleting positions from x and y, and that is as
long as any string that can be constructed that way. The edit distance d(x, y)
can be calculated as the length of x plus the length of y minus twice the length
of their LCS.
Example 3.16 : The strings x = abcde and y = acfdeg from Example 3.15
have a unique LCS, which is acde. We can be sure it is the longest possible,
because it contains every symbol appearing in both x and y. Fortunately, these
common symbols appear in the same order in both strings, so we are able to
use them all in an LCS. Note that the length of x is 5, the length of y is 6, and
the length of their LCS is 4. The edit distance is thus 5 + 6 − 2 × 4 = 3, which
agrees with the direct calculation in Example 3.15.
For another example, consider x = aba and y = bab. Their edit distance is
2. For example, we can convert x to y by deleting the first a and then inserting
b at the end. There are two LCS’s: ab and ba. Each can be obtained by
deleting one symbol from each string. As must be the case for multiple LCS’s
of the same pair of strings, both LCS’s have the same length. Therefore, we
may compute the edit distance as 3 + 3 − 2 × 2 = 2. ✷
Edit distance is a distance measure. Surely no edit distance can be negative,
and only two identical strings have an edit distance of 0. To see that edit
distance is symmetric, note that a sequence of insertions and deletions can be
reversed, with each insertion becoming a deletion, and vice versa. The triangle
inequality is also straightforward. One way to turn a string s into a string t
is to turn s into some string u and then turn u into t. Thus, the number of
edits made going from s to u, plus the number of edits made going from u to t
cannot be less than the smallest number of edits that will turn s into t.
3.5. DISTANCE MEASURES 101
Non-Euclidean Spaces
Notice that several of the distance measures introduced in this section are
not Euclidean spaces. A property of Euclidean spaces that we shall find
important when we take up clustering in Chapter 7 is that the average
of points in a Euclidean space always exists and is a point in the space.
However, consider the space of sets for which we defined the Jaccard distance. The notion of the “average” of two sets makes no sense. Likewise,
the space of strings, where we can use the edit distance, does not let us
take the “average” of strings.
Vector spaces, for which we suggested the cosine distance, may or may
not be Euclidean. If the components of the vectors can be any real numbers, then the space is Euclidean. However, if we restrict components to
be integers, then the space is not Euclidean. Notice that, for instance, we
cannot find an average of the vectors [1, 2] and [3, 1] in the space of vectors
with two integer components, although if we treated them as members of
the two-dimensional Euclidean space, then we could say that their average
was [2.0, 1.5].
3.5.6 Hamming Distance
Given a space of vectors, we define the Hamming distance between two vectors
to be the number of components in which they differ. It should be obvious
that Hamming distance is a distance measure. Clearly the Hamming distance
cannot be negative, and if it is zero, then the vectors are identical. The distance does not depend on which of two vectors we consider first. The triangle
inequality should also be evident. If x and z differ in m components, and z
and y differ in n components, then x and y cannot differ in more than m + n
components. Most commonly, Hamming distance is used when the vectors are
Boolean; they consist of 0’s and 1’s only. However, in principle, the vectors can
have components from any set.
Example 3.17 : The Hamming distance between the vectors 10101 and 11110
is 3. That is, these vectors differ in the second, fourth, and fifth components,
while they agree in the first and third components. ✷
3.5.7 Exercises for Section 3.5
! Exercise 3.5.1 : On the space of nonnegative integers, which of the following
functions are distance measures? If so, prove it; if not, prove that it fails to
satisfy one or more of the axioms.
(a) max(x, y) = the larger of x and y.
102 CHAPTER 3. FINDING SIMILAR ITEMS
(b) diff(x, y) = |x − y| (the absolute magnitude of the difference between x
and y).
(c) sum(x, y) = x + y.
Exercise 3.5.2 : Find the L1 and L2 distances between the points (5, 6, 7) and
(8, 2, 4).
!! Exercise 3.5.3 : Prove that if i and j are any positive integers, and i < j,
then the Li norm between any two points is greater than the Lj norm between
those same two points.
Exercise 3.5.4 : Find the Jaccard distances between the following pairs of
sets:
(a) {1, 2, 3, 4} and {2, 3, 4, 5}.
(b) {1, 2, 3} and {4, 5, 6}.
Exercise 3.5.5 : Compute the cosines of the angles between each of the following pairs of vectors.6
(a) (3, −1, 2) and (−2, 3, 1).
(b) (1, 2, 3) and (2, 4, 6).
(c) (5, 0, −4) and (−1, −6, 2).
(d) (0, 1, 1, 0, 1, 1) and (0, 0, 1, 0, 0, 0).
! Exercise 3.5.6 : Prove that the cosine distance between any two vectors of 0’s
and 1’s, of the same length, is at most 90 degrees.
Exercise 3.5.7 : Find the edit distances (using only insertions and deletions)
between the following pairs of strings.
(a) abcdef and bdaefc.
(b) abccdabc and acbdcab.
(c) abcdef and baedfc.
! Exercise 3.5.8 : There are a number of other notions of edit distance available.
For instance, we can allow, in addition to insertions and deletions, the following
operations:
6Note that what we are asking for is not precisely the cosine distance, but from the cosine
of an angle, you can compute the angle itself, perhaps with the aid of a table or library
function.
3.6. THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS 103
i. Mutation, where one symbol is replaced by another symbol. Note that a
mutation can always be performed by an insertion followed by a deletion,
but if we allow mutations, then this change counts for only 1, not 2, when
computing the edit distance.
ii. Transposition, where two adjacent symbols have their positions swapped.
Like a mutation, we can simulate a transposition by one insertion followed
by one deletion, but here we count only 1 for these two steps.
Repeat Exercise 3.5.7 if edit distance is defined to be the number of insertions,
deletions, mutations, and transpositions needed to transform one string into
another.
! Exercise 3.5.9 : Prove that the edit distance discussed in Exercise 3.5.8 is
indeed a distance measure.
Exercise 3.5.10 : Find the Hamming distances between each pair of the following vectors: 000000, 110011, 010101, and 011100.
3.6 The Theory of Locality-Sensitive Functions
The LSH technique developed in Section 3.4 is one example of a family of functions (the minhash functions) that can be combined (by the banding technique)
to distinguish strongly between pairs at a low distance from pairs at a high distance. The steepness of the S-curve in Fig. 3.8 reflects how effectively we can
avoid false positives and false negatives among the candidate pairs.
Now, we shall explore other families of functions, besides the minhash functions, that can serve to produce candidate pairs efficiently. These functions can
apply to the space of sets and the Jaccard distance, or to another space and/or
another distance measure. There are three conditions that we need for a family
of functions:
1. They must be more likely to make close pairs be candidate pairs than
distant pairs. We make this notion precise in Section 3.6.1.
2. They must be statistically independent, in the sense that it is possible to
estimate the probability that two or more functions will all give a certain
response by the product rule for independent events.
3. They must be efficient, in two ways:
(a) They must be able to identify candidate pairs in time much less
than the time it takes to look at all pairs. For example, minhash
functions have this capability, since we can hash sets to minhash
values in time proportional to the size of the data, rather than the
square of the number of sets in the data. Since sets with common
values are colocated in a bucket, we have implicitly produced the
104 CHAPTER 3. FINDING SIMILAR ITEMS
candidate pairs for a single minhash function in time much less than
the number of pairs of sets.
(b) They must be combinable to build functions that are better at avoiding false positives and negatives, and the combined functions must
also take time that is much less than the number of pairs. For example, the banding technique of Section 3.4.1 takes single minhash
functions, which satisfy condition 3a but do not, by themselves have
the S-curve behavior we want, and produces from a number of minhash functions a combined function that has the S-curve shape.
Our first step is to define “locality-sensitive functions” generally. We then
see how the idea can be applied in several applications. Finally, we discuss
how to apply the theory to arbitrary data with either a cosine distance or a
Euclidean distance measure.
3.6.1 Locality-Sensitive Functions
For the purposes of this section, we shall consider functions that take two items
and render a decision about whether these items should be a candidate pair.
In many cases, the function f will “hash” items, and the decision will be based
on whether or not the result is equal. Because it is convenient to use the
notation f(x) = f(y) to mean that f(x, y) is “yes; make x and y a candidate
pair,” we shall use f(x) = f(y) as a shorthand with this meaning. We also use
f(x) 6= f(y) to mean “do not make x and y a candidate pair unless some other
function concludes we should do so.”
A collection of functions of this form will be called a family of functions.
For example, the family of minhash functions, each based on one of the possible
permutations of rows of a characteristic matrix, form a family.
Let d1 < d2 be two distances according to some distance measure d. A
family F of functions is said to be (d1, d2, p1, p2)-sensitive if for every f in F:
1. If d(x, y) ≤ d1, then the probability that f(x) = f(y) is at least p1.
2. If d(x, y) ≥ d2, then the probability that f(x) = f(y) is at most p2.
Figure 3.10 illustrates what we expect about the probability that a given
function in a (d1, d2, p1, p2)-sensitive family will declare two items to be a candidate pair. Notice that we say nothing about what happens when the distance
between the items is strictly between d1 and d2, but we can make d1 and d2 as
close as we wish. The penalty is that typically p1 and p2 are then close as well.
As we shall see, it is possible to drive p1 and p2 apart while keeping d1 and d2
fixed.
3.6.2 Locality-Sensitive Families for Jaccard Distance
For the moment, we have only one way to find a family of locality-sensitive
functions: use the family of minhash functions, and assume that the distance
3.6. THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS 105
Probabilty
of being
declared a
candidate
d
p
d
p
1 2
1
2
Distance
Figure 3.10: Behavior of a (d1, d2, p1, p2)-sensitive function
measure is the Jaccard distance. As before, we interpret a minhash function h
to make x and y a candidate pair if and only if h(x) = h(y).
• The family of minhash functions is a (d1, d2, 1−d1, 1−d2)-sensitive family
for any d1 and d2, where 0 ≤ d1 < d2 ≤ 1.
The reason is that if d(x, y) ≤ d1, where d is the Jaccard distance, then
SIM(x, y) = 1 − d(x, y) ≥ 1 − d1. But we know that the Jaccard similarity
of x and y is equal to the probability that a minhash function will hash x and
y to the same value. A similar argument applies to d2 or any distance.
Example 3.18 : We could let d1 = 0.3 and d2 = 0.6. Then we can assert that
the family of minhash functions is a (0.3, 0.6, 0.7, 0.4)-sensitive family. That is,
if the Jaccard distance between x and y is at most 0.3 (i.e., SIM(x, y) ≥ 0.7)
then there is at least a 0.7 chance that a minhash function will send x and y to
the same value, and if the Jaccard distance between x and y is at least 0.6 (i.e.,
SIM(x, y) ≤ 0.4), then there is at most a 0.4 chance that x and y will be sent
to the same value. Note that we could make the same assertion with another
choice of d1 and d2; only d1 < d2 is required. ✷
3.6.3 Amplifying a Locality-Sensitive Family
Suppose we are given a (d1, d2, p1, p2)-sensitive family F. We can construct a
new family F
′
by the AND-construction on F, which is defined as follows. Each
member of F
′
consists of r members of F for some fixed r. If f is in F
′
, and f is
constructed from the set {f1, f2, . . . , fr} of members of F, we say f(x) = f(y)
if and only if fi(x) = fi(y) for all i = 1, 2, . . . , r. Notice that this construction
mirrors the effect of the r rows in a single band: the band makes x and y a
106 CHAPTER 3. FINDING SIMILAR ITEMS
candidate pair if every one of the r rows in the band say that x and y are equal
(and therefore a candidate pair according to that row).
Since the members of F are independently chosen to make a member of F
′
,
we can assert that F
′
is a
d1, d2,(p1)
r
,(p2)
r

-sensitive family. That is, for any
p, if p is the probability that a member of F will declare (x, y) to be a candidate
pair, then the probability that a member of F
′ will so declare is p
r
.
There is another construction, which we call the OR-construction, that turns
a (d1, d2, p1, p2)-sensitive family F into a
d1, d2, 1 − (1 − p1)
b
, 1 − (1 − p2)
b

-
sensitive family F
′
. Each member f of F
′
is constructed from b members of F,
say f1, f2, . . . , fb. We define f(x) = f(y) if and only if fi(x) = fi(y) for one or
more values of i. The OR-construction mirrors the effect of combining several
bands: x and y become a candidate pair if any band makes them a candidate
pair.
If p is the probability that a member of F will declare (x, y) to be a candidate
pair, then 1−p is the probability it will not so declare. (1−p)
b
is the probability
that none of f1, f2, . . . , fb will declare (x, y) a candidate pair, and 1 − (1 − p)
b
is the probability that at least one fi will declare (x, y) a candidate pair, and
therefore that f will declare (x, y) to be a candidate pair.
Notice that the AND-construction lowers all probabilities, but if we choose F
and r judiciously, we can make the small probability p2 get very close to 0, while
the higher probability p1 stays significantly away from 0. Similarly, the ORconstruction makes all probabilities rise, but by choosing F and b judiciously,
we can make the larger probability approach 1 while the smaller probability
remains bounded away from 1. We can cascade AND- and OR-constructions in
any order to make the low probability close to 0 and the high probability close
to 1. Of course the more constructions we use, and the higher the values of r
and b that we pick, the larger the number of functions from the original family
that we are forced to use. Thus, the better the final family of functions is, the
longer it takes to apply the functions from this family.
Example 3.19 : Suppose we start with a family F. We use the AND-construction with r = 4 to produce a family F1. We then apply the OR-construction
to F1 with b = 4 to produce a third family F2. Note that the members of F2
each are built from 16 members of F, and the situation is analogous to starting
with 16 minhash functions and treating them as four bands of four rows each.
The 4-way AND-function converts any probability p into p
4
. When we
follow it by the 4-way OR-construction, that probability is further converted
into 1−(1−p
4
)
4
. Some values of this transformation are indicated in Fig. 3.11.
This function is an S-curve, staying low for a while, then rising steeply (although
not too steeply; the slope never gets much higher than 2), and then leveling
off at high values. Like any S-curve, it has a fixedpoint, the value of p that is
left unchanged when we apply the function of the S-curve. In this case, the
fixedpoint is the value of p for which p = 1 − (1 − p
4
)
4
. We can see that the
fixedpoint is somewhere between 0.7 and 0.8. Below that value, probabilities are
decreased, and above it they are increased. Thus, if we pick a high probabili
3.6. THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS 107
p 1 − (1 − p
4
)
4
0.2 0.0064
0.3 0.0320
0.4 0.0985
0.5 0.2275
0.6 0.4260
0.7 0.6666
0.8 0.8785
0.9 0.9860
Figure 3.11: Effect of the 4-way AND-construction followed by the 4-way ORconstruction
above the fixedpoint and a low probability below it, we shall have the desired
effect that the low probability is decreased and the high probability is increased.
Suppose F is the minhash functions, regarded as a (0.2, 0.6, 0.8, 0.4)-sensitive family. Then F2, the family constructed by a 4-way AND followed by a
4-way OR, is a (0.2, 0.6, 0.8785, 0.0985)-sensitive family, as we can read from the
rows for 0.8 and 0.4 in Fig. 3.11. By replacing F by F2, we have reduced both
the false-negative and false-positive rates, at the cost of making application of
the functions take 16 times as long. ✷
p

1 − (1 − p)
4
4
0.1 0.0140
0.2 0.1215
0.3 0.3334
0.4 0.5740
0.5 0.7725
0.6 0.9015
0.7 0.9680
0.8 0.9936
Figure 3.12: Effect of the 4-way OR-construction followed by the 4-way ANDconstruction
Example 3.20 : For the same cost, we can apply a 4-way OR-construction
followed by a 4-way AND-construction. Figure 3.12 gives the transformation
on probabilities implied by this construction. For instance, suppose that F is a
(0.2, 0.6, 0.8, 0.4)-sensitive family. Then the constructed family is a
(0.2, 0.6, 0.9936, 0.5740)-sensitiv
108 CHAPTER 3. FINDING SIMILAR ITEMS
family. This choice is not necessarily the best. Although the higher probability
has moved much closer to 1, the lower probability has also raised, increasing
the number of false positives. ✷
Example 3.21 : We can cascade constructions as much as we like. For example, we could use the construction of Example 3.19 on the family of minhash
functions and then use the construction of Example 3.20 on the resulting family.
The constructed family would then have functions each built from 256 minhash
functions. It would, for instance transform a (0.2, 0.8, 0.8, 0.2)-sensitive family
into a (0.2, 0.8, 0.9991285, 0.0000004)-sensitive family. ✷
3.6.4 Exercises for Section 3.6
Exercise 3.6.1 : What is the effect on probability of starting with the family
of minhash functions and applying:
(a) A 2-way AND construction followed by a 3-way OR construction.
(b) A 3-way OR construction followed by a 2-way AND construction.
(c) A 2-way AND construction followed by a 2-way OR construction, followed
by a 2-way AND construction.
(d) A 2-way OR construction followed by a 2-way AND construction, followed
by a 2-way OR construction followed by a 2-way AND construction.
Exercise 3.6.2 : Find the fixedpoints for each of the functions constructed in
Exercise 3.6.1.
! Exercise 3.6.3 : Any function of probability p, such as that of Fig. 3.11, has
a slope given by the derivative of the function. The maximum slope is where
that derivative is a maximum. Find the value of p that gives a maximum slope
for the S-curves given by Fig. 3.11 and Fig. 3.12. What are the values of these
maximum slopes?
!! Exercise 3.6.4 : Generalize Exercise 3.6.3 to give, as a function of r and b, the
point of maximum slope and the value of that slope, for families of functions
defined from the minhash functions by:
(a) An r-way AND construction followed by a b-way OR construction.
(b) A b-way OR construction followed by an r-way AND construction.
3.7 LSH Families for Other Distance Measures
There is no guarantee that a distance measure has a locality-sensitive family of
hash functions. So far, we have only seen such families for the Jaccard distance.
In this section, we shall show how to construct locality-sensitive families for
Hamming distance, the cosine distance and for the normal Euclidean distance.
3.7. LSH FAMILIES FOR OTHER DISTANCE MEASURES 109
3.7.1 LSH Families for Hamming Distance
It is quite simple to build a locality-sensitive family of functions for the Hamming distance. Suppose we have a space of d-dimensional vectors, and h(x, y)
denotes the Hamming distance between vectors x and y. If we take any one
position of the vectors, say the ith position, we can define the function fi(x)
to be the ith bit of vector x. Then fi(x) = fi(y) if and only if vectors x and
y agree in the ith position. Then the probability that fi(x) = fi(y) for a randomly chosen i is exactly 1 − h(x, y)/d; i.e., it is the fraction of positions in
which x and y agree.
This situation is almost exactly like the one we encountered for minhashing.
Thus, the family F consisting of the functions {f1, f2, . . . , fd} is a
(d1, d2, 1 − d1/d, 1 − d2/d)-sensitive
family of hash functions, for any d1 < d2. There are only two differences
between this family and the family of minhash functions.
1. While Jaccard distance runs from 0 to 1, the Hamming distance on a
vector space of dimension d runs from 0 to d. It is therefore necessary to
scale the distances by dividing by d, to turn them into probabilities.
2. While there is essentially an unlimited supply of minhash functions, the
size of the family F for Hamming distance is only d.
The first point is of no consequence; it only requires that we divide by d at
appropriate times. The second point is more serious. If d is relatively small,
then we are limited in the number of functions that can be composed using
the AND and OR constructions, thereby limiting how steep we can make the
S-curve be.
3.7.2 Random Hyperplanes and the Cosine Distance
Recall from Section 3.5.4 that the cosine distance between two vectors is the
angle between the vectors. For instance, we see in Fig. 3.13 two vectors x
and y that make an angle θ between them. Note that these vectors may be
in a space of many dimensions, but they always define a plane, and the angle
between them is measured in this plane. Figure 3.13 is a “top-view” of the
plane containing x and y.
Suppose we pick a hyperplane through the origin. This hyperplane intersects
the plane of x and y in a line. Figure 3.13 suggests two possible hyperplanes,
one whose intersection is the dashed line and the other’s intersection is the
dotted line. To pick a random hyperplane, we actually pick the normal vector
to the hyperplane, say v. The hyperplane is then the set of points whose dot
product with v is 0.
First, consider a vector v that is normal to the hyperplane whose projection
is represented by the dashed line in Fig. 3.13; that is, x and y are on different
110 CHAPTER 3. FINDING SIMILAR ITEMS
θ
x
y
Figure 3.13: Two vectors make an angle θ
sides of the hyperplane. Then the dot products v.x and v.y will have different
signs. If we assume, for instance, that v is a vector whose projection onto the
plane of x and y is above the dashed line in Fig. 3.13, then v.x is positive,
while v.y is negative. The normal vector v instead might extend in the opposite
direction, below the dashed line. In that case v.x is negative and v.y is positive,
but the signs are still different.
On the other hand, the randomly chosen vector v could be normal to a
hyperplane like the dotted line in Fig. 3.13. In that case, both v.x and v.y
have the same sign. If the projection of v extends to the right, then both dot
products are positive, while if v extends to the left, then both are negative.
What is the probability that the randomly chosen vector is normal to a
hyperplane that looks like the dashed line rather than the dotted line? All
angles for the line that is the intersection of the random hyperplane and the
plane of x and y are equally likely. Thus, the hyperplane will look like the
dashed line with probability θ/180 and will look like the dotted line otherwise.
Thus, each hash function f in our locality-sensitive family F is built from
a randomly chosen vector vf . Given two vectors x and y, say f(x) = f(y) if
and only if the dot products vf .x and vf .y have the same sign. Then F is a
locality-sensitive family for the cosine distance. The parameters are essentially
the same as for the Jaccard-distance family described in Section 3.6.2, except
the scale of distances is 0–180 rather than 0–1. That is, F is a
(d1, d2,(180 − d1)/180,(180 − d2)/180)-sensitive
family of hash functions. From this basis, we can amplify the family as we wish,
just as for the minhash-based family.
3.7. LSH FAMILIES FOR OTHER DISTANCE MEASURES 111
3.7.3 Sketches
Instead of chosing a random vector from all possible vectors, it turns out to be
sufficiently random if we restrict our choice to vectors whose components are
+1 and −1. The dot product of any vector x with a vector v of +1’s and −1’s
is formed by adding the components of x where v is +1 and then subtracting
the other components of x – those where v is −1.
If we pick a collection of random vectors, say v1, v2, . . . , vn, then we can
apply them to an arbitrary vector x by computing v1.x, v2.x, . . . , vn.x and then
replacing any positive value by +1 and any negative value by −1. The result is
called the sketch of x. You can handle 0’s arbitrarily, e.g., by chosing a result +1
or −1 at random. Since there is only a tiny probability of a zero dot product,
the choice has essentially no effect.
Example 3.22 : Suppose our space consists of 4-dimensional vectors, and we
pick three random vectors: v1 = [+1, −1, +1, +1], v2 = [−1, +1, −1, +1], and
v3 = [+1, +1, −1, −1]. For the vector x = [3, 4, 5, 6], the sketch is [+1, +1, −1].
That is, v1.x = 3−4+5+6 = 10. Since the result is positive, the first component
of the sketch is +1. Similarly, v2.x = 2 and v3.x = −4, so the second component
of the sketch is +1 and the third component is −1.
Consider the vector y = [4, 3, 2, 1]. We can similarly compute its sketch to
be [+1, −1, +1]. Since the sketches for x and y agree in 1/3 of the positions,
we estimate that the angle between them is 120 degrees. That is, a randomly
chosen hyperplane is twice as likely to look like the dashed line in Fig. 3.13 than
like the dotted line.
The above conclusion turns out to be quite wrong. We can calculate the
cosine of the angle between x and y to be x.y, which is
6 × 1 + 5 × 2 + 4 × 3 + 3 × 4 = 40
divided by the magnitudes of the two vectors. These magnitudes are
p
6
2 + 52 + 42 + 32 = 9.274
and √
1
2 + 22 + 32 + 42 = 5.477. Thus, the cosine of the angle between x and
y is 0.7875, and this angle is about 38 degrees. However, if you look at all
16 different vectors v of length 4 that have +1 and −1 as components, you
find that there are only four of these whose dot products with x and y have
a different sign, namely v2, v3, and their complements [+1, −1, +1, −1] and
[−1, −1, +1, +1]. Thus, had we picked all sixteen of these vectors to form a
sketch, the estimate of the angle would have been 180/4 = 45 degrees. ✷
3.7.4 LSH Families for Euclidean Distance
Now, let us turn to the Euclidean distance (Section 3.5.2), and see if we can
develop a locality-sensitive family of hash functions for this distance. We shall
start with a 2-dimensional Euclidean space. Each hash function f in our family
112 CHAPTER 3. FINDING SIMILAR ITEMS
F will be associated with a randomly chosen line in this space. Pick a constant
a and divide the line into segments of length a, as suggested by Fig. 3.14, where
the “random” line has been oriented to be horizontal.
θ
Points at
distance
Bucket
width a
d
Figure 3.14: Two points at distance d ≫ a have a small chance of being hashed
to the same bucket
The segments of the line are the buckets into which function f hashes points.
A point is hashed to the bucket in which its projection onto the line lies. If the
distance d between two points is small compared with a, then there is a good
chance the two points hash to the same bucket, and thus the hash function f
will declare the two points equal. For example, if d = a/2, then there is at least
a 50% chance the two points will fall in the same bucket. In fact, if the angle
θ between the randomly chosen line and the line connecting the points is large,
then there is an even greater chance that the two points will fall in the same
bucket. For instance, if θ is 90 degrees, then the two points are certain to fall
in the same bucket.
However, suppose d is larger than a. In order for there to be any chance of
the two points falling in the same bucket, we need d cos θ ≤ a. The diagram of
Fig. 3.14 suggests why this requirement holds. Note that even if d cos θ ≪ a it
is still not certain that the two points will fall in the same bucket. However,
we can guarantee the following. If d ≥ 2a, then there is no more than a 1/3
chance the two points fall in the same bucket. The reason is that for cos θ to
be less than 1/2, we need to have θ in the range 60 to 90 degrees. If θ is in the
range 0 to 60 degrees, then cos θ is more than 1/2. But since θ is the smaller
angle between two randomly chosen lines in the plane, θ is twice as likely to be
between 0 and 60 as it is to be between 60 and 90.
We conclude that the family F just described forms a (a/2, 2a, 1/2, 1/3)-
sensitive family of hash functions. That is, for distances up to a/2 the probability is at least 1/2 that two points at that distance will fall in the same bucket,
while for distances at least 2a the probability points at that distance will fall in
3.7. LSH FAMILIES FOR OTHER DISTANCE MEASURES 113
the same bucket is at most 1/3. We can amplify this family as we like, just as
for the other examples of locality-sensitive hash functions we have discussed.
3.7.5 More LSH Families for Euclidean Spaces
There is something unsatisfying about the family of hash functions developed
in Section 3.7.4. First, the technique was only described for two-dimensional
Euclidean spaces. What happens if our data is points in a space with many
dimensions? Second, for Jaccard and cosine distances, we were able to develop
locality-sensitive families for any pair of distances d1 and d2 as long as d1 < d2.
In Section 3.7.4 we appear to need the stronger condition d1 < 4d2.
However, we claim that there is a locality-sensitive family of hash functions for any d1 < d2 and for any number of dimensions. The family’s hash
functions still derive from random lines through the space and a bucket size
a that partitions the line. We still hash points by projecting them onto the
line. Given that d1 < d2, we may not know what the probability p1 is that two
points at distance d1 hash to the same bucket, but we can be certain that it
is greater than p2, the probability that two points at distance d2 hash to the
same bucket. The reason is that this probability surely grows as the distance
shrinks. Thus, even if we cannot calculate p1 and p2 easily, we know that there
is a (d1, d2, p1, p2)-sensitive family of hash functions for any d1 < d2 and any
given number of dimensions.
Using the amplification techniques of Section 3.6.3, we can then adjust the
two probabilities to surround any particular value we like, and to be as far apart
as we like. Of course, the further apart we want the probabilities to be, the
larger the number of basic hash functions in F we must use.
3.7.6 Exercises for Section 3.7
Exercise 3.7.1 : Suppose we construct the basic family of six locality-sensitive
functions for vectors of length six. For each pair of the vectors 000000, 110011,
010101, and 011100, which of the six functions makes them candidates?
Exercise 3.7.2 : Let us compute sketches using the following four “random”
vectors:
v1 = [+1, +1, +1, −1] v2 = [+1, +1, −1, +1]
v3 = [+1, −1, +1, +1] v4 = [−1, +1, +1, +1]
Compute the sketches of the following vectors.
(a) [2, 3, 4, 5].
(b) [−2, 3, −4, 5].
(c) [2, −3, 4, −5].
114 CHAPTER 3. FINDING SIMILAR ITEMS
For each pair, what is the estimated angle between them, according to the
sketches? What are the true angles?
Exercise 3.7.3 : Suppose we form sketches by using all sixteen of the vectors
of length 4, whose components are each +1 or −1. Compute the sketches of
the three vectors in Exercise 3.7.2. How do the estimates of the angles between
each pair compare with the true angles?
Exercise 3.7.4 : Suppose we form sketches using the four vectors from Exercise 3.7.2.
! (a) What are the constraints on a, b, c, and d that will cause the sketch of
the vector [a, b, c, d] to be [+1, +1, +1, +1]?
!! (b) Consider two vectors [a, b, c, d] and [e, f, g, h]. What are the conditions on
a, b, . . . , h that will make the sketches of these two vectors be the same?
Exercise 3.7.5 : Suppose we have points in a 3-dimensional Euclidean space:
p1 = (1, 2, 3), p2 = (0, 2, 4), and p3 = (4, 3, 2). Consider the three hash functions
defined by the three axes (to make our calculations very easy). Let buckets be
of length a, with one bucket the interval [0, a) (i.e., the set of points x such that
0 ≤ x < a), the next [a, 2a), the previous one [−a, 0), and so on.
(a) For each of the three lines, assign each of the points to buckets, assuming
a = 1.
(b) Repeat part (a), assuming a = 2.
(c) What are the candidate pairs for the cases a = 1 and a = 2?
! (d) For each pair of points, for what values of a will that pair be a candidate
pair?
3.8 Applications of Locality-Sensitive Hashing
In this section, we shall explore three examples of how LSH is used in practice.
In each case, the techniques we have learned must be modified to meet certain
constraints of the problem. The three subjects we cover are:
1. Entity Resolution: This term refers to matching data records that refer to
the same real-world entity, e.g., the same person. The principal problem
addressed here is that the similarity of records does not match exactly
either the similar-sets or similar-vectors models of similarity on which the
theory is built.
2. Matching Fingerprints: It is possible to represent fingerprints as sets.
However, we shall explore a different family of locality-sensitive hash functions from the one we get by minhashing.
3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 115
3. Matching Newspaper Articles: Here, we consider a different notion of
shingling that focuses attention on the core article in an on-line newspaper’s Web page, ignoring all the extraneous material such as ads and
newspaper-specific material.
3.8.1 Entity Resolution
It is common to have several data sets available, and to know that they refer to
some of the same entities. For example, several different bibliographic sources
provide information about many of the same books or papers. In the general
case, we have records describing entities of some type, such as people or books.
The records may all have the same format, or they may have different formats,
with different kinds of information.
There are many reasons why information about an entity may vary, even if
the field in question is supposed to be the same. For example, names may be
expressed differently in different records because of misspellings, absence of a
middle initial, use of a nickname, and many other reasons. For example, “Bob
S. Jomes” and “Robert Jones Jr.” may or may not be the same person. If
records come from different sources, the fields may differ as well. One source’s
records may have an “age” field, while another does not. The second source
might have a “date of birth” field, or it may have no information at all about
when a person was born.
3.8.2 An Entity-Resolution Example
We shall examine a real example of how LSH was used to deal with an entityresolution problem. Company A was engaged by Company B to solicit customers for B. Company B would pay A a yearly fee, as long as the customer
maintained their subscription. They later quarreled and disagreed over how
many customers A had provided to B. Each had about 1,000,000 records, some
of which described the same people; those were the customers A had provided
to B. The records had different data fields, but unfortunately none of those
fields was “this is a customer that A had provided to B.” Thus, the problem
was to match records from the two sets to see if a pair represented the same
person.
Each record had fields for the name, address, and phone number of the
person. However, the values in these fields could differ for many reasons. Not
only were there the misspellings and other naming differences mentioned in
Section 3.8.1, but there were other opportunities to disagree as well. A customer
might give their home phone to A and their cell phone to B. Or they might
move, and tell B but not A (because they no longer had need for a relationship
with A). Area codes of phones sometimes change.
The strategy for identifying records involved scoring the differences in three
fields: name, address, and phone. To create a score describing the likelihood
that two records, one from A and the other from B, described the same per-
116 CHAPTER 3. FINDING SIMILAR ITEMS
son, 100 points was assigned to each of the three fields, so records with exact
matches in all three fields got a score of 300. However, there were deductions for
mismatches in each of the three fields. As a first approximation, edit-distance
(Section 3.5.5) was used, but the penalty grew quadratically with the distance.
Then, certain publicly available tables were used to reduce the penalty in appropriate situations. For example, “Bill” and “William” were treated as if they
differed in only one letter, even though their edit-distance is 5.
However, it is not feasible to score all one trillion pairs of records. Thus,
a simple LSH was used to focus on likely candidates. Three “hash functions”
were used. The first sent records to the same bucket only if they had identical
names; the second did the same but for identical addresses, and the third did
the same for phone numbers. In practice, there was no hashing; rather the
records were sorted by name, so records with identical names would appear
consecutively and get scored for overall similarity of the name, address, and
phone. Then the records were sorted by address, and those with the same
address were scored. Finally, the records were sorted a third time by phone,
and records with identical phones were scored.
This approach missed a record pair that truly represented the same person
but none of the three fields matched exactly. Since the goal was to prove in
a court of law that the persons were the same, it is unlikely that such a pair
would have been accepted by a judge as sufficiently similar anyway.
3.8.3 Validating Record Matches
What remains is to determine how high a score indicates that two records truly
represent the same individual. In the example at hand, there was an easy
way to make that decision, and the technique can be applied in many similar
situations. It was decided to look at the creation-dates for the records at hand,
and to assume that 90 days was an absolute maximum delay between the time
the service was bought at Company A and registered at B. Thus, a proposed
match between two records that were chosen at random, subject only to the
constraint that the date on the B-record was between 0 and 90 days after the
date on the A-record, would have an average delay of 45 days.
It was found that of the pairs with a perfect 300 score, the average delay was
10 days. If you assume that 300-score pairs are surely correct matches, then you
can look at the pool of pairs with any given score s, and compute the average
delay of those pairs. Suppose that the average delay is x, and the fraction of
true matches among those pairs with score s is f. Then x = 10f + 45(1 − f),
or x = 45 − 35f. Solving for f, we find that the fraction of the pairs with score
s that are truly matches is (45 − x)/35.
The same trick can be used whenever:
1. There is a scoring system used to evaluate the likelihood that two records
represent the same entity, and
3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 117
When Are Record Matches Good Enough?
While every case will be different, it may be of interest to know how the
experiment of Section 3.8.3 turned out on the data of Section 3.8.2. For
scores down to 185, the value of x was very close to 10; i.e., these scores
indicated that the likelihood of the records representing the same person
was essentially 1. Note that a score of 185 in this example represents a
situation where one field is the same (as would have to be the case, or the
records would never even be scored), one field was completely different,
and the third field had a small discrepancy. Moreover, for scores as low as
115, the value of x was noticeably less than 45, meaning that some of these
pairs did represent the same person. Note that a score of 115 represents
a case where one field is the same, but there is only a slight similarity in
the other two fields.
2. There is some field, not used in the scoring, from which we can derive a
measure that differs, on average, for true pairs and false pairs.
For instance, suppose there were a “height” field recorded by both companies
A and B in our running example. We can compute the average difference in
height for pairs of random records, and we can compute the average difference in
height for records that have a perfect score (and thus surely represent the same
entities). For a given score s, we can evaluate the average height difference of the
pairs with that score and estimate the probability of the records representing
the same entity. That is, if h0 is the average height difference for the perfect
matches, h1 is the average height difference for random pairs, and h is the
average height difference for pairs of score s, then the fraction of good pairs
with score s is (h1 − h)/(h1 − h0).
3.8.4 Matching Fingerprints
When fingerprints are matched by computer, the usual representation is not
an image, but a set of locations in which minutiae are located. A minutia,
in the context of fingerprint descriptions, is a place where something unusual
happens, such as two ridges merging or a ridge ending. If we place a grid over a
fingerprint, we can represent the fingerprint by the set of grid squares in which
minutiae are located.
Ideally, before overlaying the grid, fingerprints are normalized for size and
orientation, so that if we took two images of the same finger, we would find
minutiae lying in exactly the same grid squares. We shall not consider here
the best ways to normalize images. Let us assume that some combination of
techniques, including choice of grid size and placing a minutia in several adjacent
grid squares if it lies close to the border of the squares enables us to assume
118 CHAPTER 3. FINDING SIMILAR ITEMS
that grid squares from two images have a significantly higher probability of
agreeing in the presence or absence of a minutia than if they were from images
of different fingers.
Thus, fingerprints can be represented by sets of grid squares – those where
their minutiae are located – and compared like any sets, using the Jaccard similarity or distance. There are two versions of fingerprint comparison, however.
• The many-one problem is the one we typically expect. A fingerprint has
been found on a gun, and we want to compare it with all the fingerprints
in a large database, to see which one matches.
• The many-many version of the problem is to take the entire database, and
see if there are any pairs that represent the same individual.
While the many-many version matches the model that we have been following
for finding similar items, the same technology can be used to speed up the
many-one problem.
3.8.5 A LSH Family for Fingerprint Matching
We could minhash the sets that represent a fingerprint, and use the standard
LSH technique from Section 3.4. However, since the sets are chosen from a
relatively small set of grid points (perhaps 1000), the need to minhash them
into more succinct signatures is not clear. We shall study here another form of
locality-sensitive hashing that works well for data of the type we are discussing.
Suppose for an example that the probability of finding a minutia in a random
grid square of a random fingerprint is 20%. Also, assume that if two fingerprints
come from the same finger, and one has a minutia in a given grid square, then
the probability that the other does too is 80%. We can define a locality-sensitive
family of hash functions as follows. Each function f in this family F is defined
by three grid squares. Function f says “yes” for two fingerprints if both have
minutiae in all three grid squares, and otherwise f says “no.” Put another
way, we may imagine that f sends to a single bucket all fingerprints that have
minutiae in all three of f’s grid points, and sends each other fingerprint to a
bucket of its own. In what follows, we shall refer to the first of these buckets as
“the” bucket for f and ignore the buckets that are required to be singletons.
If we want to solve the many-one problem, we can use many functions from
the family F and precompute their buckets of fingerprints to which they answer
“yes.” Then, given a new fingerprint that we want to match, we determine
which of these buckets it belongs to and compare it with all the fingerprints
found in any of those buckets. To solve the many-many problem, we compute
the buckets for each of the functions and compare all fingerprints in each of the
buckets.
Let us consider how many functions we need to get a reasonable probability
of catching a match, without having to compare the fingerprint on the gun with
each of the millions of fingerprints in the database. First, the probability that
3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 119
two fingerprints from different fingers would be in the bucket for a function f
in F is (0.2)6 = 0.000064. The reason is that they will both go into the bucket
only if they each have a minutia in each of the three grid points associated with
f, and the probability of each of those six independent events is 0.2.
Now, consider the probability that two fingerprints from the same finger
wind up in the bucket for f. The probability that the first fingerprint has
minutiae in each of the three squares belonging to f is (0.2)3 = 0.008. However,
if it does, then the probability is (0.8)3 = 0.512 that the other fingerprint
will as well. Thus, if the fingerprints are from the same finger, there is a
0.008 × 0.512 = 0.004096 probability that they will both be in the bucket of f.
That is not much; it is about one in 200. However, if we use many functions
from F, but not too many, then we can get a good probability of matching
fingerprints from the same finger while not having too many false positives –
fingerprints that must be considered but do not match.
Example 3.23 : For a specific example, let us suppose that we use 1024
functions chosen randomly from F. Next, we shall construct a new family F1 by performing a 1024-way OR on F. Then the probability that F1
will put fingerprints from the same finger together in at least one bucket is
1 − (1 − 0.004096)1024 = 0.985. On the other hand, the probability that
two fingerprints from different fingers will be placed in the same bucket is
(1 − (1 − 0.000064)1024 = 0.063. That is, we get about 1.5% false negatives
and about 6.3% false positives. ✷
The result of Example 3.23 is not the best we can do. While it offers only a
1.5% chance that we shall fail to identify the fingerprint on the gun, it does force
us to look at 6.3% of the entire database. Increasing the number of functions
from F will increase the number of false positives, with only a small benefit
of reducing the number of false negatives below 1.5%. On the other hand, we
can also use the AND construction, and in so doing, we can greatly reduce
the probability of a false positive, while making only a small increase in the
false-negative rate. For instance, we could take 2048 functions from F in two
groups of 1024. Construct the buckets for each of the functions. However, given
a fingerprint P on the gun:
1. Find the buckets from the first group in which P belongs, and take the
union of these buckets.
2. Do the same for the second group.
3. Take the intersection of the two unions.
4. Compare P only with those fingerprints in the intersection.
Note that we still have to take unions and intersections of large sets of fingerprints, but we compare only a small fraction of those. It is the comparison of
120 CHAPTER 3. FINDING SIMILAR ITEMS
fingerprints that takes the bulk of the time; in steps (1) and (2) fingerprints
can be represented by their integer indices in the database.
If we use this scheme, the probability of detecting a matching fingerprint
is (0.985)2 = 0.970; that is, we get about 3% false negatives. However, the
probability of a false positive is (0.063)2 = 0.00397. That is, we only have to
examine about 1/250th of the database.
3.8.6 Similar News Articles
Our last case study concerns the problem of organizing a large repository of
on-line news articles by grouping together Web pages that were derived from
the same basic text. It is common for organizations like The Associated Press
to produce a news item and distribute it to many newspapers. Each newspaper
puts the story in its on-line edition, but surrounds it by information that is
special to that newspaper, such as the name and address of the newspaper,
links to related articles, and links to ads. In addition, it is common for the
newspaper to modify the article, perhaps by leaving off the last few paragraphs
or even deleting text from the middle. As a result, the same news article can
appear quite different at the Web sites of different newspapers.
The problem looks very much like the one that was suggested in Section 3.4:
find documents whose shingles have a high Jaccard similarity. Note that this
problem is different from the problem of finding news articles that tell about the
same events. The latter problem requires other techniques, typically examining
the set of important words in the documents (a concept we discussed briefly
in Section 1.3.1) and clustering them to group together different articles about
the same topic.
However, an interesting variation on the theme of shingling was found to be
more effective for data of the type described. The problem is that shingling as
we described it in Section 3.2 treats all parts of a document equally. However,
we wish to ignore parts of the document, such as ads or the headlines of other
articles to which the newspaper added a link, that are not part of the news
article. It turns out that there is a noticeable difference between text that
appears in prose and text that appears in ads or headlines. Prose has a much
greater frequency of stop words, the very frequent words such as “the” or “and.”
The total number of words that are considered stop words varies with the
application, but it is common to use a list of several hundred of the most
frequent words.
Example 3.24 : A typical ad might say simply “Buy Sudzo.” On the other
hand, a prose version of the same thought that might appear in an article is
“I recommend that you buy Sudzo for your laundry.” In the latter sentence, it
would be normal to treat “I,” “that,” “you,” “for,” and “your” as stop words.
✷
Suppose we define a shingle to be a stop word followed by the next two
words. Then the ad “Buy Sudzo” from Example 3.24 has no shingles and
3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 121
would not be reflected in the representation of the Web page containing that
ad. On the other hand, the sentence from Example 3.24 would be represented
by five shingles: “I recommend that,” “that you buy,” “you buy Sudzo,” “for
your laundry,” and “your laundry x,” where x is whatever word follows that
sentence.
Suppose we have two Web pages, each of which consists of half news text
and half ads or other material that has a low density of stop words. If the news
text is the same but the surrounding material is different, then we would expect
that a large fraction of the shingles of the two pages would be the same. They
might have a Jaccard similarity of 75%. However, if the surrounding material
is the same but the news content is different, then the number of common
shingles would be small, perhaps 25%. If we were to use the conventional
shingling, where shingles are (say) sequences of 10 consecutive characters, we
would expect the two documents to share half their shingles (i.e., a Jaccard
similarity of 1/3), regardless of whether it was the news or the surrounding
material that they shared.
3.8.7 Exercises for Section 3.8
Exercise 3.8.1 : Suppose we are trying to perform entity resolution among
bibliographic references, and we score pairs of references based on the similarities of their titles, list of authors, and place of publication. Suppose also that
all references include a year of publication, and this year is equally likely to be
any of the ten most recent years. Further, suppose that we discover that among
the pairs of references with a perfect score, there is an average difference in the
publication year of 0.1.7 Suppose that the pairs of references with a certain
score s are found to have an average difference in their publication dates of 2.
What is the fraction of pairs with score s that truly represent the same publication? Note: Do not make the mistake of assuming the average difference
in publication date between random pairs is 5 or 5.5. You need to calculate it
exactly, and you have enough information to do so.
Exercise 3.8.2 : Suppose we use the family F of functions described in Section 3.8.5, where there is a 20% chance of a minutia in an grid square, an 80%
chance of a second copy of a fingerprint having a minutia in a grid square where
the first copy does, and each function in F being formed from three grid squares.
In Example 3.23, we constructed family F1 by using the OR construction on
1024 members of F. Suppose we instead used family F2 that is a 2048-way OR
of members of F.
(a) Compute the rates of false positives and false negatives for F2.
(b) How do these rates compare with what we get if we organize the same
2048 functions into a 2-way AND of members of F1, as was discussed at
the end of Section 3.8.5?
7We might expect the average to be 0, but in practice, errors in publication year do occur.
122 CHAPTER 3. FINDING SIMILAR ITEMS
Exercise 3.8.3 : Suppose fingerprints have the same statistics outlined in Exercise 3.8.2, but we use a base family of functions F
′
defined like F, but using
only two randomly chosen grid squares. Construct another set of functions F
′
1
from F
′
by taking the n-way OR of functions from F
′
. What, as a function of
n, are the false positive and false negative rates for F
′
1
?
Exercise 3.8.4 : Suppose we use the functions F1 from Example 3.23, but we
want to solve the many-many problem.
(a) If two fingerprints are from the same finger, what is the probability that
they will not be compared (i.e., what is the false negative rate)?
(b) What fraction of the fingerprints from different fingers will be compared
(i.e., what is the false positive rate)?
! Exercise 3.8.5 : Assume we have the set of functions F as in Exercise 3.8.2,
and we construct a new set of functions F3 by an n-way OR of functions in
F. For what value of n is the sum of the false positive and false negative rates
minimized?
3.9 Methods for High Degrees of Similarity
LSH-based methods appear most effective when the degree of similarity we
accept is relatively low. When we want to find sets that are almost identical,
there are other methods that can be faster. Moreover, these methods are exact,
in that they find every pair of items with the desired degree of similarity. There
are no false negatives, as there can be with LSH.
3.9.1 Finding Identical Items
The extreme case is finding identical items, for example, Web pages that are
identical, character-for-character. It is straightforward to compare two documents and tell whether they are identical, but we still must avoid having to
compare every pair of documents. Our first thought would be to hash documents based on their first few characters, and compare only those documents
that fell into the same bucket. That scheme should work well, unless all the
documents begin with the same characters, such as an HTML header.
Our second thought would be to use a hash function that examines the
entire document. That would work, and if we use enough buckets, it would be
very rare that two documents went into the same bucket, yet were not identical.
The downside of this approach is that we must examine every character of every
document. If we limit our examination to a small number of characters, then
we never have to examine a document that is unique and falls into a bucket of
its own.
A better approach is to pick some fixed random positions for all documents,
and make the hash function depend only on these. This way, we can avoid
3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 123
a problem where there is a common prefix for all or most documents, yet we
need not examine entire documents unless they fall into a bucket with another
document. One problem with selecting fixed positions is that if some documents
are short, they may not have some of the selected positions. However, if we are
looking for highly similar documents, we never need to compare two documents
that differ significantly in their length. We exploit this idea in Section 3.9.3.
3.9.2 Representing Sets as Strings
Now, let us focus on the harder problem of finding, in a large collection of sets,
all pairs that have a high Jaccard similarity, say at least 0.9. We can represent
a set by sorting the elements of the universal set in some fixed order, and
representing any set by listing its elements in this order. The list is essentially
a string of “characters,” where the characters are the elements of the universal
set. These strings are unusual, however, in that:
1. No character appears more than once in a string, and
2. If two characters appear in two different strings, then they appear in the
same order in both strings.
Example 3.25 : Suppose the universal set consists of the 26 lower-case letters,
and we use the normal alphabetical order. Then the set {d, a, b} is represented
by the string abd. ✷
In what follows, we shall assume all strings represent sets in the manner just
described. Thus, we shall talk about the Jaccard similarity of strings, when
strictly speaking we mean the similarity of the sets that the strings represent.
Also, we shall talk of the length of a string, as a surrogate for the number of
elements in the set that the string represents.
Note that the documents discussed in Section 3.9.1 do not exactly match
this model, even though we can see documents as strings. To fit the model,
we would shingle the documents, assign an order to the shingles, and represent
each document by its list of shingles in the selected order.
3.9.3 Length-Based Filtering
The simplest way to exploit the string representation of Section 3.9.2 is to sort
the strings by length. Then, each string s is compared with those strings t that
follow s in the list, but are not too long. Suppose the lower bound on Jaccard
similarity between two strings is J. For any string x, denote its length by Lx.
Note that Ls ≤ Lt. The intersection of the sets represented by s and t cannot
have more than Ls members, while their union has at least Lt members. Thus,
the Jaccard similarity of s and t, which we denote SIM(s, t), is at most Ls/Lt.
That is, in order for s and t to require comparison, it must be that J ≤ Ls/Lt,
or equivalently, Lt ≤ Ls/J.
124 CHAPTER 3. FINDING SIMILAR ITEMS
A Better Ordering for Symbols
Instead of using the obvious order for elements of the universal set, e.g.,
lexicographic order for shingles, we can order symbols rarest first. That
is, determine how many times each element appears in the collection of
sets, and order them by this count, lowest first. The advantage of doing
so is that the symbols in prefixes will tend to be rare. Thus, they will
cause that string to be placed in index buckets that have relatively few
members. Then, when we need to examine a string for possible matches,
we shall find few other strings that are candidates for comparison.
Example 3.26 : Suppose that s is a string of length 9, and we are looking for
strings with at least 0.9 Jaccard similarity. Then we have only to compare s
with strings following it in the length-based sorted order that have length at
most 9/0.9 = 10. That is, we compare s with those strings of length 9 that
follow it in order, and all strings of length 10. We have no need to compare s
with any other string.
Suppose the length of s were 8 instead. Then s would be compared with
following strings of length up to 8/0.9 = 8.89. That is, a string of length 9
would be too long to have a Jaccard similarity of 0.9 with s, so we only have to
compare s with the strings that have length 8 but follow it in the sorted order.
✷
3.9.4 Prefix Indexing
In addition to length, there are several other features of strings that can be
exploited to limit the number of comparisons that must be made to identify
all pairs of similar strings. The simplest of these options is to create an index
for each symbol; recall a symbol of a string is any one of the elements of the
universal set. For each string s, we select a prefix of s consisting of the first p
symbols of s. How large p must be depends on Ls and J, the lower bound on
Jaccard similarity. We add string s to the index for each of its first p symbols.
In effect, the index for each symbol becomes a bucket of strings that must be
compared. We must be certain that any other string t such that SIM(s, t) ≥ J
will have at least one symbol in its prefix that also appears in the prefix of s.
Suppose not; rather SIM(s, t) ≥ J, but t has none of the first p symbols of
s. Then the highest Jaccard similarity that s and t can have occurs when t is
a suffix of s, consisting of everything but the first p symbols of s. The Jaccard
similarity of s and t would then be (Ls − p)/Ls. To be sure that we do not
have to compare s with t, we must be certain that J > (Ls − p)/Ls. That
is, p must be at least ⌊(1 − J)Ls⌋ + 1. Of course we want p to be as small as
possible, so we do not index string s in more buckets than we need to. Thus,
we shall hereafter take p = ⌊(1 − J)Ls⌋ + 1 to be the length of the prefix that
3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 125
gets indexed.
Example 3.27 : Suppose J = 0.9. If Ls = 9, then p = ⌊0.1 × 9⌋ + 1 =
⌊0.9⌋ + 1 = 1. That is, we need to index s under only its first symbol. Any
string t that does not have the first symbol of s in a position such that t is
indexed by that symbol will have Jaccard similarity with s that is less than 0.9.
Suppose s is bcdefghij. Then s is indexed under b only. Suppose t does not
begin with b. There are two cases to consider.
1. If t begins with a, and SIM(s, t) ≥ 0.9, then it can only be that t is
abcdefghij. But if that is the case, t will be indexed under both a and
b. The reason is that Lt = 10, so t will be indexed under the symbols of
its prefix of length ⌊0.1 × 10⌋ + 1 = 2.
2. If t begins with c or a later letter, then the maximum value of SIM(s, t)
occurs when t is cdefghij. But then SIM(s, t) = 8/9 < 0.9.
In general, with J = 0.9, strings of length up to 9 are indexed by their first
symbol, strings of lengths 10–19 are indexed under their first two symbols,
strings of length 20–29 are indexed under their first three symbols, and so on.
✷
We can use the indexing scheme in two ways, depending on whether we
are trying to solve the many-many problem or a many-one problem; recall the
distinction was introduced in Section 3.8.4. For the many-one problem, we
create the index for the entire database. To query for matches to a new set
S, we convert that set to a string s, which we call the probe string. Determine
the length of the prefix that must be considered, that is, ⌊(1 − J)Ls⌋ + 1. For
each symbol appearing in one of the prefix positions of s, we look in the index
bucket for that symbol, and we compare s with all the strings appearing in that
bucket.
If we want to solve the many-many problem, start with an empty database
of strings and indexes. For each set S, we treat S as a new set for the many-one
problem. We convert S to a string s, which we treat as a probe string in the
many-one problem. However, after we examine an index bucket, we also add s
to that bucket, so s will be compared with later strings that could be matches.
3.9.5 Using Position Information
Consider the strings s = acdefghijk and t = bcdefghijk, and assume J = 0.9.
Since both strings are of length 10, they are indexed under their first two
symbols. Thus, s is indexed under a and c, while t is indexed under b and c.
Whichever is added last will find the other in the bucket for c, and they will be
compared. However, since c is the second symbol of both, we know there will
be two symbols, a and b in this case, that are in the union of the two sets but
not in the intersection. Indeed, even though s and t are identical from c to the
126 CHAPTER 3. FINDING SIMILAR ITEMS
end, their intersection is 9 symbols and their union is 11; thus SIM(s, t) = 9/11,
which is less than 0.9.
If we build our index based not only on the symbol, but on the position of
the symbol within the string, we could avoid comparing s and t above. That
is, let our index have a bucket for each pair (x, i), containing the strings that
have symbol x in position i of their prefix. Given a string s, and assuming J is
the minimum desired Jaccard similarity, we look at the prefix of s, that is, the
positions 1 through ⌊(1 − J)Ls⌋ + 1. If the symbol in position i of the prefix is
x, add s to the index bucket for (x, i).
Now consider s as a probe string. With what buckets must it be compared?
We shall visit the symbols of the prefix of s from the left, and we shall take
advantage of the fact that we only need to find a possible matching string t if
none of the previous buckets we have examined for matches held t. That is, we
only need to find a candidate match once. Thus, if we find that the ith symbol
of s is x, then we need look in the bucket (x, j) for certain small values of j.
j
s
t
Symbols definitely
appearing in
only one string
i
Figure 3.15: Strings s and t begin with i − 1 and j − 1 unique symbols, respectively, and then agree beyond that
To compute the upper bound on j, suppose t is a string none of whose first
j −1 symbols matched anything in s, but the ith symbol of s is the same as the
jth symbol of t. The highest value of SIM(s, t) occurs if s and t are identical
beyond their ith and jth symbols, respectively, as suggested by Fig. 3.15. If
that is the case, the size of their intersection is Ls − i + 1, since that is the
number of symbols of s that could possibly be in t. The size of their union is
at least Ls + j − 1. That is, s surely contributes Ls symbols to the union, and
there are also at least j −1 symbols of t that are not in s. The ratio of the sizes
of the intersection and union must be at least J, so we must have:
Ls − i + 1
Ls + j − 1
≥ J
If we isolate j in this inequality, we have j ≤

Ls(1 − J) − i + 1 + J

/J.
Example 3.28 : Consider the string s = acdefghijk with J = 0.9 discussed
at the beginning of this section. Suppose s is now a probe string. We already
established that we need to consider the first two positions; that is, i can be
3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 127
or 2. Suppose i = 1. Then j ≤ (10 × 0.1 − 1 + 1 + 0.9)/0.9. That is, we only
have to compare the symbol a with strings in the bucket for (a, j) if j ≤ 2.11.
Thus, j can be 1 or 2, but nothing higher.
Now suppose i = 2. Then we require j ≤ (10 × 0.1 − 2 + 1 + 0.9)/0.9, Or
j ≤ 1. We conclude that we must look in the buckets for (a, 1), (a, 2), and (c, 1),
but in no other bucket. In comparison, using the buckets of Section 3.9.4, we
would look into the buckets for a and c, which is equivalent to looking to all
buckets (a, j) and (c, j) for any j. ✷
3.9.6 Using Position and Length in Indexes
When we considered the upper limit on j in the previous section, we assumed
that what follows positions i and j were as in Fig. 3.15, where what followed
these positions in strings s and t matched exactly. We do not want to build an
index that involves every symbol in the strings, because that makes the total
work excessive. However, we can add to our index a summary of what follows
the positions being indexed. Doing so expands the number of buckets, but not
beyond reasonable bounds, and yet enables us to eliminate many candidate
matches without comparing entire strings. The idea is to use index buckets
corresponding to a symbol, a position, and the suffix length, that is, the number
of symbols following the position in question.
Example 3.29 : The string s = acdefghijk, with J = 0.9, would be indexed
in the buckets for (a, 1, 9) and (c, 2, 8). That is, the first position of s has symbol
a, and its suffix is of length 9. The second position has symbol c and its suffix
is of length 8. ✷
Figure 3.15 assumes that the suffixes for position i of s and position j of t
have the same length. If not, then we can either get a smaller upper bound on
the size of the intersection of s and t (if t is shorter) or a larger lower bound
on the size of the union (if t is longer). Suppose s has suffix length p and t has
suffix length q.
Case 1: p ≥ q. Here, the maximum size of the intersection is
Ls − i + 1 − (p − q)
Since Ls = i + p, we can write the above expression for the intersection size as
q + 1. The minimum size of the union is Ls + j − 1, as it was when we did not
take suffix length into account. Thus, we require
q + 1
Ls + j − 1
≥ J
whenever p ≥ q.
128 CHAPTER 3. FINDING SIMILAR ITEMS
Case 2: p < q. Here, the maximum size of the intersection is Ls − i + 1, as
when suffix length was not considered. However, the minimum size of the union
is now Ls + j − 1 + q − p. If we again use the relationship Ls = i + p, we can
replace Ls − p by i and get the formula i + j − 1 + q for the size of the union.
If the Jaccard similarity is at least J, then
Ls − i + 1
i + j − 1 + q
≥ J
whenever p < q.
Example 3.30 : Let us again consider the string s = acdefghijk, but to make
the example show some details, let us choose J = 0.8 instead of 0.9. We know
that Ls = 10. Since ⌊(1 − J)Ls⌋ + 1 = 3, we must consider prefix positions
i = 1, 2, and 3 in what follows. As before, let p be the suffix length of s and q
the suffix length of t.
First, consider the case p ≥ q. The additional constraint we have on q and
j is (q + 1)/(9 + j) ≥ 0.8. We can enumerate the pairs of values of j and q for
each i between 1 and 3, as follows.
i = 1: Here, p = 9, so q ≤ 9. Let us consider the possible values of q:
q = 9: We must have 10/(9 + j) ≥ 0.8. Thus, we can have j = 1, j = 2,
or j = 3. Note that for j = 4, 10/13 > 0.8.
q = 8: We must have 9/(9 + j) ≥ 0.8. Thus, we can have j = 1 or j = 2.
For j = 3, 9/12 > 0.8.
q = 7: We must have 8/(9 + j) ≥ 0.8. Only j = 1 satisfies this inequality.
q = 6: There are no possible values of j, since 7/(9 + j) > 0.8 for every
positive integer j. The same holds for every smaller value of q.
i = 2: Here, p = 8, so we require q ≤ 8. Since the constraint (q+1)/(9+j) ≥ 0.8
does not depend on i,
8 we can use the analysis from the above case, but
exclude the case q = 9. Thus, the only possible values of j and q when
i = 2 are
1. q = 8; j = 1.
2. q = 8; j = 2.
3. q = 7; j = 1.
i = 3: Now, p = 7 and the constraints are q ≤ 7 and (q + 1)/(9 + j) ≥ 0.8. The
only option is q = 7 and j = 1.
Next, we must consider the case p < q. The additional constraint is
11 − i
i + j + q − 1
≥ 0.8
Again, consider each possible value of i.
8Note that i does influence the value of p, and through p, puts a limit on q.
3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 129
i = 1: Then p = 9, so we require q ≥ 10 and 10/(q + j) ≥ 0.8. The possible
values of q and j are
1. q = 10; j = 1.
2. q = 10; j = 2.
3. q = 11; j = 1.
i = 2: Now, p = 8, so we require q ≥ 9 and 9/(q + j + 1) ≥ 0.8. Since j must
be a positive integer, the only solution is q = 9 and j = 1, a possibility
that we already knew about.
i = 3: Here, p = 7, so we require q ≥ 8 and 8/(q + j + 2) ≥ 0.8. There are no
solutions.
q j = 1 j = 2 j = 3
7 x
8 x x
i = 1 9 x x x
10 x x
11 x
7 x
i = 2 8 x x
9 x
i = 3 7 x
Figure 3.16: The buckets that must be examined to find possible matches for
the string s = acdefghijk with J = 0.8 are marked with an x
When we accumulate the possible combinations of i, j, and q, we see that
the set of index buckets in which we must look forms a pyramid. Figure 3.16
shows the buckets in which we must search. That is, we must look in those
buckets (x, j, q) such that the ith symbol of the string s is x, j is the position
associated with the bucket and q the suffix length. ✷
3.9.7 Exercises for Section 3.9
Exercise 3.9.1 : Suppose our universal set is the lower-case letters, and the
order of elements is taken to be the vowels, in alphabetic order, followed by the
consonants in reverse alphabetic order. Represent the following sets as strings.
a {q, w, e, r, t, y}.
(b) {a, s, d, f, g, h, j, u, i}.
130 CHAPTER 3. FINDING SIMILAR ITEMS
Exercise 3.9.2 : Suppose we filter candidate pairs based only on length, as in
Section 3.9.3. If s is a string of length 20, with what strings is s compared when
J, the lower bound on Jaccard similarity has the following values: (a) J = 0.85
(b) J = 0.95 (c) J = 0.98?
Exercise 3.9.3 : Suppose we have a string s of length 15, and we wish to index
its prefix as in Section 3.9.4.
(a) How many positions are in the prefix if J = 0.85?
(b) How many positions are in the prefix if J = 0.95?
! (c) For what range of values of J will s be indexed under its first four symbols,
but no more?
Exercise 3.9.4 : Suppose s is a string of length 12. With what symbol-position
pairs will s be compared with if we use the indexing approach of Section 3.9.5,
and (a) J = 0.75 (b) J = 0.95?
! Exercise 3.9.5 : Suppose we use position information in our index, as in Section 3.9.5. Strings s and t are both chosen at random from a universal set of
100 elements. Assume J = 0.9. What is the probability that s and t will be
compared if
(a) s and t are both of length 9.
(b) s and t are both of length 10.
Exercise 3.9.6 : Suppose we use indexes based on both position and suffix
length, as in Section 3.9.6. If s is a string of length 20, with what symbolposition-length triples will s be compared with, if (a) J = 0.8 (b) J = 0.9?
3.10 Summary of Chapter 3
✦ Jaccard Similarity: The Jaccard similarity of sets is the ratio of the size
of the intersection of the sets to the size of the union. This measure of
similarity is suitable for many applications, including textual similarity of
documents and similarity of buying habits of customers.
✦ Shingling: A k-shingle is any k characters that appear consecutively in
a document. If we represent a document by its set of k-shingles, then
the Jaccard similarity of the shingle sets measures the textual similarity
of documents. Sometimes, it is useful to hash shingles to bit strings of
shorter length, and use sets of hash values to represent documents.
✦ Minhashing: A minhash function on sets is based on a permutation of the
universal set. Given any such permutation, the minhash value for a set is
that element of the set that appears first in the permuted order.
3.10. SUMMARY OF CHAPTER 3 131
✦ Minhash Signatures: We may represent sets by picking some list of permutations and computing for each set its minhash signature, which is the
sequence of minhash values obtained by applying each permutation on the
list to that set. Given two sets, the expected fraction of the permutations
that will yield the same minhash value is exactly the Jaccard similarity
of the sets.
✦ Efficient Minhashing: Since it is not really possible to generate random
permutations, it is normal to simulate a permutation by picking a random
hash function and taking the minhash value for a set to be the least hash
value of any of the set’s members. An additional efficiency can be had
by restricting the search for the smallest minhash value to only a small
subset of the universal set.
✦ Locality-Sensitive Hashing for Signatures: This technique allows us to
avoid computing the similarity of every pair of sets or their minhash signatures. If we are given signatures for the sets, we may divide them into
bands, and only measure the similarity of a pair of sets if they are identical in at least one band. By choosing the size of bands appropriately, we
can eliminate from consideration most of the pairs that do not meet our
threshold of similarity.
✦ Distance Measures: A distance measure is a function on pairs of points in
a space that satisfy certain axioms. The distance between two points is 0 if
the points are the same, but greater than 0 if the points are different. The
distance is symmetric; it does not matter in which order we consider the
two points. A distance measure must satisfy the triangle inequality: the
distance between two points is never more than the sum of the distances
between those points and some third point.
✦ Euclidean Distance: The most common notion of distance is the Euclidean
distance in an n-dimensional space. This distance, sometimes called the
L2-norm, is the square root of the sum of the squares of the differences
between the points in each dimension. Another distance suitable for Euclidean spaces, called Manhattan distance or the L1-norm is the sum of
the magnitudes of the differences between the points in each dimension.
✦ Jaccard Distance: One minus the Jaccard similarity is a distance measure,
called the Jaccard distance.
✦ Cosine Distance: The angle between vectors in a vector space is the cosine
distance measure. We can compute the cosine of that angle by taking the
dot product of the vectors and dividing by the lengths of the vectors.
✦ Edit Distance: This distance measure applies to a space of strings, and
is the number of insertions and/or deletions needed to convert one string
into the other. The edit distance can also be computed as the sum of
132 CHAPTER 3. FINDING SIMILAR ITEMS
the lengths of the strings minus twice the length of the longest common
subsequence of the strings.
✦ Hamming Distance: This distance measure applies to a space of vectors.
The Hamming distance between two vectors is the number of positions in
which the vectors differ.
✦ Generalized Locality-Sensitive Hashing: We may start with any collection
of functions, such as the minhash functions, that can render a decision
as to whether or not a pair of items should be candidates for similarity
checking. The only constraint on these functions is that they provide a
lower bound on the probability of saying “yes” if the distance (according
to some distance measure) is below a given limit, and an upper bound on
the probability of saying “yes” if the distance is above another given limit.
We can then increase the probability of saying “yes” for nearby items and
at the same time decrease the probability of saying “yes” for distant items
to as great an extent as we wish, by applying an AND construction and
an OR construction.
✦ Random Hyperplanes and LSH for Cosine Distance: We can get a set of
basis functions to start a generalized LSH for the cosine distance measure
by identifying each function with a list of randomly chosen vectors. We
apply a function to a given vector v by taking the dot product of v with
each vector on the list. The result is a sketch consisting of the signs (+1 or
−1) of the dot products. The fraction of positions in which the sketches of
two vectors agree, multiplied by 180, is an estimate of the angle between
the two vectors.
✦ LSH For Euclidean Distance: A set of basis functions to start LSH for
Euclidean distance can be obtained by choosing random lines and projecting points onto those lines. Each line is broken into fixed-length intervals,
and the function answers “yes” to a pair of points that fall into the same
interval.
✦ High-Similarity Detection by String Comparison: An alternative approach
to finding similar items, when the threshold of Jaccard similarity is close to
1, avoids using minhashing and LSH. Rather, the universal set is ordered,
and sets are represented by strings, consisting their elements in order.
The simplest way to avoid comparing all pairs of sets or their strings is to
note that highly similar sets will have strings of approximately the same
length. If we sort the strings, we can compare each string with only a
small number of the immediately following strings.
✦ Character Indexes: If we represent sets by strings, and the similarity
threshold is close to 1, we can index all strings by their first few characters.
The prefix whose characters must be indexed is approximately the length
of the string times the maximum Jaccard distance (1 minus the minimum
Jaccard similarity).
3.11. REFERENCES FOR CHAPTER 3 133
✦ Position Indexes: We can index strings not only on the characters in
their prefixes, but on the position of that character within the prefix. We
reduce the number of pairs of strings that must be compared, because
if two strings share a character that is not in the first position in both
strings, then we know that either there are some preceding characters that
are in the union but not the intersection, or there is an earlier symbol that
appears in both strings.
✦ Suffix Indexes: We can also index strings based not only on the characters
in their prefixes and the positions of those characters, but on the length
of the character’s suffix – the number of positions that follow it in the
string. This structure further reduces the number of pairs that must be
compared, because a common symbol with different suffix lengths implies
additional characters that must be in the union but not in the intersection.
3.11 References for Chapter 3
The technique we called shingling is attributed to [11]. The use in the manner
we discussed here is from [2].
Minhashing comes from [3]. The improvement that avoids looking at all
elements is from [10].
The original works on locality-sensitive hashing were [9] and [7]. [1] is a
useful summary of ideas in this field.
[4] introduces the idea of using random-hyperplanes to summarize items in
a way that respects the cosine distance. [8] suggests that random hyperplanes
plus LSH can be more accurate at detecting similar documents than minhashing
plus LSH.
Techniques for summarizing points in a Euclidean space are covered in [6].
[12] presented the shingling technique based on stop words.
The length and prefix-based indexing schemes for high-similarity matching
comes from [5]. The technique involving suffix length is from [13].
1. A. Andoni and P. Indyk, “Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions,” Comm. ACM 51:1, pp. 117–
122, 2008.
2. A.Z. Broder, “On the resemblance and containment of documents,” Proc.
Compression and Complexity of Sequences, pp. 21–29, Positano Italy,
1997.
3. A.Z. Broder, M. Charikar, A.M. Frieze, and M. Mitzenmacher, “Min-wise
independent permutations,” ACM Symposium on Theory of Computing,
pp. 327–336, 1998.
4. M.S. Charikar, “Similarity estimation techniques from rounding algorithms,” ACM Symposium on Theory of Computing, pp. 380–388, 2002.
134 CHAPTER 3. FINDING SIMILAR ITEMS
5. S. Chaudhuri, V. Ganti, and R. Kaushik, “A primitive operator for similarity joins in data cleaning,” Proc. Intl. Conf. on Data Engineering,
2006.
6. M. Datar, N. Immorlica, P. Indyk, and V.S. Mirrokni, “Locality-sensitive
hashing scheme based on p-stable distributions,” Symposium on Computational Geometry pp. 253–262, 2004.
7. A. Gionis, P. Indyk, and R. Motwani, “Similarity search in high dimensions via hashing,” Proc. Intl. Conf. on Very Large Databases, pp. 518–
529, 1999.
8. M. Henzinger, “Finding near-duplicate web pages: a large-scale evaluation
of algorithms,” Proc. 29th SIGIR Conf., pp. 284–291, 2006.
9. P. Indyk and R. Motwani. “Approximate nearest neighbor: towards removing the curse of dimensionality,” ACM Symposium on Theory of Computing, pp. 604–613, 1998.
10. P. Li, A.B. Owen, and C.H. Zhang. “One permutation hashing,” Conf.
on Neural Information Processing Systems 2012, pp. 3122–3130.
11. U. Manber, “Finding similar files in a large file system,” Proc. USENIX
Conference, pp. 1–10, 1994.
12. M. Theobald, J. Siddharth, and A. Paepcke, “SpotSigs: robust and efficient near duplicate detection in large web collections,” 31st Annual ACM
SIGIR Conference, July, 2008, Singapore.
13. C. Xiao, W. Wang, X. Lin, and J.X. Yu, “Efficient similarity joins for
near duplicate detection,” Proc. WWW Conference, pp. 131-140, 2008.

72
Chapter 3
Finding Similar Items
A fundamental data-mining problem is to examine data for “similar” items. We
shall take up applications in Section 3.1, but an example would be looking at a
collection of Web pages and finding near-duplicate pages. These pages could be
plagiarisms, for example, or they could be mirrors that have almost the same
content but differ in information about the host and about other mirrors.
The naive approach to finding pairs of similar items requires us to look at every pair of items. When we are dealing with a large dataset, looking at all pairs
of items may be prohibitive, even given an abundance of hardware resources.
For example, even a million items gives us half a trillion pairs to examine, and
a million items is considered a “small” dataset by today’s standards.
It is therefore a pleasant surprise to learn of a family of techniques called
locality-sensitive hashing, or LSH, that allows us to focus on pairs that are likely
to be similar, without having to look at all pairs. Thus, it is possible that we
can avoid the quadratic growth in computation time that is required by the
naive algorithm. There is usually a downside to locality-sensitive hashing, due
to the presence of false negatives, that is, pairs of items that are similar, yet
are not included in the set of pairs that we examine, but by careful tuning we
can reduce the fraction of false negatives by increasing the number of pairs we
consider.
The general idea behind LSH is that we hash items using many different
hash functions. These hash functions are not the conventional sort of hash
functions. Rather, they are carefully designed to have the property that pairs
are much more likely to wind up in the same bucket of a hash function if the
items are similar than if they are not similar. We then can examine only the
candidate pairs, which are pairs of items that wind up in the same bucket for
at least one of the hash functions.
We begin our discussion of LSH with an examination of the problem of finding similar documents – those that share a lot of common text. We first show
how to convert documents into sets (Section 3.2) in a way that lets us view
textual similarity of documents as sets having a large overlap. More precisely,
73
74 CHAPTER 3. FINDING SIMILAR ITEMS
we measure the similarity of sets by their Jaccard similarity, the ratio of the
sizes of their intersection and union. A second key trick we need is minhashing
(Section 3.3), which is a way to convert large sets into much smaller representations, called signatures, that still enable us to estimate closely the Jaccard
similarity of the represented sets. Finally, in Section 3.4 we see how to apply
the bucketing idea inherent in LSH to the signatures.
In Section 3.5 we begin our study of how to apply LSH to items other than
sets. We consider the general notion of a distance measure that tells to what
degree items are similar. Then, in Section 3.6 we consider the general idea of
locality-sensitive hashing, and in Section 3.7 we see how to do LSH for some data
types other than sets. Then, Section 3.8 examines in detail several applications
of the LSH idea. Finally, we consider in Section 3.9 some techniques for finding
similar sets that can be more efficient than LSH when the degree of similarity
we want is very high.
3.1 Applications of Set Similarity
We shall focus initially on a particular notion of “similarity”: the similarity of
sets by looking at the relative size of their intersection. This notion of similarity
is called Jaccard similarity, which is introduced in Section 3.1.1. We then
examine some of the uses of finding similar sets. These include finding textually
similar documents and collaborative filtering by finding similar customers and
similar products. In order to turn the problem of textual similarity of documents
into one of set intersection, we use the technique called shingling, which is the
subject of Section 3.2.
3.1.1 Jaccard Similarity of Sets
The Jaccard similarity of sets S and T is |S ∩ T |/|S ∪ T |, that is, the ratio
of the size of the intersection of S and T to the size of their union. We shall
denote the Jaccard similarity of S and T by SIM(S, T ).
Example 3.1 : In Fig. 3.1 we see two sets S and T . There are three elements
in their intersection and a total of eight elements that appear in S or T or both.
Thus, SIM(S, T ) = 3/8. ✷
3.1.2 Similarity of Documents
An important class of problems that Jaccard similarity addresses well is that
of finding textually similar documents in a large corpus such as the Web or a
collection of news articles. We should understand that the aspect of similarity
we are looking at here is character-level similarity, not “similar meaning,” which
requires us to examine the words in the documents and their uses. That problem
is also interesting but is addressed by other techniques, which we hinted at in
3.1. APPLICATIONS OF SET SIMILARITY 75
































T
S
Figure 3.1: Two sets with Jaccard similarity 3/8
Section 1.3.1. However, textual similarity also has important uses. Many of
these involve finding duplicates or near duplicates. First, let us observe that
testing whether two documents are exact duplicates is easy; just compare the
two documents character-by-character, and if they ever differ then they are not
the same. However, in many applications, the documents are not identical, yet
they share large portions of their text. Here are some examples:
Plagiarism
Finding plagiarized documents tests our ability to find textual similarity. The
plagiarizer may extract only some parts of a document for his own. He may
alter a few words and may alter the order in which sentences of the original
appear. Yet the resulting document may still contain much of the original. No
simple process of comparing documents character by character will detect a
sophisticated plagiarism.
Mirror Pages
It is common for important or popular Web sites to be duplicated at a number
of hosts, in order to share the load. The pages of these mirror sites will be
quite similar, but are rarely identical. For instance, they might each contain
information associated with their particular host, and they might each have
links to the other mirror sites but not to themselves. A related phenomenon is
the reuse of Web pages from one academic class to another. These pages might
include class notes, assignments, and lecture slides. Similar pages might change
the name of the course, year, and make small changes from year to year. It
is important to be able to detect similar pages of these kinds, because search
engines produce better results if they avoid showing two pages that are nearly
identical within the first
76 CHAPTER 3. FINDING SIMILAR ITEMS
Articles from the Same Source
It is common for one reporter to write a news article that gets distributed,
say through the Associated Press, to many newspapers, which then publish
the article on their Web sites. Each newspaper changes the article somewhat.
They may cut out paragraphs, or even add material of their own. They most
likely will surround the article by their own logo, ads, and links to other articles
at their site. However, the core of each newspaper’s page will be the original
article. News aggregators, such as Google News, try to find all versions of such
an article, in order to show only one, and that task requires finding when two
Web pages are textually similar, although not identical.1
3.1.3 Collaborative Filtering as a Similar-Sets Problem
Another class of applications where similarity of sets is very important is called
collaborative filtering, a process whereby we recommend to users items that were
liked by other users who have exhibited similar tastes. We shall investigate
collaborative filtering in detail in Section 9.3, but for the moment let us see
some common examples.
On-Line Purchases
Amazon.com has millions of customers and sells millions of items. Its database
records which items have been bought by which customers. We can say two customers are similar if their sets of purchased items have a high Jaccard similarity.
Likewise, two items that have sets of purchasers with high Jaccard similarity
will be deemed similar. Note that, while we might expect mirror sites to have
Jaccard similarity above 90%, it is unlikely that any two customers have Jaccard similarity that high (unless they have purchased only one item). Even a
Jaccard similarity like 20% might be unusual enough to identify customers with
similar tastes. The same observation holds for items; Jaccard similarities need
not be very high to be significant.
Collaborative filtering requires several tools, in addition to finding similar
customers or items, as we discuss in Chapter 9. For example, two Amazon
customers who like science-fiction might each buy many science-fiction books,
but only a few of these will be in common. However, by combining similarityfinding with clustering (Chapter 7), we might be able to discover that sciencefiction books are mutually similar and put them in one group. Then, we can
get a more powerful notion of customer-similarity by asking whether they made
purchases within many of the same groups.
1News aggregation also involves finding articles that are about the same topic, even though
not textually similar. This problem too can yield to a similarity search, but it requires
techniques other than Jaccard similarity of sets.
3.1. APPLICATIONS OF SET SIMILARITY 77
Movie Ratings
Netflix records which movies each of its customers rented, and also the ratings
assigned to those movies by the customers. We can regard movies as similar
if they were rented or rated highly by many of the same customers, and see
customers as similar if they rented or rated highly many of the same movies.
The same observations that we made for Amazon above apply in this situation:
similarities need not be high to be significant, and clustering movies by genre
will make things easier.
When our data consists of ratings rather than binary decisions (bought/did
not buy or liked/disliked), we cannot rely simply on sets as representations of
customers or items. Some options are:
1. Ignore low-rated customer/movie pairs; that is, treat these events as if
the customer never watched the movie.
2. When comparing customers, imagine two set elements for each movie,
“liked” and “hated.” If a customer rated a movie highly, put “liked” for
that movie in the customer’s set. If they gave a low rating to a movie, put
“hated” for that movie in their set. Then, we can look for high Jaccard
similarity among these sets. We can use a similar trick when comparing
movies.
3. If ratings are 1-to-5-stars, put a movie in a customer’s set n times if
they rated the movie n-stars. Then, use Jaccard similarity for bags when
measuring the similarity of customers. The Jaccard similarity for bags
B and C is defined by counting an element n times in the intersection if
n is the minimum of the number of times the element appears in B and
C. In the union, we count the element the sum of the number of times it
appears in B and in C.
2
Example 3.2 : The bag-similarity of bags {a, a, a, b} and {a, a, b, b, c} is 1/3.
The intersection counts a twice and b once, so its size is 3. The size of the
union of two bags is always the sum of the sizes of the two bags, or 9 in this
case. Since the highest possible Jaccard similarity for bags is 1/2, the score
of 1/3 indicates the two bags are quite similar, as should be apparent from an
examination of their contents. ✷
2Although the union for bags is normally (e.g., in the SQL standard) defined to have the
sum of the number of copies in each of the two bags, this definition causes some inconsistency
with the Jaccard similarity for sets. Under this definition of bag union, the maximum Jaccard
similarity is 1/2, not 1, since the union of a set with itself has twice as many elements as the
intersection of the same set with itself. If we prefer to have the Jaccard similarity of a set
with itself be 1, we can redefine the union of bags to have each element appear the maximum
number of times it appears in either of the two bags. This change also gives a reasonable
measure of bag similarity.
78 CHAPTER 3. FINDING SIMILAR ITEMS
3.1.4 Exercises for Section 3.1
Exercise 3.1.1 : Compute the Jaccard similarities of each pair of the following
three sets: {1, 2, 3, 4}, {2, 3, 5, 7}, and {2, 4, 6}.
Exercise 3.1.2 : Compute the Jaccard bag similarity of each pair of the following three bags: {1, 1, 1, 2}, {1, 1, 2, 2, 3}, and {1, 2, 3, 4}.
!! Exercise 3.1.3 : Suppose we have a universal set U of n elements, and we
choose two subsets S and T at random, each with m of the n elements. What
is the expected value of the Jaccard similarity of S and T ?
3.2 Shingling of Documents
The most effective way to represent documents as sets, for the purpose of identifying lexically similar documents is to construct from the document the set
of short strings that appear within it. If we do so, then documents that share
pieces as short as sentences or even phrases will have many common elements
in their sets, even if those sentences appear in different orders in the two documents. In this section, we introduce the simplest and most common approach,
shingling, as well as an interesting variation.
3.2.1 k-Shingles
A document is a string of characters. Define a k-shingle for a document to be
any substring of length k found within the document. Then, we may associate
with each document the set of k-shingles that appear one or more times within
that document.
Example 3.3 : Suppose our document D is the string abcdabd, and we pick
k = 2. Then the set of 2-shingles for D is {ab, bc, cd, da, bd}.
Note that the substring ab appears twice within D, but appears only once
as a shingle. A variation of shingling produces a bag, rather than a set, so each
shingle would appear in the result as many times as it appears in the document.
However, we shall not use bags of shingles here. ✷
There are several options regarding how white space (blank, tab, newline,
etc.) is treated. It probably makes sense to replace any sequence of one or more
white-space characters by a single blank. That way, we distinguish shingles that
cover two or more words from those that do not.
Example 3.4 : If we use k = 9, but eliminate whitespace altogether, then we
would see some lexical similarity in the sentences “The plane was ready for
touch down”. and “The quarterback scored a touchdown”. However, if we
retain the blanks, then the first has shingles touch dow and ouch down, while
the second has touchdown. If we eliminated the blanks, then both would have
touchdown. ✷
3.2. SHINGLING OF DOCUMENTS 79
3.2.2 Choosing the Shingle Size
We can pick k to be any constant we like. However, if we pick k too small, then
we would expect most sequences of k characters to appear in most documents.
If so, then we could have documents whose shingle-sets had high Jaccard similarity, yet the documents had none of the same sentences or even phrases. As
an extreme example, if we use k = 1, most Web pages will have most of the
common characters and few other characters, so almost all Web pages will have
high similarity.
How large k should be depends on how long typical documents are and how
large the set of typical characters is. The important thing to remember is:
• k should be picked large enough that the probability of any given shingle
appearing in any given document is low.
Thus, if our corpus of documents is emails, picking k = 5 should be fine.
To see why, suppose that only letters and a general white-space character appear in emails (although in practice, most of the printable ASCII characters
can be expected to appear occasionally). If so, then there would be 275 =
14,348,907 possible shingles. Since the typical email is much smaller than 14
million characters long, we would expect k = 5 to work well, and indeed it does.
However, the calculation is a bit more subtle. Surely, more than 27 characters appear in emails, However, all characters do not appear with equal probability. Common letters and blanks dominate, while ”z” and other letters that
have high point-value in Scrabble are rare. Thus, even short emails will have
many 5-shingles consisting of common letters, and the chances of unrelated
emails sharing these common shingles is greater than would be implied by the
calculation in the paragraph above. A good rule of thumb is to imagine that
there are only 20 characters and estimate the number of k-shingles as 20k
. For
large documents, such as research articles, choice k = 9 is considered safe.
3.2.3 Hashing Shingles
Instead of using substrings directly as shingles, we can pick a hash function
that maps strings of length k to some number of buckets and treat the resulting
bucket number as the shingle. The set representing a document is then the
set of integers that are bucket numbers of one or more k-shingles that appear
in the document. For instance, we could construct the set of 9-shingles for a
document and then map each of those 9-shingles to a bucket number in the
range 0 to 232 − 1. Thus, each shingle is represented by four bytes instead
of nine. Not only has the data been compacted, but we can now manipulate
(hashed) shingles by single-word machine operations.
Notice that we can differentiate documents better if we use 9-shingles and
hash them down to four bytes than to use 4-shingles, even though the space used
to represent a shingle is the same. The reason was touched upon in Section 3.2.2.
If we use 4-shingles, most sequences of four bytes are unlikely or impossible to
80 CHAPTER 3. FINDING SIMILAR ITEMS
find in typical documents. Thus, the effective number of different shingles is
much less than 232 −1. If, as in Section 3.2.2, we assume only 20 characters are
frequent in English text, then the number of different 4-shingles that are likely
to occur is only (20)4 = 160,000. However, if we use 9-shingles, there are many
more than 232 likely shingles. When we hash them down to four bytes, we can
expect almost any sequence of four bytes to be possible, as was discussed in
Section 1.3.2.
3.2.4 Shingles Built from Words
An alternative form of shingle has proved effective for the problem of identifying
similar news articles, mentioned in Section 3.1.2. The exploitable distinction for
this problem is that the news articles are written in a rather different style than
are other elements that typically appear on the page with the article. News
articles, and most prose, have a lot of stop words (see Section 1.3.1), the most
common words such as “and,” “you,” “to,” and so on. In many applications,
we want to ignore stop words, since they don’t tell us anything useful about
the article, such as its topic.
However, for the problem of finding similar news articles, it was found that
defining a shingle to be a stop word followed by the next two words, regardless
of whether or not they were stop words, formed a useful set of shingles. The
advantage of this approach is that the news article would then contribute more
shingles to the set representing the Web page than would the surrounding elements. Recall that the goal of the exercise is to find pages that had the same
articles, regardless of the surrounding elements. By biasing the set of shingles
in favor of the article, pages with the same article and different surrounding
material have higher Jaccard similarity than pages with the same surrounding
material but with a different article.
Example 3.5 : An ad might have the simple text “Buy Sudzo.” However, a
news article with the same idea might read something like “A spokesperson
for the Sudzo Corporation revealed today that studies have shown it is
good for people to buy Sudzo products.” Here, we have italicized all the
likely stop words, although there is no set number of the most frequent words
that should be considered stop words. The first three shingles made from a
stop word and the next two following are:
A spokesperson for
for the Sudzo
the Sudzo Corporation
There are nine shingles from the sentence, but none from the “ad.” ✷
3.2.5 Exercises for Section 3.2
Exercise 3.2.1 : What are the first ten 3-shingles in the first sentence of Section 3.2?
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 81
Exercise 3.2.2 : If we use the stop-word-based shingles of Section 3.2.4, and
we take the stop words to be all the words of three or fewer letters, then what
are the shingles in the first sentence of Section 3.2?
Exercise 3.2.3 : What is the largest number of k-shingles a document of n
bytes can have? You may assume that the size of the alphabet is large enough
that the number of possible strings of length k is at least n.
3.3 Similarity-Preserving Summaries of Sets
Sets of shingles are large. Even if we hash them to four bytes each, the space
needed to store a set is still roughly four times the space taken by the document.
If we have millions of documents, it may well not be possible to store all the
shingle-sets in main memory.3
Our goal in this section is to replace large sets by much smaller representations called “signatures.” The important property we need for signatures is
that we can compare the signatures of two sets and estimate the Jaccard similarity of the underlying sets from the signatures alone. It is not possible that
the signatures give the exact similarity of the sets they represent, but the estimates they provide are close, and the larger the signatures the more accurate
the estimates. For example, if we replace the 200,000-byte hashed-shingle sets
that derive from 50,000-byte documents by signatures of 1000 bytes, we can
usually get within a few percent.
3.3.1 Matrix Representation of Sets
Before explaining how it is possible to construct small signatures from large
sets, it is helpful to visualize a collection of sets as their characteristic matrix.
The columns of the matrix correspond to the sets, and the rows correspond to
elements of the universal set from which elements of the sets are drawn. There
is a 1 in row r and column c if the element for row r is a member of the set for
column c. Otherwise the value in position (r, c) is 0.
Element S1 S2 S3 S4
a 1 0 0 1
b 0 0 1 0
c 0 1 0 1
d 1 0 1 1
e 0 0 1 0
Figure 3.2: A matrix representing four sets
3There is another serious concern: even if the sets fit in main memory, the number of pairs
may be too great for us to evaluate the similarity of each pair. We take up the solution to
this problem in Section 3.4.
82 CHAPTER 3. FINDING SIMILAR ITEMS
Example 3.6 : In Fig. 3.2 is an example of a matrix representing sets chosen
from the universal set {a, b, c, d, e}. Here, S1 = {a, d}, S2 = {c}, S3 = {b, d, e},
and S4 = {a, c, d}. The top row and leftmost columns are not part of the matrix,
but are present only to remind us what the rows and columns represent. ✷
It is important to remember that the characteristic matrix is unlikely to be
the way the data is stored, but it is useful as a way to visualize the data. For one
reason not to store data as a matrix, these matrices are almost always sparse
(they have many more 0’s than 1’s) in practice. It saves space to represent a
sparse matrix of 0’s and 1’s by the positions in which the 1’s appear. For another
reason, the data is usually stored in some other format for other purposes.
As an example, if rows are products, and columns are customers, represented
by the set of products they bought, then this data would really appear in a
database table of purchases. A tuple in this table would list the item, the
purchaser, and probably other details about the purchase, such as the date and
the credit card used.
3.3.2 Minhashing
The signatures we desire to construct for sets are composed of the results of a
large number of calculations, say several hundred, each of which is a “minhash”
of the characteristic matrix. In this section, we shall learn how a minhash is
computed in principle, and in later sections we shall see how a good approximation to the minhash is computed in practice.
To minhash a set represented by a column of the characteristic matrix, pick
a permutation of the rows. The minhash value of any column is the number of
the first row, in the permuted order, in which the column has a 1.
Example 3.7 : Let us suppose we pick the order of rows beadc for the matrix
of Fig. 3.2. This permutation defines a minhash function h that maps sets to
rows. Let us compute the minhash value of set S1 according to h. The first
column, which is the column for set S1, has 0 in row b, so we proceed to row e,
the second in the permuted order. There is again a 0 in the column for S1, so
we proceed to row a, where we find a 1. Thus. h(S1) = a.
Element S1 S2 S3 S4
b 0 0 1 0
e 0 0 1 0
a 1 0 0 1
d 1 0 1 1
c 0 1 0 1
Figure 3.3: A permutation of the rows of Fig. 3.2
Although it is not physically possible to permute very large characteristic
matrices, the minhash function h implicitly reorders the rows of the matrix of
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 83
Fig. 3.2 so it becomes the matrix of Fig. 3.3. In this matrix, we can read off
the values of h by scanning from the top until we come to a 1. Thus, we see
that h(S2) = c, h(S3) = b, and h(S4) = a. ✷
3.3.3 Minhashing and Jaccard Similarity
There is a remarkable connection between minhashing and Jaccard similarity
of the sets that are minhashed.
• The probability that the minhash function for a random permutation of
rows produces the same value for two sets equals the Jaccard similarity
of those sets.
To see why, we need to picture the columns for those two sets. If we restrict
ourselves to the columns for sets S1 and S2, then rows can be divided into three
classes:
1. Type X rows have 1 in both columns.
2. Type Y rows have 1 in one of the columns and 0 in the other.
3. Type Z rows have 0 in both columns.
Since the matrix is sparse, most rows are of type Z. However, it is the ratio
of the numbers of type X and type Y rows that determine both SIM(S1, S2)
and the probability that h(S1) = h(S2). Let there be x rows of type X and y
rows of type Y . Then SIM(S1, S2) = x/(x + y). The reason is that x is the size
of S1 ∩ S2 and x + y is the size of S1 ∪ S2.
Now, consider the probability that h(S1) = h(S2). If we imagine the rows
permuted randomly, and we proceed from the top, the probability that we shall
meet a type X row before we meet a type Y row is x/(x + y). But if the
first row from the top other than type Z rows is a type X row, then surely
h(S1) = h(S2). On the other hand, if the first row other than a type Z row
that we meet is a type Y row, then the set with a 1 gets that row as its minhash
value. However the set with a 0 in that row surely gets some row further down
the permuted list. Thus, we know h(S1) 6= h(S2) if we first meet a type Y row.
We conclude the probability that h(S1) = h(S2) is x/(x + y), which is also the
Jaccard similarity of S1 and S2.
3.3.4 Minhash Signatures
Again think of a collection of sets represented by their characteristic matrix M.
To represent sets, we pick at random some number n of permutations of the
rows of M. Perhaps 100 permutations or several hundred permutations will do.
Call the minhash functions determined by these permutations h1, h2, . . . , hn.
From the column representing set S, construct the minhash signature for S, the
vector [h1(S), h2(S), . . . , hn(S)]. We normally represent this list of hash-values
84 CHAPTER 3. FINDING SIMILAR ITEMS
as a column. Thus, we can form from matrix M a signature matrix, in which
the ith column of M is replaced by the minhash signature for (the set of) the
ith column.
Note that the signature matrix has the same number of columns as M but
only n rows. Even if M is not represented explicitly, but in some compressed
form suitable for a sparse matrix (e.g., by the locations of its 1’s), it is normal
for the signature matrix to be much smaller than M.
The remarkable thing about signature matrices is that we can use their
columns to estimate the Jaccard similarity of the sets that correspond to the
columns of signature matrix. By the theorem proved in Section 3.3.3, we know
that the probability that two columns have the same value in a given row of
the signature matrix equals the Jaccard similarity of the sets corresponding to
those columns. Moreover, since the permutations on which the minhash values
are based were chosen independently, we can think of each row of the signature
matrix as an independent experiment. Thus, the expected number of rows in
which two columns agree equals the Jaccard similarity of their corresponding
sets. Moreover, the more minhashings we use, i.e., the more rows in the signature matrix, the smaller the expected error in the estimate of the Jaccard
similarity will be.
3.3.5 Computing Minhash Signatures in Practice
It is not feasible to permute a large characteristic matrix explicitly. Even picking
a random permutation of millions or billions of rows is time-consuming, and
the necessary sorting of the rows would take even more time. Thus, permuted
matrices like that suggested by Fig. 3.3, while conceptually appealing, are not
implementable.
Fortunately, it is possible to simulate the effect of a random permutation by
a random hash function that maps row numbers to as many buckets as there
are rows. A hash function that maps integers 0, 1, . . . , k − 1 to bucket numbers
0 through k−1 typically will map some pairs of integers to the same bucket and
leave other buckets unfilled. However, the difference is unimportant as long as
k is large and there are not too many collisions. We can maintain the fiction
that our hash function h “permutes” row r to position h(r) in the permuted
order.
Thus, instead of picking n random permutations of rows, we pick n randomly
chosen hash functions h1, h2, . . . , hn on the rows. We construct the signature
matrix by considering each row in their given order. Let SIG(i, c) be the element
of the signature matrix for the ith hash function and column c. Initially, set
SIG(i, c) to ∞ for all i and c. We handle row r by doing the following:
1. Compute h1(r), h2(r), . . . , hn(r).
2. For each column c do the following:
(a) If c has 0 in row r, do nothing.
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 85
(b) However, if c has 1 in row r, then for each i = 1, 2, . . . , n set SIG(i, c)
to the smaller of the current value of SIG(i, c) and hi(r).
Row S1 S2 S3 S4 x + 1 mod 5 3x + 1 mod 5
0 1 0 0 1 1 1
1 0 0 1 0 2 4
2 0 1 0 1 3 2
3 1 0 1 1 4 0
4 0 0 1 0 0 3
Figure 3.4: Hash functions computed for the matrix of Fig. 3.2
Example 3.8 : Let us reconsider the characteristic matrix of Fig. 3.2, which
we reproduce with some additional data as Fig. 3.4. We have replaced the
letters naming the rows by integers 0 through 4. We have also chosen two hash
functions: h1(x) = x+1 mod 5 and h2(x) = 3x+1 mod 5. The values of these
two functions applied to the row numbers are given in the last two columns of
Fig. 3.4. Notice that these simple hash functions are true permutations of the
rows, but a true permutation is only possible because the number of rows, 5, is
a prime. In general, there will be collisions, where two rows get the same hash
value.
Now, let us simulate the algorithm for computing the signature matrix.
Initially, this matrix consists of all ∞’s:
S1 S2 S3 S4
h1 ∞ ∞ ∞ ∞
h2 ∞ ∞ ∞ ∞
First, we consider row 0 of Fig. 3.4. We see that the values of h1(0) and
h2(0) are both 1. The row numbered 0 has 1’s in the columns for sets S1 and
S4, so only these columns of the signature matrix can change. As 1 is less than
∞, we do in fact change both values in the columns for S1 and S4. The current
estimate of the signature matrix is thus:
S1 S2 S3 S4
h1 1 ∞ ∞ 1
h2 1 ∞ ∞ 1
Now, we move to the row numbered 1 in Fig. 3.4. This row has 1 only in
S3, and its hash values are h1(1) = 2 and h2(1) = 4. Thus, we set SIG(1, 3) to 2
and SIG(2, 3) to 4. All other signature entries remain as they are because their
columns have 0 in the row numbered 1. The new signature matrix:
S1 S2 S3 S4
h1 1 ∞ 2 1
h2 1 ∞ 4 1
86 CHAPTER 3. FINDING SIMILAR ITEMS
The row of Fig. 3.4 numbered 2 has 1’s in the columns for S2 and S4, and
its hash values are h1(2) = 3 and h2(2) = 2. We could change the values in the
signature for S4, but the values in this column of the signature matrix, [1, 1], are
each less than the corresponding hash values [3, 2]. However, since the column
for S2 still has ∞’s, we replace it by [3, 2], resulting in:
S1 S2 S3 S4
h1 1 3 2 1
h2 1 2 4 1
Next comes the row numbered 3 in Fig. 3.4. Here, all columns but S2 have
1, and the hash values are h1(3) = 4 and h2(3) = 0. The value 4 for h1 exceeds
what is already in the signature matrix for all the columns, so we shall not
change any values in the first row of the signature matrix. However, the value
0 for h2 is less than what is already present, so we lower SIG(2, 1), SIG(2, 3) and
SIG(2, 4) to 0. Note that we cannot lower SIG(2, 2) because the column for S2 in
Fig. 3.4 has 0 in the row we are currently considering. The resulting signature
matrix:
S1 S2 S3 S4
h1 1 3 2 1
h2 0 2 0 0
Finally, consider the row of Fig. 3.4 numbered 4. h1(4) = 0 and h2(4) = 3.
Since row 4 has 1 only in the column for S3, we only compare the current
signature column for that set, [2, 0] with the hash values [0, 3]. Since 0 < 2, we
change SIG(1, 3) to 0, but since 3 > 0 we do not change SIG(2, 3). The final
signature matrix is:
S1 S2 S3 S4
h1 1 3 0 1
h2 0 2 0 0
We can estimate the Jaccard similarities of the underlying sets from this
signature matrix. Notice that columns 1 and 4 are identical, so we guess that
SIM(S1, S4) = 1.0. If we look at Fig. 3.4, we see that the true Jaccard similarity
of S1 and S4 is 2/3. Remember that the fraction of rows that agree in the
signature matrix is only an estimate of the true Jaccard similarity, and this
example is much too small for the law of large numbers to assure that the
estimates are close. For additional examples, the signature columns for S1 and
S3 agree in half the rows (true similarity 1/4), while the signatures of S1 and
S2 estimate 0 as their Jaccard similarity (the correct value). ✷
3.3.6 Speeding Up Minhashing
The process of minhashing is time-consuming, since we need to examine the
entire k-row matrix M for each minhash function we want. Let us first return
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 87
to the model of Section 3.3.2, where we imagine rows are actually permuted.
But to compute one minhash function on all the columns, we shall not go all
the way to the end of the permutation, but only look at the first m out of k
rows. If we make m small compared with k, we reduce the work by a large
factor, k/m.
However, there is a downside to making m small. As long as each column
has at least one 1 in the first m rows in permuted order, the rows after the mth
have no effect on any minhash value and may as well not be looked at. But
what if some columns are all-0’s in the first m rows? We have no minhash value
for those columns, and will instead have to use a special symbol, for which we
shall use ∞.
When we examine the minhash signatures of two columns in order to estimate the Jaccard similarity of their underlying sets, as in Section 3.3.4, we have
to take into account the possibility that one or both columns have ∞ as their
minhash value for some components of the signature. There are three cases:
1. If neither column has ∞ in a given row, then there is no change needed.
Count this row as an example of equal values if the two values are the
same, and as an example of unequal values if not.
2. One column has ∞ and the other does not. In this case, had we used
all the rows of the original permuted matrix M, the column that has the
∞ would eventually have been given some row number, and that number
will surely not be one of the first m rows in the permuted order. But the
other column does have a value that is one of the first m rows. Thus, we
surely have an example of unequal minhash values, and we count this row
of the signature matrix as such an example.
3. Now, suppose both columns have ∞ in row. Then in the original permuted
matrix M, the first m rows of both columns were all 0’s. We thus have no
information about the Jaccard similarity of the corresponding sets; that
similarity is only a function of the last k − m rows, which we have chosen
not to look at. We therefore count this row of the signature matrix as
neither an example of equal values nor of unequal values.
As long as the third case, where both columns have ∞, is rare, we get
almost as many examples to average as there are rows in the signature matrix.
That effect will reduce the accuracy of our estimates of the Jaccard distance
somewhat, but not much. And since we are now able to compute minhash
values for all the columns much faster than if we examined all the rows of M,
we can afford the time to apply a few more minhash functions. We get even
better accuracy than originally, and we do so faster than before.
3.3.7 Speedup Using Hash Functions
As before, there are reasons not to physically permute rows in the manner
assumed in Section 3.3.6. However, the idea of true permutations makes more
88 CHAPTER 3. FINDING SIMILAR ITEMS
sense in the context of Section 3.3.6 than it did in Section 3.3.2. The reason
is that we do not need to construct a full permutation of k elements, but only
pick a small number m out of the k rows and then pick a random permutation
of those rows. Depending on the value of m and how the matrix M is stored, it
might make sense to follow the algorithm suggested by Section 3.3.6 literally.
However, it is more likely that a strategy akin to Section 3.3.5 is needed.
Now, the rows of M are fixed, and not permuted. We choose a hash function
that hashes row numbers, and compute hash values for only the first m rows.
That is, we follow the algorithm of Section 3.3.5, but only until we reach the
mth row, whereupon we stop and, and for each columns, we take the minimum
hash value seen so far as the minhash value for that column.
Since some column may have 0 in all m rows, it is possible that some of the
minhash values will be ∞. Assuming m is sufficiently large that ∞ minhash
values are rare, we still get a good estimate of the Jaccard similarity of sets by
comparing columns of the signature matrix. Suppose T is the set of elements
of the universal set that are represented by the first m rows of matrix M. Let
S1 and S2 be the sets represented by two columns of M. Then the first m rows
of M represent the sets S1 ∩ T and S2 ∩ T . If both these sets are empty (i.e.,
both columns are all-0 in their first m rows), then this minhash function will be
∞ in both columns and will be ignored when estimating the Jaccard similarity
of the columns’ underlying sets.
If at least one of the sets S1 ∩ T and S2 ∩ T is nonempty, then the probability of the two columns having equal values for this minhash function is the
Jaccard similarity of these two sets, that is
|S1 ∩ S2 ∩ T |
|(S1 ∪ S2) ∩ T |
As long as T is chosen to be a random subset of the universal set, the expected
value of this fraction will be the same as the Jaccard similarity of S1 and S2.
However, there will be some random variation, since depending on T , we could
find more or less than an average number of type X rows (1’s in both columns)
and/or type Y rows (1 in one column and 0 in the other) among the first m
rows of matrix M.
To mitigate this variation, we do not use the same set T for each minhashing
that we do. Rather, we divide the rows of M into k/m groups.4 Then for each
hash function, we compute one minhash value by examining only the first m
rows of M, a different minhash value by examining only the second m rows,
and so on. We thus get k/m minhash values from a single hash function and
a single pass over all the rows of M. In fact, if k/m is large enough, we may
get all the rows of the signature matrix that we need by a single hash function
applied to each of the subsets of rows of M.
4
In what follows, we assume m divides k evenly, for convenience. It is unimportant, as
long as k/m is large, if some rows are not included in any group because k is not an integer
multiple of m.
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 89
Moreover, by using each of the rows of M to compute one of these minhash
values, we tend to balance out the errors in estimation of the Jaccard similarity
due to any one particular subset of the rows. That is, the Jaccard similarity
of S1 and S2 determines the ratio of type X and type Y rows. All the type X
rows are distributed among the k/m sets of rows, and likewise the type Y rows.
Thus, while one set of m rows may have more of one type of row than average,
there must then be some other set of m rows with fewer than average of that
same type.
Example 3.9 : In Fig. 3.5 we see a matrix representing three sets S1, S2, and
S3, with a universal set of eight elements; i.e., k = 8. Let us pick m = 4, so one
pass through the rows yields two minhash values, one based on the first four
rows and the other on the second four rows.
S1 S2 S3
0 0 0
0 0 0
0 0 1
0 1 1
1 1 1
1 1 0
1 0 0
0 0 0
Figure 3.5: A Boolean matrix representing three sets
First, note that the Jaccard similarities of the three sets are SIM(S1, S2) =
1/2, SIM(S1, S3) = 1/5, and SIM(S2, S3) = 1/2. Now, look at the first four
rows only. Whatever hash function we use, the minhash value for S1 will be
∞, the minhash value for S2 will be the hash value of the 4th row, and the
minhash value for S3 will be the smaller of the hash values for the third and
fourth rows. Thus, the minhash values for S1 and S2 will never agree. That
makes sense, since if T is the set of elements represented by the first four rows,
then S1 ∩ T = ∅, and therefore SIM(S1 ∩ T, S2 ∩ T ) = 0. However, in the
second four rows, the Jaccard similarity of S1 and S2 restricted to the elements
represented by the last four rows is 2/3.
We conclude that if we generate signatures consisting of two minhash values
using this hash function, one based on the first four rows and the second based
on the last four rows, the expected number of matches we get between the
signatures for S1 and S2 is the average of 0 and 2/3, or 1/3. Since the actual
Jaccard similarity of S1 and S2 is 1/2, there is an error, but not too great an
error. In larger examples, where minhash values are based on far more than
four rows, the expected error will approach zero.
Similarly, we can see the effect of splitting the rows on the other two pairs
of columns. Between S1 and S3, the top half represents sets with a Jaccard
90 CHAPTER 3. FINDING SIMILAR ITEMS
similarity of 0, while the bottom half represents sets with a Jaccard similarity
1/3. The expected number of matches in the signatures of S1 and S3 is therefore
the average of these, or 1/6. That compares with the true Jaccard similarity
SIM(S1, S3) = 1/5. Finally, when we compare S2 and S3, we note that the
Jaccard similarity of these columns in the first four rows is 1/2, and so is their
Jaccard similarity in the bottom four rows. The average, 1/2, also agrees exactly
with SIM(S2, S3) = 1/2. ✷
3.3.8 Exercises for Section 3.3
Exercise 3.3.1 : Verify the theorem from Section 3.3.3, which relates the Jaccard similarity to the probability of minhashing to equal values, for the particular case of Fig. 3.2.
(a) Compute the Jaccard similarity of each of the pairs of columns in Fig. 3.2.
! (b) Compute, for each pair of columns of that figure, the fraction of the 120
permutations of the rows that make the two columns hash to the same
value.
Exercise 3.3.2 : Using the data from Fig. 3.4, add to the signatures of the
columns the values of the following hash functions:
(a) h3(x) = 2x + 4 mod 5.
(b) h4(x) = 3x − 1 mod 5.
Element S1 S2 S3 S4
0 0 1 0 1
1 0 1 0 0
2 1 0 0 1
3 0 0 1 0
4 0 0 1 1
5 1 0 0 0
Figure 3.6: Matrix for Exercise 3.3.3
Exercise 3.3.3 : In Fig. 3.6 is a matrix with six rows.
(a) Compute the minhash signature for each column if we use the following
three hash functions: h1(x) = 2x + 1 mod 6; h2(x) = 3x + 2 mod 6;
h3(x) = 5x + 2 mod 6.
(b) Which of these hash functions are true permutations?
3.4. LOCALITY-SENSITIVE HASHING FOR DOCUMENTS 91
(c) How close are the estimated Jaccard similarities for the six pairs of columns
to the true Jaccard similarities?
! Exercise 3.3.4 : Now that we know Jaccard similarity is related to the probability that two sets minhash to the same value, reconsider Exercise 3.1.3. Can
you use this relationship to simplify the problem of computing the expected
Jaccard similarity of randomly chosen sets?
! Exercise 3.3.5 : Prove that if the Jaccard similarity of two columns is 0, then
minhashing always gives a correct estimate of the Jaccard similarity.
!! Exercise 3.3.6 : One might expect that we could estimate the Jaccard similarity of columns without using all possible permutations of rows. For example,
we could only allow cyclic permutations; i.e., start at a randomly chosen row
r, which becomes the first in the order, followed by rows r + 1, r + 2, and so
on, down to the last row, and then continuing with the first row, second row,
and so on, down to row r − 1. There are only n such permutations if there are
n rows. However, these permutations are not sufficient to estimate the Jaccard
similarity correctly. Give an example of a two-column matrix where averaging
over all the cyclic permutations does not give the Jaccard similarity.
! Exercise 3.3.7 : Suppose we want to use a MapReduce framework to compute
minhash signatures. If the matrix is stored in chunks that correspond to some
columns, then it is quite easy to exploit parallelism. Each Map task gets some
of the columns and all the hash functions, and computes the minhash signatures
of its given columns. However, suppose the matrix were chunked by rows, so
that a Map task is given the hash functions and a set of rows to work on. Design
Map and Reduce functions to exploit MapReduce with data in this form.
! Exercise 3.3.8 : As we noticed in Section 3.3.6, we have problems when a
column has only 0’s. If we compute a minhash function using entire columns
(as in Section 3.3.2), then the only time we get all 0’s in a column is if that
column represents the empty set. How should we handle the empty set to make
sure no errors in Jaccard-similarity estimation are introduced?
!! Exercise 3.3.9 : In Example 3.9, each of the three estimates of Jaccard similarity we obtained was either smaller than or the same as the true Jaccard
similarity. Is it possible that for another pair of columns, the average of the
Jaccard similarities of the upper and lower halves will exceed the actual Jaccard
similarity of the columns?
3.4 Locality-Sensitive Hashing for Documents
Even though we can use minhashing to compress large documents into small
signatures and preserve the expected similarity of any pair of documents, it
still may be impossible to find the pairs with greatest similarity efficiently. The
92 CHAPTER 3. FINDING SIMILAR ITEMS
reason is that the number of pairs of documents may be too large, even if there
are not too many documents.
Example 3.10 : Suppose we have a million documents, and we use signatures
of length 250. Then we use 1000 bytes per document for the signatures, and
the entire data fits in a gigabyte – less than a typical main memory of a laptop.
However, there are
1,000,000
2

or half a trillion pairs of documents. If it takes a
microsecond to compute the similarity of two signatures, then it takes almost
six days to compute all the similarities on that laptop. ✷
If our goal is to compute the similarity of every pair, there is nothing we
can do to reduce the work, although parallelism can reduce the elapsed time.
However, often we want only the most similar pairs or all pairs that are above
some lower bound in similarity. If so, then we need to focus our attention only
on pairs that are likely to be similar, without investigating every pair. There is
a general theory of how to provide such focus, called locality-sensitive hashing
(LSH) or near-neighbor search. In this section we shall consider a specific form
of LSH, designed for the particular problem we have been studying: documents,
represented by shingle-sets, then minhashed to short signatures. In Section 3.6
we present the general theory of locality-sensitive hashing and a number of
applications and related techniques.
3.4.1 LSH for Minhash Signatures
One general approach to LSH is to “hash” items several times, in such a way that
similar items are more likely to be hashed to the same bucket than dissimilar
items are. We then consider any pair that hashed to the same bucket for any
of the hashings to be a candidate pair. We check only the candidate pairs for
similarity. The hope is that most of the dissimilar pairs will never hash to the
same bucket, and therefore will never be checked. Those dissimilar pairs that
do hash to the same bucket are false positives; we hope these will be only a
small fraction of all pairs. We also hope that most of the truly similar pairs
will hash to the same bucket under at least one of the hash functions. Those
that do not are false negatives; we hope these will be only a small fraction of
the truly similar pairs.
If we have minhash signatures for the items, an effective way to choose the
hashings is to divide the signature matrix into b bands consisting of r rows
each. For each band, there is a hash function that takes vectors of r integers
(the portion of one column within that band) and hashes them to some large
number of buckets. We can use the same hash function for all the bands, but
we use a separate bucket array for each band, so columns with the same vector
in different bands will not hash to the same bucket.
Example 3.11 : Figure 3.7 shows part of a signature matrix of 12 rows divided
into four bands of three rows each. The second and fourth of the explicitly
shown columns each have the column vector [0, 2, 1] in the first band, so the
3.4. LOCALITY-SENSITIVE HASHING FOR DOCUMENTS 93
1 0 0 0 2
3 2 1 2 2
0 1 3 1 1
. . . . . . band 1
band 2
band 3
band 4
Figure 3.7: Dividing a signature matrix into four bands of three rows per band
will definitely hash to the same bucket in the hashing for the first band. Thus,
regardless of what those columns look like in the other three bands, this pair
of columns will be a candidate pair. It is possible that other columns, such as
the first two shown explicitly, will also hash to the same bucket according to
the hashing of the first band. However, since their column vectors are different,
[1, 3, 0] and [0, 2, 1], and there are many buckets for each hashing, we expect the
chances of an accidental collision to be very small. We shall normally assume
that two vectors hash to the same bucket if and only if they are identical.
Two columns that do not agree in band 1 have three other chances to become
a candidate pair; they might be identical in any one of these other bands.
However, observe that the more similar two columns are, the more likely it is
that they will be identical in some band. Thus, intuitively the banding strategy
makes similar columns much more likely to be candidate pairs than dissimilar
pairs. ✷
3.4.2 Analysis of the Banding Technique
Suppose we use b bands of r rows each, and suppose that a particular pair of
documents have Jaccard similarity s. Recall from Section 3.3.3 that the probability the minhash signatures for these documents agree in any one particular
row of the signature matrix is s. We can calculate the probability that these
documents (or rather their signatures) become a candidate pair as follows:
1. The probability that the signatures agree in all rows of one particular
band is s
r
.
2. The probability that the signatures disagree in at least one row of a particular band is 1 − s
r
.
3. The probability that the signatures disagree in at least one row of each
of the bands is (1 − s
r
)
b
.
94 CHAPTER 3. FINDING SIMILAR ITEMS
4. The probability that the signatures agree in all the rows of at least one
band, and therefore become a candidate pair, is 1 − (1 − s
r
)
b
.
0 1
 of documents
Jaccard similarity
Probability
of becoming
a candidate
Figure 3.8: The S-curve
It may not be obvious, but regardless of the chosen constants b and r, this
function has the form of an S-curve, as suggested in Fig. 3.8. The threshold,
that is, the value of similarity s at which the probability of becoming a candidate is 1/2, is a function of b and r. The threshold is roughly where the rise is
the steepest, and for large b and r we find that pairs with similarity above the
threshold are very likely to become candidates, while those below the threshold
are unlikely to become candidates – exactly the situation we want. An approximation to the threshold is (1/b)
1/r. For example, if b = 16 and r = 4, then the
threshold is approximately at s = 1/2, since the 4th root of 1/16 is 1/2.
Example 3.12 : Let us consider the case b = 20 and r = 5. That is, we suppose
we have signatures of length 100, divided into twenty bands of five rows each.
Figure 3.9 tabulates some of the values of the function 1 − (1 − s
5
)
20. Notice
that the threshold, the value of s at which the curve has risen halfway, is just
slightly more than 0.5. Also notice that the curve is not exactly the ideal step
function that jumps from 0 to 1 at the threshold, but the slope of the curve
in the middle is significant. For example, it rises by more than 0.6 going from
s = 0.4 to s = 0.6, so the slope in the middle is greater than 3.
For example, at s = 0.8, 1 − (0.8)5
is about 0.672. If you raise this number
to the 20th power, you get about 0.00035. Subtracting this fraction from 1
yields 0.99965. That is, if we consider two documents with 80% similarity, then
in any one band, they have only about a 33% chance of agreeing in all five rows
and thus becoming a candidate pair. However, there are 20 bands and thus 20
3.4. LOCALITY-SENSITIVE HASHING FOR DOCUMENTS 95
s 1 − (1 − s
r
)
b
.2 .006
.3 .047
.4 .186
.5 .470
.6 .802
.7 .975
.8 .9996
Figure 3.9: Values of the S-curve for b = 20 and r = 5
chances to become a candidate. Only roughly one in 3000 pairs that are as high
as 80% similar will fail to become a candidate pair and thus be a false negative.
✷
3.4.3 Combining the Techniques
We can now give an approach to finding the set of candidate pairs for similar
documents and then discovering the truly similar documents among them. It
must be emphasized that this approach can produce false negatives – pairs of
similar documents that are not identified as such because they never become
a candidate pair. There will also be false positives – candidate pairs that are
evaluated, but are found not to be sufficiently similar.
1. Pick a value of k and construct from each document the set of k-shingles.
Optionally, hash the k-shingles to shorter bucket numbers.
2. Sort the document-shingle pairs to order them by shingle.
3. Pick a length n for the minhash signatures. Feed the sorted list to the
algorithm of Section 3.3.5 to compute the minhash signatures for all the
documents.
4. Choose a threshold t that defines how similar documents have to be in
order for them to be regarded as a desired “similar pair.” Pick a number
of bands b and a number of rows r such that br = n, and the threshold
t is approximately (1/b)
1/r. If avoidance of false negatives is important,
you may wish to select b and r to produce a threshold lower than t; if
speed is important and you wish to limit false positives, select b and r to
produce a higher threshold.
5. Construct candidate pairs by applying the LSH technique of Section 3.4.1.
6. Examine each candidate pair’s signatures and determine whether the fraction of components in which they agree is at least t.
96 CHAPTER 3. FINDING SIMILAR ITEMS
7. Optionally, if the signatures are sufficiently similar, go to the documents
themselves and check that they are truly similar, rather than documents
that, by luck, had similar signatures.
3.4.4 Exercises for Section 3.4
Exercise 3.4.1 : Evaluate the S-curve 1 − (1 − s
r
)
b
for s = 0.1, 0.2, . . . , 0.9, for
the following values of r and b:
• r = 3 and b = 10.
• r = 6 and b = 20.
• r = 5 and b = 50.
! Exercise 3.4.2 : For each of the (r, b) pairs in Exercise 3.4.1, compute the
threshold, that is, the value of s for which the value of 1−(1−s
r
)
b
is exactly 1/2.
How does this value compare with the estimate of (1/b)
1/r that was suggested
in Section 3.4.2?
! Exercise 3.4.3 : Use the techniques explained in Section 1.3.5 to approximate
the S-curve 1 − (1 − s
r
)
b when s
r
is very small.
! Exercise 3.4.4 : Suppose we wish to implement LSH by MapReduce. Specifically, assume chunks of the signature matrix consist of columns, and elements
are key-value pairs where the key is the column number and the value is the
signature itself (i.e., a vector of values).
(a) Show how to produce the buckets for all the bands as output of a single
MapReduce process. Hint: Remember that a Map function can produce
several key-value pairs from a single element.
(b) Show how another MapReduce process can convert the output of (a) to
a list of pairs that need to be compared. Specifically, for each column i,
there should be a list of those columns j > i with which i needs to be
compared.
3.5 Distance Measures
We now take a short detour to study the general notion of distance measures.
The Jaccard similarity is a measure of how close sets are, although it is not
really a distance measure. That is, the closer sets are, the higher the Jaccard
similarity. Rather, 1 minus the Jaccard similarity is a distance measure, as we
shall see; it is called the Jaccard distance.
However, Jaccard distance is not the only measure of closeness that makes
sense. We shall examine in this section some other distance measures that have
applications. Then, in Section 3.6 we see how some of these distance measures
3.5. DISTANCE MEASURES 97
also have an LSH technique that allows us to focus on nearby points without
comparing all points. Other applications of distance measures will appear when
we study clustering in Chapter 7.
3.5.1 Definition of a Distance Measure
Suppose we have a set of points, called a space. A distance measure on this
space is a function d(x, y) that takes two points in the space as arguments and
produces a real number, and satisfies the following axioms:
1. d(x, y) ≥ 0 (no negative distances).
2. d(x, y) = 0 if and only if x = y (distances are positive, except for the
distance from a point to itself).
3. d(x, y) = d(y, x) (distance is symmetric).
4. d(x, y) ≤ d(x, z) + d(z, y) (the triangle inequality).
The triangle inequality is the most complex condition. It says, intuitively, that
to travel from x to y, we cannot obtain any benefit if we are forced to travel via
some particular third point z. The triangle-inequality axiom is what makes all
distance measures behave as if distance describes the length of a shortest path
from one point to another.
3.5.2 Euclidean Distances
The most familiar distance measure is the one we normally think of as “distance.” An n-dimensional Euclidean space is one where points are vectors of n
real numbers. The conventional distance measure in this space, which we shall
refer to as the L2-norm, is defined:
d([x1, x2, . . . , xn], [y1, y2, . . . , yn]) =
vuutXn
i=1
(xi − yi)
2
That is, we square the distance in each dimension, sum the squares, and take
the positive square root.
It is easy to verify the first three requirements for a distance measure are
satisfied. The Euclidean distance between two points cannot be negative, because the positive square root is intended. Since all squares of real numbers are
nonnegative, any i such that xi 6= yi forces the distance to be strictly positive.
On the other hand, if xi = yi for all i, then the distance is clearly 0. Symmetry
follows because (xi − yi)
2 = (yi − xi)
2
. The triangle inequality requires a good
deal of algebra to verify. However, it is well understood to be a property of
Euclidean space: the sum of the lengths of any two sides of a triangle is no less
than the length of the third side.
98 CHAPTER 3. FINDING SIMILAR ITEMS
There are other distance measures that have been used for Euclidean spaces.
For any constant r, we can define the Lr-norm to be the distance measure d
defined by:
d([x1, x2, . . . , xn], [y1, y2, . . . , yn]) = (Xn
i=1
|xi − yi
|
r
)
1/r
The case r = 2 is the usual L2-norm just mentioned. Another common distance
measure is the L1-norm, or Manhattan distance. There, the distance between
two points is the sum of the magnitudes of the differences in each dimension.
It is called “Manhattan distance” because it is the distance one would have to
travel between points if one were constrained to travel along grid lines, as on
the streets of a city such as Manhattan.
Another interesting distance measure is the L∞-norm, which is the limit
as r approaches infinity of the Lr-norm. As r gets larger, only the dimension
with the largest difference matters, so formally, the L∞-norm is defined as the
maximum of |xi − yi
| over all dimensions i.
Example 3.13 : Consider the two-dimensional Euclidean space (the customary plane) and the points (2, 7) and (6, 4). The L2-norm gives a distance
of p
(2 − 6)2 + (7 − 4)2 =
√
4
2 + 32 = 5. The L1-norm gives a distance of
|2 − 6| + |7 − 4| = 4 + 3 = 7. The L∞-norm gives a distance of
max(|2 − 6|, |7 − 4|) = max(4, 3) = 4
✷
3.5.3 Jaccard Distance
As mentioned at the beginning of the section, we define the Jaccard distance
of sets by d(x, y) = 1 − SIM(x, y). That is, the Jaccard distance is 1 minus the
ratio of the sizes of the intersection and union of sets x and y. We must verify
that this function is a distance measure.
1. d(x, y) is nonnegative because the size of the intersection cannot exceed
the size of the union.
2. d(x, y) = 0 if x = y, because x ∪ x = x ∩ x = x. However, if x 6= y, then
the size of x ∩ y is strictly less than the size of x ∪ y, so d(x, y) is strictly
positive.
3. d(x, y) = d(y, x) because both union and intersection are symmetric; i.e.,
x ∪ y = y ∪ x and x ∩ y = y ∩ x.
4. For the triangle inequality, recall from Section 3.3.3 that SIM(x, y) is the
probability a random minhash function maps x and y to the same value.
Thus, the Jaccard distance d(x, y) is the probability that a random minhash function does not send x and y to the same value. We can therefore
3.5. DISTANCE MEASURES 99
translate the condition d(x, y) ≤ d(x, z) + d(z, y) to the statement that if
h is a random minhash function, then the probability that h(x) 6= h(y)
is no greater than the sum of the probability that h(x) 6= h(z) and the
probability that h(z) 6= h(y). However, this statement is true because
whenever h(x) 6= h(y), at least one of h(x) and h(y) must be different
from h(z). They could not both be h(z), because then h(x) and h(y)
would be the same.
.
3.5.4 Cosine Distance
The cosine distance makes sense in spaces that have dimensions, including Euclidean spaces and discrete versions of Euclidean spaces, such as spaces where
points are vectors with integer components or Boolean (0 or 1) components. In
such a space, points may be thought of as directions. We do not distinguish between a vector and a multiple of that vector. Then the cosine distance between
two points is the angle that the vectors to those points make. This angle will
be in the range 0 to 180 degrees, regardless of how many dimensions the space
has.
We can calculate the cosine distance by first computing the cosine of the
angle, and then applying the arc-cosine function to translate to an angle in the
0-180 degree range. Given two vectors x and y, the cosine of the angle between
them is the dot product x.y divided by the L2-norms of x and y (i.e., their
Euclidean distances from the origin). Recall that the dot product of vectors
[x1, x2, . . . , xn].[y1, y2, . . . , yn] is Pn
i=1 xiyi
.
Example 3.14 : Let our two vectors be x = [1, 2, −1] and = [2, 1, 1]. The dot
√
product x.y is 1 × 2 + 2 × 1 + (−1) × 1 = 3. The L2-norm of both vectors is
6. For example, x has L2-norm p
1
2 + 22 + (−1)2 =
√
6. Thus, the cosine of
the angle between x and y is 3/(
√
6
√
6) or 1/2. The angle whose cosine is 1/2
is 60 degrees, so that is the cosine distance between x and y. ✷
We must show that the cosine distance is indeed a distance measure. We
have defined it so the values are in the range 0 to 180, so no negative distances
are possible. Two vectors have angle 0 if and only if they are the same direction.5
Symmetry is obvious: the angle between x and y is the same as the angle
between y and x. The triangle inequality is best argued by physical reasoning.
One way to rotate from x to y is to rotate to z and thence to y. The sum of
those two rotations cannot be less than the rotation directly from x to y.
5Notice that to satisfy the second axiom, we have to treat vectors that are multiples of
one another, e.g. [1, 2] and [3, 6], as the same direction, which they are. If we regarded these
as different vectors, we would give them distance 0 and thus violate the condition that only
d(x, x) is 0.
100 CHAPTER 3. FINDING SIMILAR ITEMS
3.5.5 Edit Distance
This distance makes sense when points are strings. The distance between two
strings x = x1x2 · · · xn and y = y1y2 · · · ym is the smallest number of insertions
and deletions of single characters that will convert x to y.
Example 3.15 : The edit distance between the strings x = abcde and y =
acfdeg is 3. To convert x to y:
1. Delete b.
2. Insert f after c.
3. Insert g after e.
No sequence of fewer than three insertions and/or deletions will convert x to y.
Thus, d(x, y) = 3. ✷
Another way to define and calculate the edit distance d(x, y) is to compute
a longest common subsequence (LCS) of x and y. An LCS of x and y is a
string that is constructed by deleting positions from x and y, and that is as
long as any string that can be constructed that way. The edit distance d(x, y)
can be calculated as the length of x plus the length of y minus twice the length
of their LCS.
Example 3.16 : The strings x = abcde and y = acfdeg from Example 3.15
have a unique LCS, which is acde. We can be sure it is the longest possible,
because it contains every symbol appearing in both x and y. Fortunately, these
common symbols appear in the same order in both strings, so we are able to
use them all in an LCS. Note that the length of x is 5, the length of y is 6, and
the length of their LCS is 4. The edit distance is thus 5 + 6 − 2 × 4 = 3, which
agrees with the direct calculation in Example 3.15.
For another example, consider x = aba and y = bab. Their edit distance is
2. For example, we can convert x to y by deleting the first a and then inserting
b at the end. There are two LCS’s: ab and ba. Each can be obtained by
deleting one symbol from each string. As must be the case for multiple LCS’s
of the same pair of strings, both LCS’s have the same length. Therefore, we
may compute the edit distance as 3 + 3 − 2 × 2 = 2. ✷
Edit distance is a distance measure. Surely no edit distance can be negative,
and only two identical strings have an edit distance of 0. To see that edit
distance is symmetric, note that a sequence of insertions and deletions can be
reversed, with each insertion becoming a deletion, and vice versa. The triangle
inequality is also straightforward. One way to turn a string s into a string t
is to turn s into some string u and then turn u into t. Thus, the number of
edits made going from s to u, plus the number of edits made going from u to t
cannot be less than the smallest number of edits that will turn s into t.
3.5. DISTANCE MEASURES 101
Non-Euclidean Spaces
Notice that several of the distance measures introduced in this section are
not Euclidean spaces. A property of Euclidean spaces that we shall find
important when we take up clustering in Chapter 7 is that the average
of points in a Euclidean space always exists and is a point in the space.
However, consider the space of sets for which we defined the Jaccard distance. The notion of the “average” of two sets makes no sense. Likewise,
the space of strings, where we can use the edit distance, does not let us
take the “average” of strings.
Vector spaces, for which we suggested the cosine distance, may or may
not be Euclidean. If the components of the vectors can be any real numbers, then the space is Euclidean. However, if we restrict components to
be integers, then the space is not Euclidean. Notice that, for instance, we
cannot find an average of the vectors [1, 2] and [3, 1] in the space of vectors
with two integer components, although if we treated them as members of
the two-dimensional Euclidean space, then we could say that their average
was [2.0, 1.5].
3.5.6 Hamming Distance
Given a space of vectors, we define the Hamming distance between two vectors
to be the number of components in which they differ. It should be obvious
that Hamming distance is a distance measure. Clearly the Hamming distance
cannot be negative, and if it is zero, then the vectors are identical. The distance does not depend on which of two vectors we consider first. The triangle
inequality should also be evident. If x and z differ in m components, and z
and y differ in n components, then x and y cannot differ in more than m + n
components. Most commonly, Hamming distance is used when the vectors are
Boolean; they consist of 0’s and 1’s only. However, in principle, the vectors can
have components from any set.
Example 3.17 : The Hamming distance between the vectors 10101 and 11110
is 3. That is, these vectors differ in the second, fourth, and fifth components,
while they agree in the first and third components. ✷
3.5.7 Exercises for Section 3.5
! Exercise 3.5.1 : On the space of nonnegative integers, which of the following
functions are distance measures? If so, prove it; if not, prove that it fails to
satisfy one or more of the axioms.
(a) max(x, y) = the larger of x and y.
102 CHAPTER 3. FINDING SIMILAR ITEMS
(b) diff(x, y) = |x − y| (the absolute magnitude of the difference between x
and y).
(c) sum(x, y) = x + y.
Exercise 3.5.2 : Find the L1 and L2 distances between the points (5, 6, 7) and
(8, 2, 4).
!! Exercise 3.5.3 : Prove that if i and j are any positive integers, and i < j,
then the Li norm between any two points is greater than the Lj norm between
those same two points.
Exercise 3.5.4 : Find the Jaccard distances between the following pairs of
sets:
(a) {1, 2, 3, 4} and {2, 3, 4, 5}.
(b) {1, 2, 3} and {4, 5, 6}.
Exercise 3.5.5 : Compute the cosines of the angles between each of the following pairs of vectors.6
(a) (3, −1, 2) and (−2, 3, 1).
(b) (1, 2, 3) and (2, 4, 6).
(c) (5, 0, −4) and (−1, −6, 2).
(d) (0, 1, 1, 0, 1, 1) and (0, 0, 1, 0, 0, 0).
! Exercise 3.5.6 : Prove that the cosine distance between any two vectors of 0’s
and 1’s, of the same length, is at most 90 degrees.
Exercise 3.5.7 : Find the edit distances (using only insertions and deletions)
between the following pairs of strings.
(a) abcdef and bdaefc.
(b) abccdabc and acbdcab.
(c) abcdef and baedfc.
! Exercise 3.5.8 : There are a number of other notions of edit distance available.
For instance, we can allow, in addition to insertions and deletions, the following
operations:
6Note that what we are asking for is not precisely the cosine distance, but from the cosine
of an angle, you can compute the angle itself, perhaps with the aid of a table or library
function.
3.6. THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS 103
i. Mutation, where one symbol is replaced by another symbol. Note that a
mutation can always be performed by an insertion followed by a deletion,
but if we allow mutations, then this change counts for only 1, not 2, when
computing the edit distance.
ii. Transposition, where two adjacent symbols have their positions swapped.
Like a mutation, we can simulate a transposition by one insertion followed
by one deletion, but here we count only 1 for these two steps.
Repeat Exercise 3.5.7 if edit distance is defined to be the number of insertions,
deletions, mutations, and transpositions needed to transform one string into
another.
! Exercise 3.5.9 : Prove that the edit distance discussed in Exercise 3.5.8 is
indeed a distance measure.
Exercise 3.5.10 : Find the Hamming distances between each pair of the following vectors: 000000, 110011, 010101, and 011100.
3.6 The Theory of Locality-Sensitive Functions
The LSH technique developed in Section 3.4 is one example of a family of functions (the minhash functions) that can be combined (by the banding technique)
to distinguish strongly between pairs at a low distance from pairs at a high distance. The steepness of the S-curve in Fig. 3.8 reflects how effectively we can
avoid false positives and false negatives among the candidate pairs.
Now, we shall explore other families of functions, besides the minhash functions, that can serve to produce candidate pairs efficiently. These functions can
apply to the space of sets and the Jaccard distance, or to another space and/or
another distance measure. There are three conditions that we need for a family
of functions:
1. They must be more likely to make close pairs be candidate pairs than
distant pairs. We make this notion precise in Section 3.6.1.
2. They must be statistically independent, in the sense that it is possible to
estimate the probability that two or more functions will all give a certain
response by the product rule for independent events.
3. They must be efficient, in two ways:
(a) They must be able to identify candidate pairs in time much less
than the time it takes to look at all pairs. For example, minhash
functions have this capability, since we can hash sets to minhash
values in time proportional to the size of the data, rather than the
square of the number of sets in the data. Since sets with common
values are colocated in a bucket, we have implicitly produced the
104 CHAPTER 3. FINDING SIMILAR ITEMS
candidate pairs for a single minhash function in time much less than
the number of pairs of sets.
(b) They must be combinable to build functions that are better at avoiding false positives and negatives, and the combined functions must
also take time that is much less than the number of pairs. For example, the banding technique of Section 3.4.1 takes single minhash
functions, which satisfy condition 3a but do not, by themselves have
the S-curve behavior we want, and produces from a number of minhash functions a combined function that has the S-curve shape.
Our first step is to define “locality-sensitive functions” generally. We then
see how the idea can be applied in several applications. Finally, we discuss
how to apply the theory to arbitrary data with either a cosine distance or a
Euclidean distance measure.
3.6.1 Locality-Sensitive Functions
For the purposes of this section, we shall consider functions that take two items
and render a decision about whether these items should be a candidate pair.
In many cases, the function f will “hash” items, and the decision will be based
on whether or not the result is equal. Because it is convenient to use the
notation f(x) = f(y) to mean that f(x, y) is “yes; make x and y a candidate
pair,” we shall use f(x) = f(y) as a shorthand with this meaning. We also use
f(x) 6= f(y) to mean “do not make x and y a candidate pair unless some other
function concludes we should do so.”
A collection of functions of this form will be called a family of functions.
For example, the family of minhash functions, each based on one of the possible
permutations of rows of a characteristic matrix, form a family.
Let d1 < d2 be two distances according to some distance measure d. A
family F of functions is said to be (d1, d2, p1, p2)-sensitive if for every f in F:
1. If d(x, y) ≤ d1, then the probability that f(x) = f(y) is at least p1.
2. If d(x, y) ≥ d2, then the probability that f(x) = f(y) is at most p2.
Figure 3.10 illustrates what we expect about the probability that a given
function in a (d1, d2, p1, p2)-sensitive family will declare two items to be a candidate pair. Notice that we say nothing about what happens when the distance
between the items is strictly between d1 and d2, but we can make d1 and d2 as
close as we wish. The penalty is that typically p1 and p2 are then close as well.
As we shall see, it is possible to drive p1 and p2 apart while keeping d1 and d2
fixed.
3.6.2 Locality-Sensitive Families for Jaccard Distance
For the moment, we have only one way to find a family of locality-sensitive
functions: use the family of minhash functions, and assume that the distance
3.6. THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS 105
Probabilty
of being
declared a
candidate
d
p
d
p
1 2
1
2
Distance
Figure 3.10: Behavior of a (d1, d2, p1, p2)-sensitive function
measure is the Jaccard distance. As before, we interpret a minhash function h
to make x and y a candidate pair if and only if h(x) = h(y).
• The family of minhash functions is a (d1, d2, 1−d1, 1−d2)-sensitive family
for any d1 and d2, where 0 ≤ d1 < d2 ≤ 1.
The reason is that if d(x, y) ≤ d1, where d is the Jaccard distance, then
SIM(x, y) = 1 − d(x, y) ≥ 1 − d1. But we know that the Jaccard similarity
of x and y is equal to the probability that a minhash function will hash x and
y to the same value. A similar argument applies to d2 or any distance.
Example 3.18 : We could let d1 = 0.3 and d2 = 0.6. Then we can assert that
the family of minhash functions is a (0.3, 0.6, 0.7, 0.4)-sensitive family. That is,
if the Jaccard distance between x and y is at most 0.3 (i.e., SIM(x, y) ≥ 0.7)
then there is at least a 0.7 chance that a minhash function will send x and y to
the same value, and if the Jaccard distance between x and y is at least 0.6 (i.e.,
SIM(x, y) ≤ 0.4), then there is at most a 0.4 chance that x and y will be sent
to the same value. Note that we could make the same assertion with another
choice of d1 and d2; only d1 < d2 is required. ✷
3.6.3 Amplifying a Locality-Sensitive Family
Suppose we are given a (d1, d2, p1, p2)-sensitive family F. We can construct a
new family F
′
by the AND-construction on F, which is defined as follows. Each
member of F
′
consists of r members of F for some fixed r. If f is in F
′
, and f is
constructed from the set {f1, f2, . . . , fr} of members of F, we say f(x) = f(y)
if and only if fi(x) = fi(y) for all i = 1, 2, . . . , r. Notice that this construction
mirrors the effect of the r rows in a single band: the band makes x and y a
106 CHAPTER 3. FINDING SIMILAR ITEMS
candidate pair if every one of the r rows in the band say that x and y are equal
(and therefore a candidate pair according to that row).
Since the members of F are independently chosen to make a member of F
′
,
we can assert that F
′
is a
d1, d2,(p1)
r
,(p2)
r

-sensitive family. That is, for any
p, if p is the probability that a member of F will declare (x, y) to be a candidate
pair, then the probability that a member of F
′ will so declare is p
r
.
There is another construction, which we call the OR-construction, that turns
a (d1, d2, p1, p2)-sensitive family F into a
d1, d2, 1 − (1 − p1)
b
, 1 − (1 − p2)
b

-
sensitive family F
′
. Each member f of F
′
is constructed from b members of F,
say f1, f2, . . . , fb. We define f(x) = f(y) if and only if fi(x) = fi(y) for one or
more values of i. The OR-construction mirrors the effect of combining several
bands: x and y become a candidate pair if any band makes them a candidate
pair.
If p is the probability that a member of F will declare (x, y) to be a candidate
pair, then 1−p is the probability it will not so declare. (1−p)
b
is the probability
that none of f1, f2, . . . , fb will declare (x, y) a candidate pair, and 1 − (1 − p)
b
is the probability that at least one fi will declare (x, y) a candidate pair, and
therefore that f will declare (x, y) to be a candidate pair.
Notice that the AND-construction lowers all probabilities, but if we choose F
and r judiciously, we can make the small probability p2 get very close to 0, while
the higher probability p1 stays significantly away from 0. Similarly, the ORconstruction makes all probabilities rise, but by choosing F and b judiciously,
we can make the larger probability approach 1 while the smaller probability
remains bounded away from 1. We can cascade AND- and OR-constructions in
any order to make the low probability close to 0 and the high probability close
to 1. Of course the more constructions we use, and the higher the values of r
and b that we pick, the larger the number of functions from the original family
that we are forced to use. Thus, the better the final family of functions is, the
longer it takes to apply the functions from this family.
Example 3.19 : Suppose we start with a family F. We use the AND-construction with r = 4 to produce a family F1. We then apply the OR-construction
to F1 with b = 4 to produce a third family F2. Note that the members of F2
each are built from 16 members of F, and the situation is analogous to starting
with 16 minhash functions and treating them as four bands of four rows each.
The 4-way AND-function converts any probability p into p
4
. When we
follow it by the 4-way OR-construction, that probability is further converted
into 1−(1−p
4
)
4
. Some values of this transformation are indicated in Fig. 3.11.
This function is an S-curve, staying low for a while, then rising steeply (although
not too steeply; the slope never gets much higher than 2), and then leveling
off at high values. Like any S-curve, it has a fixedpoint, the value of p that is
left unchanged when we apply the function of the S-curve. In this case, the
fixedpoint is the value of p for which p = 1 − (1 − p
4
)
4
. We can see that the
fixedpoint is somewhere between 0.7 and 0.8. Below that value, probabilities are
decreased, and above it they are increased. Thus, if we pick a high probabili
3.6. THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS 107
p 1 − (1 − p
4
)
4
0.2 0.0064
0.3 0.0320
0.4 0.0985
0.5 0.2275
0.6 0.4260
0.7 0.6666
0.8 0.8785
0.9 0.9860
Figure 3.11: Effect of the 4-way AND-construction followed by the 4-way ORconstruction
above the fixedpoint and a low probability below it, we shall have the desired
effect that the low probability is decreased and the high probability is increased.
Suppose F is the minhash functions, regarded as a (0.2, 0.6, 0.8, 0.4)-sensitive family. Then F2, the family constructed by a 4-way AND followed by a
4-way OR, is a (0.2, 0.6, 0.8785, 0.0985)-sensitive family, as we can read from the
rows for 0.8 and 0.4 in Fig. 3.11. By replacing F by F2, we have reduced both
the false-negative and false-positive rates, at the cost of making application of
the functions take 16 times as long. ✷
p

1 − (1 − p)
4
4
0.1 0.0140
0.2 0.1215
0.3 0.3334
0.4 0.5740
0.5 0.7725
0.6 0.9015
0.7 0.9680
0.8 0.9936
Figure 3.12: Effect of the 4-way OR-construction followed by the 4-way ANDconstruction
Example 3.20 : For the same cost, we can apply a 4-way OR-construction
followed by a 4-way AND-construction. Figure 3.12 gives the transformation
on probabilities implied by this construction. For instance, suppose that F is a
(0.2, 0.6, 0.8, 0.4)-sensitive family. Then the constructed family is a
(0.2, 0.6, 0.9936, 0.5740)-sensitiv
108 CHAPTER 3. FINDING SIMILAR ITEMS
family. This choice is not necessarily the best. Although the higher probability
has moved much closer to 1, the lower probability has also raised, increasing
the number of false positives. ✷
Example 3.21 : We can cascade constructions as much as we like. For example, we could use the construction of Example 3.19 on the family of minhash
functions and then use the construction of Example 3.20 on the resulting family.
The constructed family would then have functions each built from 256 minhash
functions. It would, for instance transform a (0.2, 0.8, 0.8, 0.2)-sensitive family
into a (0.2, 0.8, 0.9991285, 0.0000004)-sensitive family. ✷
3.6.4 Exercises for Section 3.6
Exercise 3.6.1 : What is the effect on probability of starting with the family
of minhash functions and applying:
(a) A 2-way AND construction followed by a 3-way OR construction.
(b) A 3-way OR construction followed by a 2-way AND construction.
(c) A 2-way AND construction followed by a 2-way OR construction, followed
by a 2-way AND construction.
(d) A 2-way OR construction followed by a 2-way AND construction, followed
by a 2-way OR construction followed by a 2-way AND construction.
Exercise 3.6.2 : Find the fixedpoints for each of the functions constructed in
Exercise 3.6.1.
! Exercise 3.6.3 : Any function of probability p, such as that of Fig. 3.11, has
a slope given by the derivative of the function. The maximum slope is where
that derivative is a maximum. Find the value of p that gives a maximum slope
for the S-curves given by Fig. 3.11 and Fig. 3.12. What are the values of these
maximum slopes?
!! Exercise 3.6.4 : Generalize Exercise 3.6.3 to give, as a function of r and b, the
point of maximum slope and the value of that slope, for families of functions
defined from the minhash functions by:
(a) An r-way AND construction followed by a b-way OR construction.
(b) A b-way OR construction followed by an r-way AND construction.
3.7 LSH Families for Other Distance Measures
There is no guarantee that a distance measure has a locality-sensitive family of
hash functions. So far, we have only seen such families for the Jaccard distance.
In this section, we shall show how to construct locality-sensitive families for
Hamming distance, the cosine distance and for the normal Euclidean distance.
3.7. LSH FAMILIES FOR OTHER DISTANCE MEASURES 109
3.7.1 LSH Families for Hamming Distance
It is quite simple to build a locality-sensitive family of functions for the Hamming distance. Suppose we have a space of d-dimensional vectors, and h(x, y)
denotes the Hamming distance between vectors x and y. If we take any one
position of the vectors, say the ith position, we can define the function fi(x)
to be the ith bit of vector x. Then fi(x) = fi(y) if and only if vectors x and
y agree in the ith position. Then the probability that fi(x) = fi(y) for a randomly chosen i is exactly 1 − h(x, y)/d; i.e., it is the fraction of positions in
which x and y agree.
This situation is almost exactly like the one we encountered for minhashing.
Thus, the family F consisting of the functions {f1, f2, . . . , fd} is a
(d1, d2, 1 − d1/d, 1 − d2/d)-sensitive
family of hash functions, for any d1 < d2. There are only two differences
between this family and the family of minhash functions.
1. While Jaccard distance runs from 0 to 1, the Hamming distance on a
vector space of dimension d runs from 0 to d. It is therefore necessary to
scale the distances by dividing by d, to turn them into probabilities.
2. While there is essentially an unlimited supply of minhash functions, the
size of the family F for Hamming distance is only d.
The first point is of no consequence; it only requires that we divide by d at
appropriate times. The second point is more serious. If d is relatively small,
then we are limited in the number of functions that can be composed using
the AND and OR constructions, thereby limiting how steep we can make the
S-curve be.
3.7.2 Random Hyperplanes and the Cosine Distance
Recall from Section 3.5.4 that the cosine distance between two vectors is the
angle between the vectors. For instance, we see in Fig. 3.13 two vectors x
and y that make an angle θ between them. Note that these vectors may be
in a space of many dimensions, but they always define a plane, and the angle
between them is measured in this plane. Figure 3.13 is a “top-view” of the
plane containing x and y.
Suppose we pick a hyperplane through the origin. This hyperplane intersects
the plane of x and y in a line. Figure 3.13 suggests two possible hyperplanes,
one whose intersection is the dashed line and the other’s intersection is the
dotted line. To pick a random hyperplane, we actually pick the normal vector
to the hyperplane, say v. The hyperplane is then the set of points whose dot
product with v is 0.
First, consider a vector v that is normal to the hyperplane whose projection
is represented by the dashed line in Fig. 3.13; that is, x and y are on different
110 CHAPTER 3. FINDING SIMILAR ITEMS
θ
x
y
Figure 3.13: Two vectors make an angle θ
sides of the hyperplane. Then the dot products v.x and v.y will have different
signs. If we assume, for instance, that v is a vector whose projection onto the
plane of x and y is above the dashed line in Fig. 3.13, then v.x is positive,
while v.y is negative. The normal vector v instead might extend in the opposite
direction, below the dashed line. In that case v.x is negative and v.y is positive,
but the signs are still different.
On the other hand, the randomly chosen vector v could be normal to a
hyperplane like the dotted line in Fig. 3.13. In that case, both v.x and v.y
have the same sign. If the projection of v extends to the right, then both dot
products are positive, while if v extends to the left, then both are negative.
What is the probability that the randomly chosen vector is normal to a
hyperplane that looks like the dashed line rather than the dotted line? All
angles for the line that is the intersection of the random hyperplane and the
plane of x and y are equally likely. Thus, the hyperplane will look like the
dashed line with probability θ/180 and will look like the dotted line otherwise.
Thus, each hash function f in our locality-sensitive family F is built from
a randomly chosen vector vf . Given two vectors x and y, say f(x) = f(y) if
and only if the dot products vf .x and vf .y have the same sign. Then F is a
locality-sensitive family for the cosine distance. The parameters are essentially
the same as for the Jaccard-distance family described in Section 3.6.2, except
the scale of distances is 0–180 rather than 0–1. That is, F is a
(d1, d2,(180 − d1)/180,(180 − d2)/180)-sensitive
family of hash functions. From this basis, we can amplify the family as we wish,
just as for the minhash-based family.
3.7. LSH FAMILIES FOR OTHER DISTANCE MEASURES 111
3.7.3 Sketches
Instead of chosing a random vector from all possible vectors, it turns out to be
sufficiently random if we restrict our choice to vectors whose components are
+1 and −1. The dot product of any vector x with a vector v of +1’s and −1’s
is formed by adding the components of x where v is +1 and then subtracting
the other components of x – those where v is −1.
If we pick a collection of random vectors, say v1, v2, . . . , vn, then we can
apply them to an arbitrary vector x by computing v1.x, v2.x, . . . , vn.x and then
replacing any positive value by +1 and any negative value by −1. The result is
called the sketch of x. You can handle 0’s arbitrarily, e.g., by chosing a result +1
or −1 at random. Since there is only a tiny probability of a zero dot product,
the choice has essentially no effect.
Example 3.22 : Suppose our space consists of 4-dimensional vectors, and we
pick three random vectors: v1 = [+1, −1, +1, +1], v2 = [−1, +1, −1, +1], and
v3 = [+1, +1, −1, −1]. For the vector x = [3, 4, 5, 6], the sketch is [+1, +1, −1].
That is, v1.x = 3−4+5+6 = 10. Since the result is positive, the first component
of the sketch is +1. Similarly, v2.x = 2 and v3.x = −4, so the second component
of the sketch is +1 and the third component is −1.
Consider the vector y = [4, 3, 2, 1]. We can similarly compute its sketch to
be [+1, −1, +1]. Since the sketches for x and y agree in 1/3 of the positions,
we estimate that the angle between them is 120 degrees. That is, a randomly
chosen hyperplane is twice as likely to look like the dashed line in Fig. 3.13 than
like the dotted line.
The above conclusion turns out to be quite wrong. We can calculate the
cosine of the angle between x and y to be x.y, which is
6 × 1 + 5 × 2 + 4 × 3 + 3 × 4 = 40
divided by the magnitudes of the two vectors. These magnitudes are
p
6
2 + 52 + 42 + 32 = 9.274
and √
1
2 + 22 + 32 + 42 = 5.477. Thus, the cosine of the angle between x and
y is 0.7875, and this angle is about 38 degrees. However, if you look at all
16 different vectors v of length 4 that have +1 and −1 as components, you
find that there are only four of these whose dot products with x and y have
a different sign, namely v2, v3, and their complements [+1, −1, +1, −1] and
[−1, −1, +1, +1]. Thus, had we picked all sixteen of these vectors to form a
sketch, the estimate of the angle would have been 180/4 = 45 degrees. ✷
3.7.4 LSH Families for Euclidean Distance
Now, let us turn to the Euclidean distance (Section 3.5.2), and see if we can
develop a locality-sensitive family of hash functions for this distance. We shall
start with a 2-dimensional Euclidean space. Each hash function f in our family
112 CHAPTER 3. FINDING SIMILAR ITEMS
F will be associated with a randomly chosen line in this space. Pick a constant
a and divide the line into segments of length a, as suggested by Fig. 3.14, where
the “random” line has been oriented to be horizontal.
θ
Points at
distance
Bucket
width a
d
Figure 3.14: Two points at distance d ≫ a have a small chance of being hashed
to the same bucket
The segments of the line are the buckets into which function f hashes points.
A point is hashed to the bucket in which its projection onto the line lies. If the
distance d between two points is small compared with a, then there is a good
chance the two points hash to the same bucket, and thus the hash function f
will declare the two points equal. For example, if d = a/2, then there is at least
a 50% chance the two points will fall in the same bucket. In fact, if the angle
θ between the randomly chosen line and the line connecting the points is large,
then there is an even greater chance that the two points will fall in the same
bucket. For instance, if θ is 90 degrees, then the two points are certain to fall
in the same bucket.
However, suppose d is larger than a. In order for there to be any chance of
the two points falling in the same bucket, we need d cos θ ≤ a. The diagram of
Fig. 3.14 suggests why this requirement holds. Note that even if d cos θ ≪ a it
is still not certain that the two points will fall in the same bucket. However,
we can guarantee the following. If d ≥ 2a, then there is no more than a 1/3
chance the two points fall in the same bucket. The reason is that for cos θ to
be less than 1/2, we need to have θ in the range 60 to 90 degrees. If θ is in the
range 0 to 60 degrees, then cos θ is more than 1/2. But since θ is the smaller
angle between two randomly chosen lines in the plane, θ is twice as likely to be
between 0 and 60 as it is to be between 60 and 90.
We conclude that the family F just described forms a (a/2, 2a, 1/2, 1/3)-
sensitive family of hash functions. That is, for distances up to a/2 the probability is at least 1/2 that two points at that distance will fall in the same bucket,
while for distances at least 2a the probability points at that distance will fall in
3.7. LSH FAMILIES FOR OTHER DISTANCE MEASURES 113
the same bucket is at most 1/3. We can amplify this family as we like, just as
for the other examples of locality-sensitive hash functions we have discussed.
3.7.5 More LSH Families for Euclidean Spaces
There is something unsatisfying about the family of hash functions developed
in Section 3.7.4. First, the technique was only described for two-dimensional
Euclidean spaces. What happens if our data is points in a space with many
dimensions? Second, for Jaccard and cosine distances, we were able to develop
locality-sensitive families for any pair of distances d1 and d2 as long as d1 < d2.
In Section 3.7.4 we appear to need the stronger condition d1 < 4d2.
However, we claim that there is a locality-sensitive family of hash functions for any d1 < d2 and for any number of dimensions. The family’s hash
functions still derive from random lines through the space and a bucket size
a that partitions the line. We still hash points by projecting them onto the
line. Given that d1 < d2, we may not know what the probability p1 is that two
points at distance d1 hash to the same bucket, but we can be certain that it
is greater than p2, the probability that two points at distance d2 hash to the
same bucket. The reason is that this probability surely grows as the distance
shrinks. Thus, even if we cannot calculate p1 and p2 easily, we know that there
is a (d1, d2, p1, p2)-sensitive family of hash functions for any d1 < d2 and any
given number of dimensions.
Using the amplification techniques of Section 3.6.3, we can then adjust the
two probabilities to surround any particular value we like, and to be as far apart
as we like. Of course, the further apart we want the probabilities to be, the
larger the number of basic hash functions in F we must use.
3.7.6 Exercises for Section 3.7
Exercise 3.7.1 : Suppose we construct the basic family of six locality-sensitive
functions for vectors of length six. For each pair of the vectors 000000, 110011,
010101, and 011100, which of the six functions makes them candidates?
Exercise 3.7.2 : Let us compute sketches using the following four “random”
vectors:
v1 = [+1, +1, +1, −1] v2 = [+1, +1, −1, +1]
v3 = [+1, −1, +1, +1] v4 = [−1, +1, +1, +1]
Compute the sketches of the following vectors.
(a) [2, 3, 4, 5].
(b) [−2, 3, −4, 5].
(c) [2, −3, 4, −5].
114 CHAPTER 3. FINDING SIMILAR ITEMS
For each pair, what is the estimated angle between them, according to the
sketches? What are the true angles?
Exercise 3.7.3 : Suppose we form sketches by using all sixteen of the vectors
of length 4, whose components are each +1 or −1. Compute the sketches of
the three vectors in Exercise 3.7.2. How do the estimates of the angles between
each pair compare with the true angles?
Exercise 3.7.4 : Suppose we form sketches using the four vectors from Exercise 3.7.2.
! (a) What are the constraints on a, b, c, and d that will cause the sketch of
the vector [a, b, c, d] to be [+1, +1, +1, +1]?
!! (b) Consider two vectors [a, b, c, d] and [e, f, g, h]. What are the conditions on
a, b, . . . , h that will make the sketches of these two vectors be the same?
Exercise 3.7.5 : Suppose we have points in a 3-dimensional Euclidean space:
p1 = (1, 2, 3), p2 = (0, 2, 4), and p3 = (4, 3, 2). Consider the three hash functions
defined by the three axes (to make our calculations very easy). Let buckets be
of length a, with one bucket the interval [0, a) (i.e., the set of points x such that
0 ≤ x < a), the next [a, 2a), the previous one [−a, 0), and so on.
(a) For each of the three lines, assign each of the points to buckets, assuming
a = 1.
(b) Repeat part (a), assuming a = 2.
(c) What are the candidate pairs for the cases a = 1 and a = 2?
! (d) For each pair of points, for what values of a will that pair be a candidate
pair?
3.8 Applications of Locality-Sensitive Hashing
In this section, we shall explore three examples of how LSH is used in practice.
In each case, the techniques we have learned must be modified to meet certain
constraints of the problem. The three subjects we cover are:
1. Entity Resolution: This term refers to matching data records that refer to
the same real-world entity, e.g., the same person. The principal problem
addressed here is that the similarity of records does not match exactly
either the similar-sets or similar-vectors models of similarity on which the
theory is built.
2. Matching Fingerprints: It is possible to represent fingerprints as sets.
However, we shall explore a different family of locality-sensitive hash functions from the one we get by minhashing.
3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 115
3. Matching Newspaper Articles: Here, we consider a different notion of
shingling that focuses attention on the core article in an on-line newspaper’s Web page, ignoring all the extraneous material such as ads and
newspaper-specific material.
3.8.1 Entity Resolution
It is common to have several data sets available, and to know that they refer to
some of the same entities. For example, several different bibliographic sources
provide information about many of the same books or papers. In the general
case, we have records describing entities of some type, such as people or books.
The records may all have the same format, or they may have different formats,
with different kinds of information.
There are many reasons why information about an entity may vary, even if
the field in question is supposed to be the same. For example, names may be
expressed differently in different records because of misspellings, absence of a
middle initial, use of a nickname, and many other reasons. For example, “Bob
S. Jomes” and “Robert Jones Jr.” may or may not be the same person. If
records come from different sources, the fields may differ as well. One source’s
records may have an “age” field, while another does not. The second source
might have a “date of birth” field, or it may have no information at all about
when a person was born.
3.8.2 An Entity-Resolution Example
We shall examine a real example of how LSH was used to deal with an entityresolution problem. Company A was engaged by Company B to solicit customers for B. Company B would pay A a yearly fee, as long as the customer
maintained their subscription. They later quarreled and disagreed over how
many customers A had provided to B. Each had about 1,000,000 records, some
of which described the same people; those were the customers A had provided
to B. The records had different data fields, but unfortunately none of those
fields was “this is a customer that A had provided to B.” Thus, the problem
was to match records from the two sets to see if a pair represented the same
person.
Each record had fields for the name, address, and phone number of the
person. However, the values in these fields could differ for many reasons. Not
only were there the misspellings and other naming differences mentioned in
Section 3.8.1, but there were other opportunities to disagree as well. A customer
might give their home phone to A and their cell phone to B. Or they might
move, and tell B but not A (because they no longer had need for a relationship
with A). Area codes of phones sometimes change.
The strategy for identifying records involved scoring the differences in three
fields: name, address, and phone. To create a score describing the likelihood
that two records, one from A and the other from B, described the same per-
116 CHAPTER 3. FINDING SIMILAR ITEMS
son, 100 points was assigned to each of the three fields, so records with exact
matches in all three fields got a score of 300. However, there were deductions for
mismatches in each of the three fields. As a first approximation, edit-distance
(Section 3.5.5) was used, but the penalty grew quadratically with the distance.
Then, certain publicly available tables were used to reduce the penalty in appropriate situations. For example, “Bill” and “William” were treated as if they
differed in only one letter, even though their edit-distance is 5.
However, it is not feasible to score all one trillion pairs of records. Thus,
a simple LSH was used to focus on likely candidates. Three “hash functions”
were used. The first sent records to the same bucket only if they had identical
names; the second did the same but for identical addresses, and the third did
the same for phone numbers. In practice, there was no hashing; rather the
records were sorted by name, so records with identical names would appear
consecutively and get scored for overall similarity of the name, address, and
phone. Then the records were sorted by address, and those with the same
address were scored. Finally, the records were sorted a third time by phone,
and records with identical phones were scored.
This approach missed a record pair that truly represented the same person
but none of the three fields matched exactly. Since the goal was to prove in
a court of law that the persons were the same, it is unlikely that such a pair
would have been accepted by a judge as sufficiently similar anyway.
3.8.3 Validating Record Matches
What remains is to determine how high a score indicates that two records truly
represent the same individual. In the example at hand, there was an easy
way to make that decision, and the technique can be applied in many similar
situations. It was decided to look at the creation-dates for the records at hand,
and to assume that 90 days was an absolute maximum delay between the time
the service was bought at Company A and registered at B. Thus, a proposed
match between two records that were chosen at random, subject only to the
constraint that the date on the B-record was between 0 and 90 days after the
date on the A-record, would have an average delay of 45 days.
It was found that of the pairs with a perfect 300 score, the average delay was
10 days. If you assume that 300-score pairs are surely correct matches, then you
can look at the pool of pairs with any given score s, and compute the average
delay of those pairs. Suppose that the average delay is x, and the fraction of
true matches among those pairs with score s is f. Then x = 10f + 45(1 − f),
or x = 45 − 35f. Solving for f, we find that the fraction of the pairs with score
s that are truly matches is (45 − x)/35.
The same trick can be used whenever:
1. There is a scoring system used to evaluate the likelihood that two records
represent the same entity, and
3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 117
When Are Record Matches Good Enough?
While every case will be different, it may be of interest to know how the
experiment of Section 3.8.3 turned out on the data of Section 3.8.2. For
scores down to 185, the value of x was very close to 10; i.e., these scores
indicated that the likelihood of the records representing the same person
was essentially 1. Note that a score of 185 in this example represents a
situation where one field is the same (as would have to be the case, or the
records would never even be scored), one field was completely different,
and the third field had a small discrepancy. Moreover, for scores as low as
115, the value of x was noticeably less than 45, meaning that some of these
pairs did represent the same person. Note that a score of 115 represents
a case where one field is the same, but there is only a slight similarity in
the other two fields.
2. There is some field, not used in the scoring, from which we can derive a
measure that differs, on average, for true pairs and false pairs.
For instance, suppose there were a “height” field recorded by both companies
A and B in our running example. We can compute the average difference in
height for pairs of random records, and we can compute the average difference in
height for records that have a perfect score (and thus surely represent the same
entities). For a given score s, we can evaluate the average height difference of the
pairs with that score and estimate the probability of the records representing
the same entity. That is, if h0 is the average height difference for the perfect
matches, h1 is the average height difference for random pairs, and h is the
average height difference for pairs of score s, then the fraction of good pairs
with score s is (h1 − h)/(h1 − h0).
3.8.4 Matching Fingerprints
When fingerprints are matched by computer, the usual representation is not
an image, but a set of locations in which minutiae are located. A minutia,
in the context of fingerprint descriptions, is a place where something unusual
happens, such as two ridges merging or a ridge ending. If we place a grid over a
fingerprint, we can represent the fingerprint by the set of grid squares in which
minutiae are located.
Ideally, before overlaying the grid, fingerprints are normalized for size and
orientation, so that if we took two images of the same finger, we would find
minutiae lying in exactly the same grid squares. We shall not consider here
the best ways to normalize images. Let us assume that some combination of
techniques, including choice of grid size and placing a minutia in several adjacent
grid squares if it lies close to the border of the squares enables us to assume
118 CHAPTER 3. FINDING SIMILAR ITEMS
that grid squares from two images have a significantly higher probability of
agreeing in the presence or absence of a minutia than if they were from images
of different fingers.
Thus, fingerprints can be represented by sets of grid squares – those where
their minutiae are located – and compared like any sets, using the Jaccard similarity or distance. There are two versions of fingerprint comparison, however.
• The many-one problem is the one we typically expect. A fingerprint has
been found on a gun, and we want to compare it with all the fingerprints
in a large database, to see which one matches.
• The many-many version of the problem is to take the entire database, and
see if there are any pairs that represent the same individual.
While the many-many version matches the model that we have been following
for finding similar items, the same technology can be used to speed up the
many-one problem.
3.8.5 A LSH Family for Fingerprint Matching
We could minhash the sets that represent a fingerprint, and use the standard
LSH technique from Section 3.4. However, since the sets are chosen from a
relatively small set of grid points (perhaps 1000), the need to minhash them
into more succinct signatures is not clear. We shall study here another form of
locality-sensitive hashing that works well for data of the type we are discussing.
Suppose for an example that the probability of finding a minutia in a random
grid square of a random fingerprint is 20%. Also, assume that if two fingerprints
come from the same finger, and one has a minutia in a given grid square, then
the probability that the other does too is 80%. We can define a locality-sensitive
family of hash functions as follows. Each function f in this family F is defined
by three grid squares. Function f says “yes” for two fingerprints if both have
minutiae in all three grid squares, and otherwise f says “no.” Put another
way, we may imagine that f sends to a single bucket all fingerprints that have
minutiae in all three of f’s grid points, and sends each other fingerprint to a
bucket of its own. In what follows, we shall refer to the first of these buckets as
“the” bucket for f and ignore the buckets that are required to be singletons.
If we want to solve the many-one problem, we can use many functions from
the family F and precompute their buckets of fingerprints to which they answer
“yes.” Then, given a new fingerprint that we want to match, we determine
which of these buckets it belongs to and compare it with all the fingerprints
found in any of those buckets. To solve the many-many problem, we compute
the buckets for each of the functions and compare all fingerprints in each of the
buckets.
Let us consider how many functions we need to get a reasonable probability
of catching a match, without having to compare the fingerprint on the gun with
each of the millions of fingerprints in the database. First, the probability that
3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 119
two fingerprints from different fingers would be in the bucket for a function f
in F is (0.2)6 = 0.000064. The reason is that they will both go into the bucket
only if they each have a minutia in each of the three grid points associated with
f, and the probability of each of those six independent events is 0.2.
Now, consider the probability that two fingerprints from the same finger
wind up in the bucket for f. The probability that the first fingerprint has
minutiae in each of the three squares belonging to f is (0.2)3 = 0.008. However,
if it does, then the probability is (0.8)3 = 0.512 that the other fingerprint
will as well. Thus, if the fingerprints are from the same finger, there is a
0.008 × 0.512 = 0.004096 probability that they will both be in the bucket of f.
That is not much; it is about one in 200. However, if we use many functions
from F, but not too many, then we can get a good probability of matching
fingerprints from the same finger while not having too many false positives –
fingerprints that must be considered but do not match.
Example 3.23 : For a specific example, let us suppose that we use 1024
functions chosen randomly from F. Next, we shall construct a new family F1 by performing a 1024-way OR on F. Then the probability that F1
will put fingerprints from the same finger together in at least one bucket is
1 − (1 − 0.004096)1024 = 0.985. On the other hand, the probability that
two fingerprints from different fingers will be placed in the same bucket is
(1 − (1 − 0.000064)1024 = 0.063. That is, we get about 1.5% false negatives
and about 6.3% false positives. ✷
The result of Example 3.23 is not the best we can do. While it offers only a
1.5% chance that we shall fail to identify the fingerprint on the gun, it does force
us to look at 6.3% of the entire database. Increasing the number of functions
from F will increase the number of false positives, with only a small benefit
of reducing the number of false negatives below 1.5%. On the other hand, we
can also use the AND construction, and in so doing, we can greatly reduce
the probability of a false positive, while making only a small increase in the
false-negative rate. For instance, we could take 2048 functions from F in two
groups of 1024. Construct the buckets for each of the functions. However, given
a fingerprint P on the gun:
1. Find the buckets from the first group in which P belongs, and take the
union of these buckets.
2. Do the same for the second group.
3. Take the intersection of the two unions.
4. Compare P only with those fingerprints in the intersection.
Note that we still have to take unions and intersections of large sets of fingerprints, but we compare only a small fraction of those. It is the comparison of
120 CHAPTER 3. FINDING SIMILAR ITEMS
fingerprints that takes the bulk of the time; in steps (1) and (2) fingerprints
can be represented by their integer indices in the database.
If we use this scheme, the probability of detecting a matching fingerprint
is (0.985)2 = 0.970; that is, we get about 3% false negatives. However, the
probability of a false positive is (0.063)2 = 0.00397. That is, we only have to
examine about 1/250th of the database.
3.8.6 Similar News Articles
Our last case study concerns the problem of organizing a large repository of
on-line news articles by grouping together Web pages that were derived from
the same basic text. It is common for organizations like The Associated Press
to produce a news item and distribute it to many newspapers. Each newspaper
puts the story in its on-line edition, but surrounds it by information that is
special to that newspaper, such as the name and address of the newspaper,
links to related articles, and links to ads. In addition, it is common for the
newspaper to modify the article, perhaps by leaving off the last few paragraphs
or even deleting text from the middle. As a result, the same news article can
appear quite different at the Web sites of different newspapers.
The problem looks very much like the one that was suggested in Section 3.4:
find documents whose shingles have a high Jaccard similarity. Note that this
problem is different from the problem of finding news articles that tell about the
same events. The latter problem requires other techniques, typically examining
the set of important words in the documents (a concept we discussed briefly
in Section 1.3.1) and clustering them to group together different articles about
the same topic.
However, an interesting variation on the theme of shingling was found to be
more effective for data of the type described. The problem is that shingling as
we described it in Section 3.2 treats all parts of a document equally. However,
we wish to ignore parts of the document, such as ads or the headlines of other
articles to which the newspaper added a link, that are not part of the news
article. It turns out that there is a noticeable difference between text that
appears in prose and text that appears in ads or headlines. Prose has a much
greater frequency of stop words, the very frequent words such as “the” or “and.”
The total number of words that are considered stop words varies with the
application, but it is common to use a list of several hundred of the most
frequent words.
Example 3.24 : A typical ad might say simply “Buy Sudzo.” On the other
hand, a prose version of the same thought that might appear in an article is
“I recommend that you buy Sudzo for your laundry.” In the latter sentence, it
would be normal to treat “I,” “that,” “you,” “for,” and “your” as stop words.
✷
Suppose we define a shingle to be a stop word followed by the next two
words. Then the ad “Buy Sudzo” from Example 3.24 has no shingles and
3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 121
would not be reflected in the representation of the Web page containing that
ad. On the other hand, the sentence from Example 3.24 would be represented
by five shingles: “I recommend that,” “that you buy,” “you buy Sudzo,” “for
your laundry,” and “your laundry x,” where x is whatever word follows that
sentence.
Suppose we have two Web pages, each of which consists of half news text
and half ads or other material that has a low density of stop words. If the news
text is the same but the surrounding material is different, then we would expect
that a large fraction of the shingles of the two pages would be the same. They
might have a Jaccard similarity of 75%. However, if the surrounding material
is the same but the news content is different, then the number of common
shingles would be small, perhaps 25%. If we were to use the conventional
shingling, where shingles are (say) sequences of 10 consecutive characters, we
would expect the two documents to share half their shingles (i.e., a Jaccard
similarity of 1/3), regardless of whether it was the news or the surrounding
material that they shared.
3.8.7 Exercises for Section 3.8
Exercise 3.8.1 : Suppose we are trying to perform entity resolution among
bibliographic references, and we score pairs of references based on the similarities of their titles, list of authors, and place of publication. Suppose also that
all references include a year of publication, and this year is equally likely to be
any of the ten most recent years. Further, suppose that we discover that among
the pairs of references with a perfect score, there is an average difference in the
publication year of 0.1.7 Suppose that the pairs of references with a certain
score s are found to have an average difference in their publication dates of 2.
What is the fraction of pairs with score s that truly represent the same publication? Note: Do not make the mistake of assuming the average difference
in publication date between random pairs is 5 or 5.5. You need to calculate it
exactly, and you have enough information to do so.
Exercise 3.8.2 : Suppose we use the family F of functions described in Section 3.8.5, where there is a 20% chance of a minutia in an grid square, an 80%
chance of a second copy of a fingerprint having a minutia in a grid square where
the first copy does, and each function in F being formed from three grid squares.
In Example 3.23, we constructed family F1 by using the OR construction on
1024 members of F. Suppose we instead used family F2 that is a 2048-way OR
of members of F.
(a) Compute the rates of false positives and false negatives for F2.
(b) How do these rates compare with what we get if we organize the same
2048 functions into a 2-way AND of members of F1, as was discussed at
the end of Section 3.8.5?
7We might expect the average to be 0, but in practice, errors in publication year do occur.
122 CHAPTER 3. FINDING SIMILAR ITEMS
Exercise 3.8.3 : Suppose fingerprints have the same statistics outlined in Exercise 3.8.2, but we use a base family of functions F
′
defined like F, but using
only two randomly chosen grid squares. Construct another set of functions F
′
1
from F
′
by taking the n-way OR of functions from F
′
. What, as a function of
n, are the false positive and false negative rates for F
′
1
?
Exercise 3.8.4 : Suppose we use the functions F1 from Example 3.23, but we
want to solve the many-many problem.
(a) If two fingerprints are from the same finger, what is the probability that
they will not be compared (i.e., what is the false negative rate)?
(b) What fraction of the fingerprints from different fingers will be compared
(i.e., what is the false positive rate)?
! Exercise 3.8.5 : Assume we have the set of functions F as in Exercise 3.8.2,
and we construct a new set of functions F3 by an n-way OR of functions in
F. For what value of n is the sum of the false positive and false negative rates
minimized?
3.9 Methods for High Degrees of Similarity
LSH-based methods appear most effective when the degree of similarity we
accept is relatively low. When we want to find sets that are almost identical,
there are other methods that can be faster. Moreover, these methods are exact,
in that they find every pair of items with the desired degree of similarity. There
are no false negatives, as there can be with LSH.
3.9.1 Finding Identical Items
The extreme case is finding identical items, for example, Web pages that are
identical, character-for-character. It is straightforward to compare two documents and tell whether they are identical, but we still must avoid having to
compare every pair of documents. Our first thought would be to hash documents based on their first few characters, and compare only those documents
that fell into the same bucket. That scheme should work well, unless all the
documents begin with the same characters, such as an HTML header.
Our second thought would be to use a hash function that examines the
entire document. That would work, and if we use enough buckets, it would be
very rare that two documents went into the same bucket, yet were not identical.
The downside of this approach is that we must examine every character of every
document. If we limit our examination to a small number of characters, then
we never have to examine a document that is unique and falls into a bucket of
its own.
A better approach is to pick some fixed random positions for all documents,
and make the hash function depend only on these. This way, we can avoid
3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 123
a problem where there is a common prefix for all or most documents, yet we
need not examine entire documents unless they fall into a bucket with another
document. One problem with selecting fixed positions is that if some documents
are short, they may not have some of the selected positions. However, if we are
looking for highly similar documents, we never need to compare two documents
that differ significantly in their length. We exploit this idea in Section 3.9.3.
3.9.2 Representing Sets as Strings
Now, let us focus on the harder problem of finding, in a large collection of sets,
all pairs that have a high Jaccard similarity, say at least 0.9. We can represent
a set by sorting the elements of the universal set in some fixed order, and
representing any set by listing its elements in this order. The list is essentially
a string of “characters,” where the characters are the elements of the universal
set. These strings are unusual, however, in that:
1. No character appears more than once in a string, and
2. If two characters appear in two different strings, then they appear in the
same order in both strings.
Example 3.25 : Suppose the universal set consists of the 26 lower-case letters,
and we use the normal alphabetical order. Then the set {d, a, b} is represented
by the string abd. ✷
In what follows, we shall assume all strings represent sets in the manner just
described. Thus, we shall talk about the Jaccard similarity of strings, when
strictly speaking we mean the similarity of the sets that the strings represent.
Also, we shall talk of the length of a string, as a surrogate for the number of
elements in the set that the string represents.
Note that the documents discussed in Section 3.9.1 do not exactly match
this model, even though we can see documents as strings. To fit the model,
we would shingle the documents, assign an order to the shingles, and represent
each document by its list of shingles in the selected order.
3.9.3 Length-Based Filtering
The simplest way to exploit the string representation of Section 3.9.2 is to sort
the strings by length. Then, each string s is compared with those strings t that
follow s in the list, but are not too long. Suppose the lower bound on Jaccard
similarity between two strings is J. For any string x, denote its length by Lx.
Note that Ls ≤ Lt. The intersection of the sets represented by s and t cannot
have more than Ls members, while their union has at least Lt members. Thus,
the Jaccard similarity of s and t, which we denote SIM(s, t), is at most Ls/Lt.
That is, in order for s and t to require comparison, it must be that J ≤ Ls/Lt,
or equivalently, Lt ≤ Ls/J.
124 CHAPTER 3. FINDING SIMILAR ITEMS
A Better Ordering for Symbols
Instead of using the obvious order for elements of the universal set, e.g.,
lexicographic order for shingles, we can order symbols rarest first. That
is, determine how many times each element appears in the collection of
sets, and order them by this count, lowest first. The advantage of doing
so is that the symbols in prefixes will tend to be rare. Thus, they will
cause that string to be placed in index buckets that have relatively few
members. Then, when we need to examine a string for possible matches,
we shall find few other strings that are candidates for comparison.
Example 3.26 : Suppose that s is a string of length 9, and we are looking for
strings with at least 0.9 Jaccard similarity. Then we have only to compare s
with strings following it in the length-based sorted order that have length at
most 9/0.9 = 10. That is, we compare s with those strings of length 9 that
follow it in order, and all strings of length 10. We have no need to compare s
with any other string.
Suppose the length of s were 8 instead. Then s would be compared with
following strings of length up to 8/0.9 = 8.89. That is, a string of length 9
would be too long to have a Jaccard similarity of 0.9 with s, so we only have to
compare s with the strings that have length 8 but follow it in the sorted order.
✷
3.9.4 Prefix Indexing
In addition to length, there are several other features of strings that can be
exploited to limit the number of comparisons that must be made to identify
all pairs of similar strings. The simplest of these options is to create an index
for each symbol; recall a symbol of a string is any one of the elements of the
universal set. For each string s, we select a prefix of s consisting of the first p
symbols of s. How large p must be depends on Ls and J, the lower bound on
Jaccard similarity. We add string s to the index for each of its first p symbols.
In effect, the index for each symbol becomes a bucket of strings that must be
compared. We must be certain that any other string t such that SIM(s, t) ≥ J
will have at least one symbol in its prefix that also appears in the prefix of s.
Suppose not; rather SIM(s, t) ≥ J, but t has none of the first p symbols of
s. Then the highest Jaccard similarity that s and t can have occurs when t is
a suffix of s, consisting of everything but the first p symbols of s. The Jaccard
similarity of s and t would then be (Ls − p)/Ls. To be sure that we do not
have to compare s with t, we must be certain that J > (Ls − p)/Ls. That
is, p must be at least ⌊(1 − J)Ls⌋ + 1. Of course we want p to be as small as
possible, so we do not index string s in more buckets than we need to. Thus,
we shall hereafter take p = ⌊(1 − J)Ls⌋ + 1 to be the length of the prefix that
3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 125
gets indexed.
Example 3.27 : Suppose J = 0.9. If Ls = 9, then p = ⌊0.1 × 9⌋ + 1 =
⌊0.9⌋ + 1 = 1. That is, we need to index s under only its first symbol. Any
string t that does not have the first symbol of s in a position such that t is
indexed by that symbol will have Jaccard similarity with s that is less than 0.9.
Suppose s is bcdefghij. Then s is indexed under b only. Suppose t does not
begin with b. There are two cases to consider.
1. If t begins with a, and SIM(s, t) ≥ 0.9, then it can only be that t is
abcdefghij. But if that is the case, t will be indexed under both a and
b. The reason is that Lt = 10, so t will be indexed under the symbols of
its prefix of length ⌊0.1 × 10⌋ + 1 = 2.
2. If t begins with c or a later letter, then the maximum value of SIM(s, t)
occurs when t is cdefghij. But then SIM(s, t) = 8/9 < 0.9.
In general, with J = 0.9, strings of length up to 9 are indexed by their first
symbol, strings of lengths 10–19 are indexed under their first two symbols,
strings of length 20–29 are indexed under their first three symbols, and so on.
✷
We can use the indexing scheme in two ways, depending on whether we
are trying to solve the many-many problem or a many-one problem; recall the
distinction was introduced in Section 3.8.4. For the many-one problem, we
create the index for the entire database. To query for matches to a new set
S, we convert that set to a string s, which we call the probe string. Determine
the length of the prefix that must be considered, that is, ⌊(1 − J)Ls⌋ + 1. For
each symbol appearing in one of the prefix positions of s, we look in the index
bucket for that symbol, and we compare s with all the strings appearing in that
bucket.
If we want to solve the many-many problem, start with an empty database
of strings and indexes. For each set S, we treat S as a new set for the many-one
problem. We convert S to a string s, which we treat as a probe string in the
many-one problem. However, after we examine an index bucket, we also add s
to that bucket, so s will be compared with later strings that could be matches.
3.9.5 Using Position Information
Consider the strings s = acdefghijk and t = bcdefghijk, and assume J = 0.9.
Since both strings are of length 10, they are indexed under their first two
symbols. Thus, s is indexed under a and c, while t is indexed under b and c.
Whichever is added last will find the other in the bucket for c, and they will be
compared. However, since c is the second symbol of both, we know there will
be two symbols, a and b in this case, that are in the union of the two sets but
not in the intersection. Indeed, even though s and t are identical from c to the
126 CHAPTER 3. FINDING SIMILAR ITEMS
end, their intersection is 9 symbols and their union is 11; thus SIM(s, t) = 9/11,
which is less than 0.9.
If we build our index based not only on the symbol, but on the position of
the symbol within the string, we could avoid comparing s and t above. That
is, let our index have a bucket for each pair (x, i), containing the strings that
have symbol x in position i of their prefix. Given a string s, and assuming J is
the minimum desired Jaccard similarity, we look at the prefix of s, that is, the
positions 1 through ⌊(1 − J)Ls⌋ + 1. If the symbol in position i of the prefix is
x, add s to the index bucket for (x, i).
Now consider s as a probe string. With what buckets must it be compared?
We shall visit the symbols of the prefix of s from the left, and we shall take
advantage of the fact that we only need to find a possible matching string t if
none of the previous buckets we have examined for matches held t. That is, we
only need to find a candidate match once. Thus, if we find that the ith symbol
of s is x, then we need look in the bucket (x, j) for certain small values of j.
j
s
t
Symbols definitely
appearing in
only one string
i
Figure 3.15: Strings s and t begin with i − 1 and j − 1 unique symbols, respectively, and then agree beyond that
To compute the upper bound on j, suppose t is a string none of whose first
j −1 symbols matched anything in s, but the ith symbol of s is the same as the
jth symbol of t. The highest value of SIM(s, t) occurs if s and t are identical
beyond their ith and jth symbols, respectively, as suggested by Fig. 3.15. If
that is the case, the size of their intersection is Ls − i + 1, since that is the
number of symbols of s that could possibly be in t. The size of their union is
at least Ls + j − 1. That is, s surely contributes Ls symbols to the union, and
there are also at least j −1 symbols of t that are not in s. The ratio of the sizes
of the intersection and union must be at least J, so we must have:
Ls − i + 1
Ls + j − 1
≥ J
If we isolate j in this inequality, we have j ≤

Ls(1 − J) − i + 1 + J

/J.
Example 3.28 : Consider the string s = acdefghijk with J = 0.9 discussed
at the beginning of this section. Suppose s is now a probe string. We already
established that we need to consider the first two positions; that is, i can be
3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 127
or 2. Suppose i = 1. Then j ≤ (10 × 0.1 − 1 + 1 + 0.9)/0.9. That is, we only
have to compare the symbol a with strings in the bucket for (a, j) if j ≤ 2.11.
Thus, j can be 1 or 2, but nothing higher.
Now suppose i = 2. Then we require j ≤ (10 × 0.1 − 2 + 1 + 0.9)/0.9, Or
j ≤ 1. We conclude that we must look in the buckets for (a, 1), (a, 2), and (c, 1),
but in no other bucket. In comparison, using the buckets of Section 3.9.4, we
would look into the buckets for a and c, which is equivalent to looking to all
buckets (a, j) and (c, j) for any j. ✷
3.9.6 Using Position and Length in Indexes
When we considered the upper limit on j in the previous section, we assumed
that what follows positions i and j were as in Fig. 3.15, where what followed
these positions in strings s and t matched exactly. We do not want to build an
index that involves every symbol in the strings, because that makes the total
work excessive. However, we can add to our index a summary of what follows
the positions being indexed. Doing so expands the number of buckets, but not
beyond reasonable bounds, and yet enables us to eliminate many candidate
matches without comparing entire strings. The idea is to use index buckets
corresponding to a symbol, a position, and the suffix length, that is, the number
of symbols following the position in question.
Example 3.29 : The string s = acdefghijk, with J = 0.9, would be indexed
in the buckets for (a, 1, 9) and (c, 2, 8). That is, the first position of s has symbol
a, and its suffix is of length 9. The second position has symbol c and its suffix
is of length 8. ✷
Figure 3.15 assumes that the suffixes for position i of s and position j of t
have the same length. If not, then we can either get a smaller upper bound on
the size of the intersection of s and t (if t is shorter) or a larger lower bound
on the size of the union (if t is longer). Suppose s has suffix length p and t has
suffix length q.
Case 1: p ≥ q. Here, the maximum size of the intersection is
Ls − i + 1 − (p − q)
Since Ls = i + p, we can write the above expression for the intersection size as
q + 1. The minimum size of the union is Ls + j − 1, as it was when we did not
take suffix length into account. Thus, we require
q + 1
Ls + j − 1
≥ J
whenever p ≥ q.
128 CHAPTER 3. FINDING SIMILAR ITEMS
Case 2: p < q. Here, the maximum size of the intersection is Ls − i + 1, as
when suffix length was not considered. However, the minimum size of the union
is now Ls + j − 1 + q − p. If we again use the relationship Ls = i + p, we can
replace Ls − p by i and get the formula i + j − 1 + q for the size of the union.
If the Jaccard similarity is at least J, then
Ls − i + 1
i + j − 1 + q
≥ J
whenever p < q.
Example 3.30 : Let us again consider the string s = acdefghijk, but to make
the example show some details, let us choose J = 0.8 instead of 0.9. We know
that Ls = 10. Since ⌊(1 − J)Ls⌋ + 1 = 3, we must consider prefix positions
i = 1, 2, and 3 in what follows. As before, let p be the suffix length of s and q
the suffix length of t.
First, consider the case p ≥ q. The additional constraint we have on q and
j is (q + 1)/(9 + j) ≥ 0.8. We can enumerate the pairs of values of j and q for
each i between 1 and 3, as follows.
i = 1: Here, p = 9, so q ≤ 9. Let us consider the possible values of q:
q = 9: We must have 10/(9 + j) ≥ 0.8. Thus, we can have j = 1, j = 2,
or j = 3. Note that for j = 4, 10/13 > 0.8.
q = 8: We must have 9/(9 + j) ≥ 0.8. Thus, we can have j = 1 or j = 2.
For j = 3, 9/12 > 0.8.
q = 7: We must have 8/(9 + j) ≥ 0.8. Only j = 1 satisfies this inequality.
q = 6: There are no possible values of j, since 7/(9 + j) > 0.8 for every
positive integer j. The same holds for every smaller value of q.
i = 2: Here, p = 8, so we require q ≤ 8. Since the constraint (q+1)/(9+j) ≥ 0.8
does not depend on i,
8 we can use the analysis from the above case, but
exclude the case q = 9. Thus, the only possible values of j and q when
i = 2 are
1. q = 8; j = 1.
2. q = 8; j = 2.
3. q = 7; j = 1.
i = 3: Now, p = 7 and the constraints are q ≤ 7 and (q + 1)/(9 + j) ≥ 0.8. The
only option is q = 7 and j = 1.
Next, we must consider the case p < q. The additional constraint is
11 − i
i + j + q − 1
≥ 0.8
Again, consider each possible value of i.
8Note that i does influence the value of p, and through p, puts a limit on q.
3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 129
i = 1: Then p = 9, so we require q ≥ 10 and 10/(q + j) ≥ 0.8. The possible
values of q and j are
1. q = 10; j = 1.
2. q = 10; j = 2.
3. q = 11; j = 1.
i = 2: Now, p = 8, so we require q ≥ 9 and 9/(q + j + 1) ≥ 0.8. Since j must
be a positive integer, the only solution is q = 9 and j = 1, a possibility
that we already knew about.
i = 3: Here, p = 7, so we require q ≥ 8 and 8/(q + j + 2) ≥ 0.8. There are no
solutions.
q j = 1 j = 2 j = 3
7 x
8 x x
i = 1 9 x x x
10 x x
11 x
7 x
i = 2 8 x x
9 x
i = 3 7 x
Figure 3.16: The buckets that must be examined to find possible matches for
the string s = acdefghijk with J = 0.8 are marked with an x
When we accumulate the possible combinations of i, j, and q, we see that
the set of index buckets in which we must look forms a pyramid. Figure 3.16
shows the buckets in which we must search. That is, we must look in those
buckets (x, j, q) such that the ith symbol of the string s is x, j is the position
associated with the bucket and q the suffix length. ✷
3.9.7 Exercises for Section 3.9
Exercise 3.9.1 : Suppose our universal set is the lower-case letters, and the
order of elements is taken to be the vowels, in alphabetic order, followed by the
consonants in reverse alphabetic order. Represent the following sets as strings.
a {q, w, e, r, t, y}.
(b) {a, s, d, f, g, h, j, u, i}.
130 CHAPTER 3. FINDING SIMILAR ITEMS
Exercise 3.9.2 : Suppose we filter candidate pairs based only on length, as in
Section 3.9.3. If s is a string of length 20, with what strings is s compared when
J, the lower bound on Jaccard similarity has the following values: (a) J = 0.85
(b) J = 0.95 (c) J = 0.98?
Exercise 3.9.3 : Suppose we have a string s of length 15, and we wish to index
its prefix as in Section 3.9.4.
(a) How many positions are in the prefix if J = 0.85?
(b) How many positions are in the prefix if J = 0.95?
! (c) For what range of values of J will s be indexed under its first four symbols,
but no more?
Exercise 3.9.4 : Suppose s is a string of length 12. With what symbol-position
pairs will s be compared with if we use the indexing approach of Section 3.9.5,
and (a) J = 0.75 (b) J = 0.95?
! Exercise 3.9.5 : Suppose we use position information in our index, as in Section 3.9.5. Strings s and t are both chosen at random from a universal set of
100 elements. Assume J = 0.9. What is the probability that s and t will be
compared if
(a) s and t are both of length 9.
(b) s and t are both of length 10.
Exercise 3.9.6 : Suppose we use indexes based on both position and suffix
length, as in Section 3.9.6. If s is a string of length 20, with what symbolposition-length triples will s be compared with, if (a) J = 0.8 (b) J = 0.9?
3.10 Summary of Chapter 3
✦ Jaccard Similarity: The Jaccard similarity of sets is the ratio of the size
of the intersection of the sets to the size of the union. This measure of
similarity is suitable for many applications, including textual similarity of
documents and similarity of buying habits of customers.
✦ Shingling: A k-shingle is any k characters that appear consecutively in
a document. If we represent a document by its set of k-shingles, then
the Jaccard similarity of the shingle sets measures the textual similarity
of documents. Sometimes, it is useful to hash shingles to bit strings of
shorter length, and use sets of hash values to represent documents.
✦ Minhashing: A minhash function on sets is based on a permutation of the
universal set. Given any such permutation, the minhash value for a set is
that element of the set that appears first in the permuted order.
3.10. SUMMARY OF CHAPTER 3 131
✦ Minhash Signatures: We may represent sets by picking some list of permutations and computing for each set its minhash signature, which is the
sequence of minhash values obtained by applying each permutation on the
list to that set. Given two sets, the expected fraction of the permutations
that will yield the same minhash value is exactly the Jaccard similarity
of the sets.
✦ Efficient Minhashing: Since it is not really possible to generate random
permutations, it is normal to simulate a permutation by picking a random
hash function and taking the minhash value for a set to be the least hash
value of any of the set’s members. An additional efficiency can be had
by restricting the search for the smallest minhash value to only a small
subset of the universal set.
✦ Locality-Sensitive Hashing for Signatures: This technique allows us to
avoid computing the similarity of every pair of sets or their minhash signatures. If we are given signatures for the sets, we may divide them into
bands, and only measure the similarity of a pair of sets if they are identical in at least one band. By choosing the size of bands appropriately, we
can eliminate from consideration most of the pairs that do not meet our
threshold of similarity.
✦ Distance Measures: A distance measure is a function on pairs of points in
a space that satisfy certain axioms. The distance between two points is 0 if
the points are the same, but greater than 0 if the points are different. The
distance is symmetric; it does not matter in which order we consider the
two points. A distance measure must satisfy the triangle inequality: the
distance between two points is never more than the sum of the distances
between those points and some third point.
✦ Euclidean Distance: The most common notion of distance is the Euclidean
distance in an n-dimensional space. This distance, sometimes called the
L2-norm, is the square root of the sum of the squares of the differences
between the points in each dimension. Another distance suitable for Euclidean spaces, called Manhattan distance or the L1-norm is the sum of
the magnitudes of the differences between the points in each dimension.
✦ Jaccard Distance: One minus the Jaccard similarity is a distance measure,
called the Jaccard distance.
✦ Cosine Distance: The angle between vectors in a vector space is the cosine
distance measure. We can compute the cosine of that angle by taking the
dot product of the vectors and dividing by the lengths of the vectors.
✦ Edit Distance: This distance measure applies to a space of strings, and
is the number of insertions and/or deletions needed to convert one string
into the other. The edit distance can also be computed as the sum of
132 CHAPTER 3. FINDING SIMILAR ITEMS
the lengths of the strings minus twice the length of the longest common
subsequence of the strings.
✦ Hamming Distance: This distance measure applies to a space of vectors.
The Hamming distance between two vectors is the number of positions in
which the vectors differ.
✦ Generalized Locality-Sensitive Hashing: We may start with any collection
of functions, such as the minhash functions, that can render a decision
as to whether or not a pair of items should be candidates for similarity
checking. The only constraint on these functions is that they provide a
lower bound on the probability of saying “yes” if the distance (according
to some distance measure) is below a given limit, and an upper bound on
the probability of saying “yes” if the distance is above another given limit.
We can then increase the probability of saying “yes” for nearby items and
at the same time decrease the probability of saying “yes” for distant items
to as great an extent as we wish, by applying an AND construction and
an OR construction.
✦ Random Hyperplanes and LSH for Cosine Distance: We can get a set of
basis functions to start a generalized LSH for the cosine distance measure
by identifying each function with a list of randomly chosen vectors. We
apply a function to a given vector v by taking the dot product of v with
each vector on the list. The result is a sketch consisting of the signs (+1 or
−1) of the dot products. The fraction of positions in which the sketches of
two vectors agree, multiplied by 180, is an estimate of the angle between
the two vectors.
✦ LSH For Euclidean Distance: A set of basis functions to start LSH for
Euclidean distance can be obtained by choosing random lines and projecting points onto those lines. Each line is broken into fixed-length intervals,
and the function answers “yes” to a pair of points that fall into the same
interval.
✦ High-Similarity Detection by String Comparison: An alternative approach
to finding similar items, when the threshold of Jaccard similarity is close to
1, avoids using minhashing and LSH. Rather, the universal set is ordered,
and sets are represented by strings, consisting their elements in order.
The simplest way to avoid comparing all pairs of sets or their strings is to
note that highly similar sets will have strings of approximately the same
length. If we sort the strings, we can compare each string with only a
small number of the immediately following strings.
✦ Character Indexes: If we represent sets by strings, and the similarity
threshold is close to 1, we can index all strings by their first few characters.
The prefix whose characters must be indexed is approximately the length
of the string times the maximum Jaccard distance (1 minus the minimum
Jaccard similarity).
3.11. REFERENCES FOR CHAPTER 3 133
✦ Position Indexes: We can index strings not only on the characters in
their prefixes, but on the position of that character within the prefix. We
reduce the number of pairs of strings that must be compared, because
if two strings share a character that is not in the first position in both
strings, then we know that either there are some preceding characters that
are in the union but not the intersection, or there is an earlier symbol that
appears in both strings.
✦ Suffix Indexes: We can also index strings based not only on the characters
in their prefixes and the positions of those characters, but on the length
of the character’s suffix – the number of positions that follow it in the
string. This structure further reduces the number of pairs that must be
compared, because a common symbol with different suffix lengths implies
additional characters that must be in the union but not in the intersection.
3.11 References for Chapter 3
The technique we called shingling is attributed to [11]. The use in the manner
we discussed here is from [2].
Minhashing comes from [3]. The improvement that avoids looking at all
elements is from [10].
The original works on locality-sensitive hashing were [9] and [7]. [1] is a
useful summary of ideas in this field.
[4] introduces the idea of using random-hyperplanes to summarize items in
a way that respects the cosine distance. [8] suggests that random hyperplanes
plus LSH can be more accurate at detecting similar documents than minhashing
plus LSH.
Techniques for summarizing points in a Euclidean space are covered in [6].
[12] presented the shingling technique based on stop words.
The length and prefix-based indexing schemes for high-similarity matching
comes from [5]. The technique involving suffix length is from [13].
1. A. Andoni and P. Indyk, “Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions,” Comm. ACM 51:1, pp. 117–
122, 2008.
2. A.Z. Broder, “On the resemblance and containment of documents,” Proc.
Compression and Complexity of Sequences, pp. 21–29, Positano Italy,
1997.
3. A.Z. Broder, M. Charikar, A.M. Frieze, and M. Mitzenmacher, “Min-wise
independent permutations,” ACM Symposium on Theory of Computing,
pp. 327–336, 1998.
4. M.S. Charikar, “Similarity estimation techniques from rounding algorithms,” ACM Symposium on Theory of Computing, pp. 380–388, 2002.
134 CHAPTER 3. FINDING SIMILAR ITEMS
5. S. Chaudhuri, V. Ganti, and R. Kaushik, “A primitive operator for similarity joins in data cleaning,” Proc. Intl. Conf. on Data Engineering,
2006.
6. M. Datar, N. Immorlica, P. Indyk, and V.S. Mirrokni, “Locality-sensitive
hashing scheme based on p-stable distributions,” Symposium on Computational Geometry pp. 253–262, 2004.
7. A. Gionis, P. Indyk, and R. Motwani, “Similarity search in high dimensions via hashing,” Proc. Intl. Conf. on Very Large Databases, pp. 518–
529, 1999.
8. M. Henzinger, “Finding near-duplicate web pages: a large-scale evaluation
of algorithms,” Proc. 29th SIGIR Conf., pp. 284–291, 2006.
9. P. Indyk and R. Motwani. “Approximate nearest neighbor: towards removing the curse of dimensionality,” ACM Symposium on Theory of Computing, pp. 604–613, 1998.
10. P. Li, A.B. Owen, and C.H. Zhang. “One permutation hashing,” Conf.
on Neural Information Processing Systems 2012, pp. 3122–3130.
11. U. Manber, “Finding similar files in a large file system,” Proc. USENIX
Conference, pp. 1–10, 1994.
12. M. Theobald, J. Siddharth, and A. Paepcke, “SpotSigs: robust and efficient near duplicate detection in large web collections,” 31st Annual ACM
SIGIR Conference, July, 2008, Singapore.
13. C. Xiao, W. Wang, X. Lin, and J.X. Yu, “Efficient similarity joins for
near duplicate detection,” Proc. WWW Conference, pp. 131-140, 2008.

72
Chapter 3
Finding Similar Items
A fundamental data-mining problem is to examine data for “similar” items. We
shall take up applications in Section 3.1, but an example would be looking at a
collection of Web pages and finding near-duplicate pages. These pages could be
plagiarisms, for example, or they could be mirrors that have almost the same
content but differ in information about the host and about other mirrors.
The naive approach to finding pairs of similar items requires us to look at every pair of items. When we are dealing with a large dataset, looking at all pairs
of items may be prohibitive, even given an abundance of hardware resources.
For example, even a million items gives us half a trillion pairs to examine, and
a million items is considered a “small” dataset by today’s standards.
It is therefore a pleasant surprise to learn of a family of techniques called
locality-sensitive hashing, or LSH, that allows us to focus on pairs that are likely
to be similar, without having to look at all pairs. Thus, it is possible that we
can avoid the quadratic growth in computation time that is required by the
naive algorithm. There is usually a downside to locality-sensitive hashing, due
to the presence of false negatives, that is, pairs of items that are similar, yet
are not included in the set of pairs that we examine, but by careful tuning we
can reduce the fraction of false negatives by increasing the number of pairs we
consider.
The general idea behind LSH is that we hash items using many different
hash functions. These hash functions are not the conventional sort of hash
functions. Rather, they are carefully designed to have the property that pairs
are much more likely to wind up in the same bucket of a hash function if the
items are similar than if they are not similar. We then can examine only the
candidate pairs, which are pairs of items that wind up in the same bucket for
at least one of the hash functions.
We begin our discussion of LSH with an examination of the problem of finding similar documents – those that share a lot of common text. We first show
how to convert documents into sets (Section 3.2) in a way that lets us view
textual similarity of documents as sets having a large overlap. More precisely,
73
74 CHAPTER 3. FINDING SIMILAR ITEMS
we measure the similarity of sets by their Jaccard similarity, the ratio of the
sizes of their intersection and union. A second key trick we need is minhashing
(Section 3.3), which is a way to convert large sets into much smaller representations, called signatures, that still enable us to estimate closely the Jaccard
similarity of the represented sets. Finally, in Section 3.4 we see how to apply
the bucketing idea inherent in LSH to the signatures.
In Section 3.5 we begin our study of how to apply LSH to items other than
sets. We consider the general notion of a distance measure that tells to what
degree items are similar. Then, in Section 3.6 we consider the general idea of
locality-sensitive hashing, and in Section 3.7 we see how to do LSH for some data
types other than sets. Then, Section 3.8 examines in detail several applications
of the LSH idea. Finally, we consider in Section 3.9 some techniques for finding
similar sets that can be more efficient than LSH when the degree of similarity
we want is very high.
3.1 Applications of Set Similarity
We shall focus initially on a particular notion of “similarity”: the similarity of
sets by looking at the relative size of their intersection. This notion of similarity
is called Jaccard similarity, which is introduced in Section 3.1.1. We then
examine some of the uses of finding similar sets. These include finding textually
similar documents and collaborative filtering by finding similar customers and
similar products. In order to turn the problem of textual similarity of documents
into one of set intersection, we use the technique called shingling, which is the
subject of Section 3.2.
3.1.1 Jaccard Similarity of Sets
The Jaccard similarity of sets S and T is |S ∩ T |/|S ∪ T |, that is, the ratio
of the size of the intersection of S and T to the size of their union. We shall
denote the Jaccard similarity of S and T by SIM(S, T ).
Example 3.1 : In Fig. 3.1 we see two sets S and T . There are three elements
in their intersection and a total of eight elements that appear in S or T or both.
Thus, SIM(S, T ) = 3/8. ✷
3.1.2 Similarity of Documents
An important class of problems that Jaccard similarity addresses well is that
of finding textually similar documents in a large corpus such as the Web or a
collection of news articles. We should understand that the aspect of similarity
we are looking at here is character-level similarity, not “similar meaning,” which
requires us to examine the words in the documents and their uses. That problem
is also interesting but is addressed by other techniques, which we hinted at in
3.1. APPLICATIONS OF SET SIMILARITY 75
































T
S
Figure 3.1: Two sets with Jaccard similarity 3/8
Section 1.3.1. However, textual similarity also has important uses. Many of
these involve finding duplicates or near duplicates. First, let us observe that
testing whether two documents are exact duplicates is easy; just compare the
two documents character-by-character, and if they ever differ then they are not
the same. However, in many applications, the documents are not identical, yet
they share large portions of their text. Here are some examples:
Plagiarism
Finding plagiarized documents tests our ability to find textual similarity. The
plagiarizer may extract only some parts of a document for his own. He may
alter a few words and may alter the order in which sentences of the original
appear. Yet the resulting document may still contain much of the original. No
simple process of comparing documents character by character will detect a
sophisticated plagiarism.
Mirror Pages
It is common for important or popular Web sites to be duplicated at a number
of hosts, in order to share the load. The pages of these mirror sites will be
quite similar, but are rarely identical. For instance, they might each contain
information associated with their particular host, and they might each have
links to the other mirror sites but not to themselves. A related phenomenon is
the reuse of Web pages from one academic class to another. These pages might
include class notes, assignments, and lecture slides. Similar pages might change
the name of the course, year, and make small changes from year to year. It
is important to be able to detect similar pages of these kinds, because search
engines produce better results if they avoid showing two pages that are nearly
identical within the first
76 CHAPTER 3. FINDING SIMILAR ITEMS
Articles from the Same Source
It is common for one reporter to write a news article that gets distributed,
say through the Associated Press, to many newspapers, which then publish
the article on their Web sites. Each newspaper changes the article somewhat.
They may cut out paragraphs, or even add material of their own. They most
likely will surround the article by their own logo, ads, and links to other articles
at their site. However, the core of each newspaper’s page will be the original
article. News aggregators, such as Google News, try to find all versions of such
an article, in order to show only one, and that task requires finding when two
Web pages are textually similar, although not identical.1
3.1.3 Collaborative Filtering as a Similar-Sets Problem
Another class of applications where similarity of sets is very important is called
collaborative filtering, a process whereby we recommend to users items that were
liked by other users who have exhibited similar tastes. We shall investigate
collaborative filtering in detail in Section 9.3, but for the moment let us see
some common examples.
On-Line Purchases
Amazon.com has millions of customers and sells millions of items. Its database
records which items have been bought by which customers. We can say two customers are similar if their sets of purchased items have a high Jaccard similarity.
Likewise, two items that have sets of purchasers with high Jaccard similarity
will be deemed similar. Note that, while we might expect mirror sites to have
Jaccard similarity above 90%, it is unlikely that any two customers have Jaccard similarity that high (unless they have purchased only one item). Even a
Jaccard similarity like 20% might be unusual enough to identify customers with
similar tastes. The same observation holds for items; Jaccard similarities need
not be very high to be significant.
Collaborative filtering requires several tools, in addition to finding similar
customers or items, as we discuss in Chapter 9. For example, two Amazon
customers who like science-fiction might each buy many science-fiction books,
but only a few of these will be in common. However, by combining similarityfinding with clustering (Chapter 7), we might be able to discover that sciencefiction books are mutually similar and put them in one group. Then, we can
get a more powerful notion of customer-similarity by asking whether they made
purchases within many of the same groups.
1News aggregation also involves finding articles that are about the same topic, even though
not textually similar. This problem too can yield to a similarity search, but it requires
techniques other than Jaccard similarity of sets.
3.1. APPLICATIONS OF SET SIMILARITY 77
Movie Ratings
Netflix records which movies each of its customers rented, and also the ratings
assigned to those movies by the customers. We can regard movies as similar
if they were rented or rated highly by many of the same customers, and see
customers as similar if they rented or rated highly many of the same movies.
The same observations that we made for Amazon above apply in this situation:
similarities need not be high to be significant, and clustering movies by genre
will make things easier.
When our data consists of ratings rather than binary decisions (bought/did
not buy or liked/disliked), we cannot rely simply on sets as representations of
customers or items. Some options are:
1. Ignore low-rated customer/movie pairs; that is, treat these events as if
the customer never watched the movie.
2. When comparing customers, imagine two set elements for each movie,
“liked” and “hated.” If a customer rated a movie highly, put “liked” for
that movie in the customer’s set. If they gave a low rating to a movie, put
“hated” for that movie in their set. Then, we can look for high Jaccard
similarity among these sets. We can use a similar trick when comparing
movies.
3. If ratings are 1-to-5-stars, put a movie in a customer’s set n times if
they rated the movie n-stars. Then, use Jaccard similarity for bags when
measuring the similarity of customers. The Jaccard similarity for bags
B and C is defined by counting an element n times in the intersection if
n is the minimum of the number of times the element appears in B and
C. In the union, we count the element the sum of the number of times it
appears in B and in C.
2
Example 3.2 : The bag-similarity of bags {a, a, a, b} and {a, a, b, b, c} is 1/3.
The intersection counts a twice and b once, so its size is 3. The size of the
union of two bags is always the sum of the sizes of the two bags, or 9 in this
case. Since the highest possible Jaccard similarity for bags is 1/2, the score
of 1/3 indicates the two bags are quite similar, as should be apparent from an
examination of their contents. ✷
2Although the union for bags is normally (e.g., in the SQL standard) defined to have the
sum of the number of copies in each of the two bags, this definition causes some inconsistency
with the Jaccard similarity for sets. Under this definition of bag union, the maximum Jaccard
similarity is 1/2, not 1, since the union of a set with itself has twice as many elements as the
intersection of the same set with itself. If we prefer to have the Jaccard similarity of a set
with itself be 1, we can redefine the union of bags to have each element appear the maximum
number of times it appears in either of the two bags. This change also gives a reasonable
measure of bag similarity.
78 CHAPTER 3. FINDING SIMILAR ITEMS
3.1.4 Exercises for Section 3.1
Exercise 3.1.1 : Compute the Jaccard similarities of each pair of the following
three sets: {1, 2, 3, 4}, {2, 3, 5, 7}, and {2, 4, 6}.
Exercise 3.1.2 : Compute the Jaccard bag similarity of each pair of the following three bags: {1, 1, 1, 2}, {1, 1, 2, 2, 3}, and {1, 2, 3, 4}.
!! Exercise 3.1.3 : Suppose we have a universal set U of n elements, and we
choose two subsets S and T at random, each with m of the n elements. What
is the expected value of the Jaccard similarity of S and T ?
3.2 Shingling of Documents
The most effective way to represent documents as sets, for the purpose of identifying lexically similar documents is to construct from the document the set
of short strings that appear within it. If we do so, then documents that share
pieces as short as sentences or even phrases will have many common elements
in their sets, even if those sentences appear in different orders in the two documents. In this section, we introduce the simplest and most common approach,
shingling, as well as an interesting variation.
3.2.1 k-Shingles
A document is a string of characters. Define a k-shingle for a document to be
any substring of length k found within the document. Then, we may associate
with each document the set of k-shingles that appear one or more times within
that document.
Example 3.3 : Suppose our document D is the string abcdabd, and we pick
k = 2. Then the set of 2-shingles for D is {ab, bc, cd, da, bd}.
Note that the substring ab appears twice within D, but appears only once
as a shingle. A variation of shingling produces a bag, rather than a set, so each
shingle would appear in the result as many times as it appears in the document.
However, we shall not use bags of shingles here. ✷
There are several options regarding how white space (blank, tab, newline,
etc.) is treated. It probably makes sense to replace any sequence of one or more
white-space characters by a single blank. That way, we distinguish shingles that
cover two or more words from those that do not.
Example 3.4 : If we use k = 9, but eliminate whitespace altogether, then we
would see some lexical similarity in the sentences “The plane was ready for
touch down”. and “The quarterback scored a touchdown”. However, if we
retain the blanks, then the first has shingles touch dow and ouch down, while
the second has touchdown. If we eliminated the blanks, then both would have
touchdown. ✷
3.2. SHINGLING OF DOCUMENTS 79
3.2.2 Choosing the Shingle Size
We can pick k to be any constant we like. However, if we pick k too small, then
we would expect most sequences of k characters to appear in most documents.
If so, then we could have documents whose shingle-sets had high Jaccard similarity, yet the documents had none of the same sentences or even phrases. As
an extreme example, if we use k = 1, most Web pages will have most of the
common characters and few other characters, so almost all Web pages will have
high similarity.
How large k should be depends on how long typical documents are and how
large the set of typical characters is. The important thing to remember is:
• k should be picked large enough that the probability of any given shingle
appearing in any given document is low.
Thus, if our corpus of documents is emails, picking k = 5 should be fine.
To see why, suppose that only letters and a general white-space character appear in emails (although in practice, most of the printable ASCII characters
can be expected to appear occasionally). If so, then there would be 275 =
14,348,907 possible shingles. Since the typical email is much smaller than 14
million characters long, we would expect k = 5 to work well, and indeed it does.
However, the calculation is a bit more subtle. Surely, more than 27 characters appear in emails, However, all characters do not appear with equal probability. Common letters and blanks dominate, while ”z” and other letters that
have high point-value in Scrabble are rare. Thus, even short emails will have
many 5-shingles consisting of common letters, and the chances of unrelated
emails sharing these common shingles is greater than would be implied by the
calculation in the paragraph above. A good rule of thumb is to imagine that
there are only 20 characters and estimate the number of k-shingles as 20k
. For
large documents, such as research articles, choice k = 9 is considered safe.
3.2.3 Hashing Shingles
Instead of using substrings directly as shingles, we can pick a hash function
that maps strings of length k to some number of buckets and treat the resulting
bucket number as the shingle. The set representing a document is then the
set of integers that are bucket numbers of one or more k-shingles that appear
in the document. For instance, we could construct the set of 9-shingles for a
document and then map each of those 9-shingles to a bucket number in the
range 0 to 232 − 1. Thus, each shingle is represented by four bytes instead
of nine. Not only has the data been compacted, but we can now manipulate
(hashed) shingles by single-word machine operations.
Notice that we can differentiate documents better if we use 9-shingles and
hash them down to four bytes than to use 4-shingles, even though the space used
to represent a shingle is the same. The reason was touched upon in Section 3.2.2.
If we use 4-shingles, most sequences of four bytes are unlikely or impossible to
80 CHAPTER 3. FINDING SIMILAR ITEMS
find in typical documents. Thus, the effective number of different shingles is
much less than 232 −1. If, as in Section 3.2.2, we assume only 20 characters are
frequent in English text, then the number of different 4-shingles that are likely
to occur is only (20)4 = 160,000. However, if we use 9-shingles, there are many
more than 232 likely shingles. When we hash them down to four bytes, we can
expect almost any sequence of four bytes to be possible, as was discussed in
Section 1.3.2.
3.2.4 Shingles Built from Words
An alternative form of shingle has proved effective for the problem of identifying
similar news articles, mentioned in Section 3.1.2. The exploitable distinction for
this problem is that the news articles are written in a rather different style than
are other elements that typically appear on the page with the article. News
articles, and most prose, have a lot of stop words (see Section 1.3.1), the most
common words such as “and,” “you,” “to,” and so on. In many applications,
we want to ignore stop words, since they don’t tell us anything useful about
the article, such as its topic.
However, for the problem of finding similar news articles, it was found that
defining a shingle to be a stop word followed by the next two words, regardless
of whether or not they were stop words, formed a useful set of shingles. The
advantage of this approach is that the news article would then contribute more
shingles to the set representing the Web page than would the surrounding elements. Recall that the goal of the exercise is to find pages that had the same
articles, regardless of the surrounding elements. By biasing the set of shingles
in favor of the article, pages with the same article and different surrounding
material have higher Jaccard similarity than pages with the same surrounding
material but with a different article.
Example 3.5 : An ad might have the simple text “Buy Sudzo.” However, a
news article with the same idea might read something like “A spokesperson
for the Sudzo Corporation revealed today that studies have shown it is
good for people to buy Sudzo products.” Here, we have italicized all the
likely stop words, although there is no set number of the most frequent words
that should be considered stop words. The first three shingles made from a
stop word and the next two following are:
A spokesperson for
for the Sudzo
the Sudzo Corporation
There are nine shingles from the sentence, but none from the “ad.” ✷
3.2.5 Exercises for Section 3.2
Exercise 3.2.1 : What are the first ten 3-shingles in the first sentence of Section 3.2?
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 81
Exercise 3.2.2 : If we use the stop-word-based shingles of Section 3.2.4, and
we take the stop words to be all the words of three or fewer letters, then what
are the shingles in the first sentence of Section 3.2?
Exercise 3.2.3 : What is the largest number of k-shingles a document of n
bytes can have? You may assume that the size of the alphabet is large enough
that the number of possible strings of length k is at least n.
3.3 Similarity-Preserving Summaries of Sets
Sets of shingles are large. Even if we hash them to four bytes each, the space
needed to store a set is still roughly four times the space taken by the document.
If we have millions of documents, it may well not be possible to store all the
shingle-sets in main memory.3
Our goal in this section is to replace large sets by much smaller representations called “signatures.” The important property we need for signatures is
that we can compare the signatures of two sets and estimate the Jaccard similarity of the underlying sets from the signatures alone. It is not possible that
the signatures give the exact similarity of the sets they represent, but the estimates they provide are close, and the larger the signatures the more accurate
the estimates. For example, if we replace the 200,000-byte hashed-shingle sets
that derive from 50,000-byte documents by signatures of 1000 bytes, we can
usually get within a few percent.
3.3.1 Matrix Representation of Sets
Before explaining how it is possible to construct small signatures from large
sets, it is helpful to visualize a collection of sets as their characteristic matrix.
The columns of the matrix correspond to the sets, and the rows correspond to
elements of the universal set from which elements of the sets are drawn. There
is a 1 in row r and column c if the element for row r is a member of the set for
column c. Otherwise the value in position (r, c) is 0.
Element S1 S2 S3 S4
a 1 0 0 1
b 0 0 1 0
c 0 1 0 1
d 1 0 1 1
e 0 0 1 0
Figure 3.2: A matrix representing four sets
3There is another serious concern: even if the sets fit in main memory, the number of pairs
may be too great for us to evaluate the similarity of each pair. We take up the solution to
this problem in Section 3.4.
82 CHAPTER 3. FINDING SIMILAR ITEMS
Example 3.6 : In Fig. 3.2 is an example of a matrix representing sets chosen
from the universal set {a, b, c, d, e}. Here, S1 = {a, d}, S2 = {c}, S3 = {b, d, e},
and S4 = {a, c, d}. The top row and leftmost columns are not part of the matrix,
but are present only to remind us what the rows and columns represent. ✷
It is important to remember that the characteristic matrix is unlikely to be
the way the data is stored, but it is useful as a way to visualize the data. For one
reason not to store data as a matrix, these matrices are almost always sparse
(they have many more 0’s than 1’s) in practice. It saves space to represent a
sparse matrix of 0’s and 1’s by the positions in which the 1’s appear. For another
reason, the data is usually stored in some other format for other purposes.
As an example, if rows are products, and columns are customers, represented
by the set of products they bought, then this data would really appear in a
database table of purchases. A tuple in this table would list the item, the
purchaser, and probably other details about the purchase, such as the date and
the credit card used.
3.3.2 Minhashing
The signatures we desire to construct for sets are composed of the results of a
large number of calculations, say several hundred, each of which is a “minhash”
of the characteristic matrix. In this section, we shall learn how a minhash is
computed in principle, and in later sections we shall see how a good approximation to the minhash is computed in practice.
To minhash a set represented by a column of the characteristic matrix, pick
a permutation of the rows. The minhash value of any column is the number of
the first row, in the permuted order, in which the column has a 1.
Example 3.7 : Let us suppose we pick the order of rows beadc for the matrix
of Fig. 3.2. This permutation defines a minhash function h that maps sets to
rows. Let us compute the minhash value of set S1 according to h. The first
column, which is the column for set S1, has 0 in row b, so we proceed to row e,
the second in the permuted order. There is again a 0 in the column for S1, so
we proceed to row a, where we find a 1. Thus. h(S1) = a.
Element S1 S2 S3 S4
b 0 0 1 0
e 0 0 1 0
a 1 0 0 1
d 1 0 1 1
c 0 1 0 1
Figure 3.3: A permutation of the rows of Fig. 3.2
Although it is not physically possible to permute very large characteristic
matrices, the minhash function h implicitly reorders the rows of the matrix of
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 83
Fig. 3.2 so it becomes the matrix of Fig. 3.3. In this matrix, we can read off
the values of h by scanning from the top until we come to a 1. Thus, we see
that h(S2) = c, h(S3) = b, and h(S4) = a. ✷
3.3.3 Minhashing and Jaccard Similarity
There is a remarkable connection between minhashing and Jaccard similarity
of the sets that are minhashed.
• The probability that the minhash function for a random permutation of
rows produces the same value for two sets equals the Jaccard similarity
of those sets.
To see why, we need to picture the columns for those two sets. If we restrict
ourselves to the columns for sets S1 and S2, then rows can be divided into three
classes:
1. Type X rows have 1 in both columns.
2. Type Y rows have 1 in one of the columns and 0 in the other.
3. Type Z rows have 0 in both columns.
Since the matrix is sparse, most rows are of type Z. However, it is the ratio
of the numbers of type X and type Y rows that determine both SIM(S1, S2)
and the probability that h(S1) = h(S2). Let there be x rows of type X and y
rows of type Y . Then SIM(S1, S2) = x/(x + y). The reason is that x is the size
of S1 ∩ S2 and x + y is the size of S1 ∪ S2.
Now, consider the probability that h(S1) = h(S2). If we imagine the rows
permuted randomly, and we proceed from the top, the probability that we shall
meet a type X row before we meet a type Y row is x/(x + y). But if the
first row from the top other than type Z rows is a type X row, then surely
h(S1) = h(S2). On the other hand, if the first row other than a type Z row
that we meet is a type Y row, then the set with a 1 gets that row as its minhash
value. However the set with a 0 in that row surely gets some row further down
the permuted list. Thus, we know h(S1) 6= h(S2) if we first meet a type Y row.
We conclude the probability that h(S1) = h(S2) is x/(x + y), which is also the
Jaccard similarity of S1 and S2.
3.3.4 Minhash Signatures
Again think of a collection of sets represented by their characteristic matrix M.
To represent sets, we pick at random some number n of permutations of the
rows of M. Perhaps 100 permutations or several hundred permutations will do.
Call the minhash functions determined by these permutations h1, h2, . . . , hn.
From the column representing set S, construct the minhash signature for S, the
vector [h1(S), h2(S), . . . , hn(S)]. We normally represent this list of hash-values
84 CHAPTER 3. FINDING SIMILAR ITEMS
as a column. Thus, we can form from matrix M a signature matrix, in which
the ith column of M is replaced by the minhash signature for (the set of) the
ith column.
Note that the signature matrix has the same number of columns as M but
only n rows. Even if M is not represented explicitly, but in some compressed
form suitable for a sparse matrix (e.g., by the locations of its 1’s), it is normal
for the signature matrix to be much smaller than M.
The remarkable thing about signature matrices is that we can use their
columns to estimate the Jaccard similarity of the sets that correspond to the
columns of signature matrix. By the theorem proved in Section 3.3.3, we know
that the probability that two columns have the same value in a given row of
the signature matrix equals the Jaccard similarity of the sets corresponding to
those columns. Moreover, since the permutations on which the minhash values
are based were chosen independently, we can think of each row of the signature
matrix as an independent experiment. Thus, the expected number of rows in
which two columns agree equals the Jaccard similarity of their corresponding
sets. Moreover, the more minhashings we use, i.e., the more rows in the signature matrix, the smaller the expected error in the estimate of the Jaccard
similarity will be.
3.3.5 Computing Minhash Signatures in Practice
It is not feasible to permute a large characteristic matrix explicitly. Even picking
a random permutation of millions or billions of rows is time-consuming, and
the necessary sorting of the rows would take even more time. Thus, permuted
matrices like that suggested by Fig. 3.3, while conceptually appealing, are not
implementable.
Fortunately, it is possible to simulate the effect of a random permutation by
a random hash function that maps row numbers to as many buckets as there
are rows. A hash function that maps integers 0, 1, . . . , k − 1 to bucket numbers
0 through k−1 typically will map some pairs of integers to the same bucket and
leave other buckets unfilled. However, the difference is unimportant as long as
k is large and there are not too many collisions. We can maintain the fiction
that our hash function h “permutes” row r to position h(r) in the permuted
order.
Thus, instead of picking n random permutations of rows, we pick n randomly
chosen hash functions h1, h2, . . . , hn on the rows. We construct the signature
matrix by considering each row in their given order. Let SIG(i, c) be the element
of the signature matrix for the ith hash function and column c. Initially, set
SIG(i, c) to ∞ for all i and c. We handle row r by doing the following:
1. Compute h1(r), h2(r), . . . , hn(r).
2. For each column c do the following:
(a) If c has 0 in row r, do nothing.
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 85
(b) However, if c has 1 in row r, then for each i = 1, 2, . . . , n set SIG(i, c)
to the smaller of the current value of SIG(i, c) and hi(r).
Row S1 S2 S3 S4 x + 1 mod 5 3x + 1 mod 5
0 1 0 0 1 1 1
1 0 0 1 0 2 4
2 0 1 0 1 3 2
3 1 0 1 1 4 0
4 0 0 1 0 0 3
Figure 3.4: Hash functions computed for the matrix of Fig. 3.2
Example 3.8 : Let us reconsider the characteristic matrix of Fig. 3.2, which
we reproduce with some additional data as Fig. 3.4. We have replaced the
letters naming the rows by integers 0 through 4. We have also chosen two hash
functions: h1(x) = x+1 mod 5 and h2(x) = 3x+1 mod 5. The values of these
two functions applied to the row numbers are given in the last two columns of
Fig. 3.4. Notice that these simple hash functions are true permutations of the
rows, but a true permutation is only possible because the number of rows, 5, is
a prime. In general, there will be collisions, where two rows get the same hash
value.
Now, let us simulate the algorithm for computing the signature matrix.
Initially, this matrix consists of all ∞’s:
S1 S2 S3 S4
h1 ∞ ∞ ∞ ∞
h2 ∞ ∞ ∞ ∞
First, we consider row 0 of Fig. 3.4. We see that the values of h1(0) and
h2(0) are both 1. The row numbered 0 has 1’s in the columns for sets S1 and
S4, so only these columns of the signature matrix can change. As 1 is less than
∞, we do in fact change both values in the columns for S1 and S4. The current
estimate of the signature matrix is thus:
S1 S2 S3 S4
h1 1 ∞ ∞ 1
h2 1 ∞ ∞ 1
Now, we move to the row numbered 1 in Fig. 3.4. This row has 1 only in
S3, and its hash values are h1(1) = 2 and h2(1) = 4. Thus, we set SIG(1, 3) to 2
and SIG(2, 3) to 4. All other signature entries remain as they are because their
columns have 0 in the row numbered 1. The new signature matrix:
S1 S2 S3 S4
h1 1 ∞ 2 1
h2 1 ∞ 4 1
86 CHAPTER 3. FINDING SIMILAR ITEMS
The row of Fig. 3.4 numbered 2 has 1’s in the columns for S2 and S4, and
its hash values are h1(2) = 3 and h2(2) = 2. We could change the values in the
signature for S4, but the values in this column of the signature matrix, [1, 1], are
each less than the corresponding hash values [3, 2]. However, since the column
for S2 still has ∞’s, we replace it by [3, 2], resulting in:
S1 S2 S3 S4
h1 1 3 2 1
h2 1 2 4 1
Next comes the row numbered 3 in Fig. 3.4. Here, all columns but S2 have
1, and the hash values are h1(3) = 4 and h2(3) = 0. The value 4 for h1 exceeds
what is already in the signature matrix for all the columns, so we shall not
change any values in the first row of the signature matrix. However, the value
0 for h2 is less than what is already present, so we lower SIG(2, 1), SIG(2, 3) and
SIG(2, 4) to 0. Note that we cannot lower SIG(2, 2) because the column for S2 in
Fig. 3.4 has 0 in the row we are currently considering. The resulting signature
matrix:
S1 S2 S3 S4
h1 1 3 2 1
h2 0 2 0 0
Finally, consider the row of Fig. 3.4 numbered 4. h1(4) = 0 and h2(4) = 3.
Since row 4 has 1 only in the column for S3, we only compare the current
signature column for that set, [2, 0] with the hash values [0, 3]. Since 0 < 2, we
change SIG(1, 3) to 0, but since 3 > 0 we do not change SIG(2, 3). The final
signature matrix is:
S1 S2 S3 S4
h1 1 3 0 1
h2 0 2 0 0
We can estimate the Jaccard similarities of the underlying sets from this
signature matrix. Notice that columns 1 and 4 are identical, so we guess that
SIM(S1, S4) = 1.0. If we look at Fig. 3.4, we see that the true Jaccard similarity
of S1 and S4 is 2/3. Remember that the fraction of rows that agree in the
signature matrix is only an estimate of the true Jaccard similarity, and this
example is much too small for the law of large numbers to assure that the
estimates are close. For additional examples, the signature columns for S1 and
S3 agree in half the rows (true similarity 1/4), while the signatures of S1 and
S2 estimate 0 as their Jaccard similarity (the correct value). ✷
3.3.6 Speeding Up Minhashing
The process of minhashing is time-consuming, since we need to examine the
entire k-row matrix M for each minhash function we want. Let us first return
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 87
to the model of Section 3.3.2, where we imagine rows are actually permuted.
But to compute one minhash function on all the columns, we shall not go all
the way to the end of the permutation, but only look at the first m out of k
rows. If we make m small compared with k, we reduce the work by a large
factor, k/m.
However, there is a downside to making m small. As long as each column
has at least one 1 in the first m rows in permuted order, the rows after the mth
have no effect on any minhash value and may as well not be looked at. But
what if some columns are all-0’s in the first m rows? We have no minhash value
for those columns, and will instead have to use a special symbol, for which we
shall use ∞.
When we examine the minhash signatures of two columns in order to estimate the Jaccard similarity of their underlying sets, as in Section 3.3.4, we have
to take into account the possibility that one or both columns have ∞ as their
minhash value for some components of the signature. There are three cases:
1. If neither column has ∞ in a given row, then there is no change needed.
Count this row as an example of equal values if the two values are the
same, and as an example of unequal values if not.
2. One column has ∞ and the other does not. In this case, had we used
all the rows of the original permuted matrix M, the column that has the
∞ would eventually have been given some row number, and that number
will surely not be one of the first m rows in the permuted order. But the
other column does have a value that is one of the first m rows. Thus, we
surely have an example of unequal minhash values, and we count this row
of the signature matrix as such an example.
3. Now, suppose both columns have ∞ in row. Then in the original permuted
matrix M, the first m rows of both columns were all 0’s. We thus have no
information about the Jaccard similarity of the corresponding sets; that
similarity is only a function of the last k − m rows, which we have chosen
not to look at. We therefore count this row of the signature matrix as
neither an example of equal values nor of unequal values.
As long as the third case, where both columns have ∞, is rare, we get
almost as many examples to average as there are rows in the signature matrix.
That effect will reduce the accuracy of our estimates of the Jaccard distance
somewhat, but not much. And since we are now able to compute minhash
values for all the columns much faster than if we examined all the rows of M,
we can afford the time to apply a few more minhash functions. We get even
better accuracy than originally, and we do so faster than before.
3.3.7 Speedup Using Hash Functions
As before, there are reasons not to physically permute rows in the manner
assumed in Section 3.3.6. However, the idea of true permutations makes more
88 CHAPTER 3. FINDING SIMILAR ITEMS
sense in the context of Section 3.3.6 than it did in Section 3.3.2. The reason
is that we do not need to construct a full permutation of k elements, but only
pick a small number m out of the k rows and then pick a random permutation
of those rows. Depending on the value of m and how the matrix M is stored, it
might make sense to follow the algorithm suggested by Section 3.3.6 literally.
However, it is more likely that a strategy akin to Section 3.3.5 is needed.
Now, the rows of M are fixed, and not permuted. We choose a hash function
that hashes row numbers, and compute hash values for only the first m rows.
That is, we follow the algorithm of Section 3.3.5, but only until we reach the
mth row, whereupon we stop and, and for each columns, we take the minimum
hash value seen so far as the minhash value for that column.
Since some column may have 0 in all m rows, it is possible that some of the
minhash values will be ∞. Assuming m is sufficiently large that ∞ minhash
values are rare, we still get a good estimate of the Jaccard similarity of sets by
comparing columns of the signature matrix. Suppose T is the set of elements
of the universal set that are represented by the first m rows of matrix M. Let
S1 and S2 be the sets represented by two columns of M. Then the first m rows
of M represent the sets S1 ∩ T and S2 ∩ T . If both these sets are empty (i.e.,
both columns are all-0 in their first m rows), then this minhash function will be
∞ in both columns and will be ignored when estimating the Jaccard similarity
of the columns’ underlying sets.
If at least one of the sets S1 ∩ T and S2 ∩ T is nonempty, then the probability of the two columns having equal values for this minhash function is the
Jaccard similarity of these two sets, that is
|S1 ∩ S2 ∩ T |
|(S1 ∪ S2) ∩ T |
As long as T is chosen to be a random subset of the universal set, the expected
value of this fraction will be the same as the Jaccard similarity of S1 and S2.
However, there will be some random variation, since depending on T , we could
find more or less than an average number of type X rows (1’s in both columns)
and/or type Y rows (1 in one column and 0 in the other) among the first m
rows of matrix M.
To mitigate this variation, we do not use the same set T for each minhashing
that we do. Rather, we divide the rows of M into k/m groups.4 Then for each
hash function, we compute one minhash value by examining only the first m
rows of M, a different minhash value by examining only the second m rows,
and so on. We thus get k/m minhash values from a single hash function and
a single pass over all the rows of M. In fact, if k/m is large enough, we may
get all the rows of the signature matrix that we need by a single hash function
applied to each of the subsets of rows of M.
4
In what follows, we assume m divides k evenly, for convenience. It is unimportant, as
long as k/m is large, if some rows are not included in any group because k is not an integer
multiple of m.
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 89
Moreover, by using each of the rows of M to compute one of these minhash
values, we tend to balance out the errors in estimation of the Jaccard similarity
due to any one particular subset of the rows. That is, the Jaccard similarity
of S1 and S2 determines the ratio of type X and type Y rows. All the type X
rows are distributed among the k/m sets of rows, and likewise the type Y rows.
Thus, while one set of m rows may have more of one type of row than average,
there must then be some other set of m rows with fewer than average of that
same type.
Example 3.9 : In Fig. 3.5 we see a matrix representing three sets S1, S2, and
S3, with a universal set of eight elements; i.e., k = 8. Let us pick m = 4, so one
pass through the rows yields two minhash values, one based on the first four
rows and the other on the second four rows.
S1 S2 S3
0 0 0
0 0 0
0 0 1
0 1 1
1 1 1
1 1 0
1 0 0
0 0 0
Figure 3.5: A Boolean matrix representing three sets
First, note that the Jaccard similarities of the three sets are SIM(S1, S2) =
1/2, SIM(S1, S3) = 1/5, and SIM(S2, S3) = 1/2. Now, look at the first four
rows only. Whatever hash function we use, the minhash value for S1 will be
∞, the minhash value for S2 will be the hash value of the 4th row, and the
minhash value for S3 will be the smaller of the hash values for the third and
fourth rows. Thus, the minhash values for S1 and S2 will never agree. That
makes sense, since if T is the set of elements represented by the first four rows,
then S1 ∩ T = ∅, and therefore SIM(S1 ∩ T, S2 ∩ T ) = 0. However, in the
second four rows, the Jaccard similarity of S1 and S2 restricted to the elements
represented by the last four rows is 2/3.
We conclude that if we generate signatures consisting of two minhash values
using this hash function, one based on the first four rows and the second based
on the last four rows, the expected number of matches we get between the
signatures for S1 and S2 is the average of 0 and 2/3, or 1/3. Since the actual
Jaccard similarity of S1 and S2 is 1/2, there is an error, but not too great an
error. In larger examples, where minhash values are based on far more than
four rows, the expected error will approach zero.
Similarly, we can see the effect of splitting the rows on the other two pairs
of columns. Between S1 and S3, the top half represents sets with a Jaccard
90 CHAPTER 3. FINDING SIMILAR ITEMS
similarity of 0, while the bottom half represents sets with a Jaccard similarity
1/3. The expected number of matches in the signatures of S1 and S3 is therefore
the average of these, or 1/6. That compares with the true Jaccard similarity
SIM(S1, S3) = 1/5. Finally, when we compare S2 and S3, we note that the
Jaccard similarity of these columns in the first four rows is 1/2, and so is their
Jaccard similarity in the bottom four rows. The average, 1/2, also agrees exactly
with SIM(S2, S3) = 1/2. ✷
3.3.8 Exercises for Section 3.3
Exercise 3.3.1 : Verify the theorem from Section 3.3.3, which relates the Jaccard similarity to the probability of minhashing to equal values, for the particular case of Fig. 3.2.
(a) Compute the Jaccard similarity of each of the pairs of columns in Fig. 3.2.
! (b) Compute, for each pair of columns of that figure, the fraction of the 120
permutations of the rows that make the two columns hash to the same
value.
Exercise 3.3.2 : Using the data from Fig. 3.4, add to the signatures of the
columns the values of the following hash functions:
(a) h3(x) = 2x + 4 mod 5.
(b) h4(x) = 3x − 1 mod 5.
Element S1 S2 S3 S4
0 0 1 0 1
1 0 1 0 0
2 1 0 0 1
3 0 0 1 0
4 0 0 1 1
5 1 0 0 0
Figure 3.6: Matrix for Exercise 3.3.3
Exercise 3.3.3 : In Fig. 3.6 is a matrix with six rows.
(a) Compute the minhash signature for each column if we use the following
three hash functions: h1(x) = 2x + 1 mod 6; h2(x) = 3x + 2 mod 6;
h3(x) = 5x + 2 mod 6.
(b) Which of these hash functions are true permutations?
3.4. LOCALITY-SENSITIVE HASHING FOR DOCUMENTS 91
(c) How close are the estimated Jaccard similarities for the six pairs of columns
to the true Jaccard similarities?
! Exercise 3.3.4 : Now that we know Jaccard similarity is related to the probability that two sets minhash to the same value, reconsider Exercise 3.1.3. Can
you use this relationship to simplify the problem of computing the expected
Jaccard similarity of randomly chosen sets?
! Exercise 3.3.5 : Prove that if the Jaccard similarity of two columns is 0, then
minhashing always gives a correct estimate of the Jaccard similarity.
!! Exercise 3.3.6 : One might expect that we could estimate the Jaccard similarity of columns without using all possible permutations of rows. For example,
we could only allow cyclic permutations; i.e., start at a randomly chosen row
r, which becomes the first in the order, followed by rows r + 1, r + 2, and so
on, down to the last row, and then continuing with the first row, second row,
and so on, down to row r − 1. There are only n such permutations if there are
n rows. However, these permutations are not sufficient to estimate the Jaccard
similarity correctly. Give an example of a two-column matrix where averaging
over all the cyclic permutations does not give the Jaccard similarity.
! Exercise 3.3.7 : Suppose we want to use a MapReduce framework to compute
minhash signatures. If the matrix is stored in chunks that correspond to some
columns, then it is quite easy to exploit parallelism. Each Map task gets some
of the columns and all the hash functions, and computes the minhash signatures
of its given columns. However, suppose the matrix were chunked by rows, so
that a Map task is given the hash functions and a set of rows to work on. Design
Map and Reduce functions to exploit MapReduce with data in this form.
! Exercise 3.3.8 : As we noticed in Section 3.3.6, we have problems when a
column has only 0’s. If we compute a minhash function using entire columns
(as in Section 3.3.2), then the only time we get all 0’s in a column is if that
column represents the empty set. How should we handle the empty set to make
sure no errors in Jaccard-similarity estimation are introduced?
!! Exercise 3.3.9 : In Example 3.9, each of the three estimates of Jaccard similarity we obtained was either smaller than or the same as the true Jaccard
similarity. Is it possible that for another pair of columns, the average of the
Jaccard similarities of the upper and lower halves will exceed the actual Jaccard
similarity of the columns?
3.4 Locality-Sensitive Hashing for Documents
Even though we can use minhashing to compress large documents into small
signatures and preserve the expected similarity of any pair of documents, it
still may be impossible to find the pairs with greatest similarity efficiently. The
92 CHAPTER 3. FINDING SIMILAR ITEMS
reason is that the number of pairs of documents may be too large, even if there
are not too many documents.
Example 3.10 : Suppose we have a million documents, and we use signatures
of length 250. Then we use 1000 bytes per document for the signatures, and
the entire data fits in a gigabyte – less than a typical main memory of a laptop.
However, there are
1,000,000
2

or half a trillion pairs of documents. If it takes a
microsecond to compute the similarity of two signatures, then it takes almost
six days to compute all the similarities on that laptop. ✷
If our goal is to compute the similarity of every pair, there is nothing we
can do to reduce the work, although parallelism can reduce the elapsed time.
However, often we want only the most similar pairs or all pairs that are above
some lower bound in similarity. If so, then we need to focus our attention only
on pairs that are likely to be similar, without investigating every pair. There is
a general theory of how to provide such focus, called locality-sensitive hashing
(LSH) or near-neighbor search. In this section we shall consider a specific form
of LSH, designed for the particular problem we have been studying: documents,
represented by shingle-sets, then minhashed to short signatures. In Section 3.6
we present the general theory of locality-sensitive hashing and a number of
applications and related techniques.
3.4.1 LSH for Minhash Signatures
One general approach to LSH is to “hash” items several times, in such a way that
similar items are more likely to be hashed to the same bucket than dissimilar
items are. We then consider any pair that hashed to the same bucket for any
of the hashings to be a candidate pair. We check only the candidate pairs for
similarity. The hope is that most of the dissimilar pairs will never hash to the
same bucket, and therefore will never be checked. Those dissimilar pairs that
do hash to the same bucket are false positives; we hope these will be only a
small fraction of all pairs. We also hope that most of the truly similar pairs
will hash to the same bucket under at least one of the hash functions. Those
that do not are false negatives; we hope these will be only a small fraction of
the truly similar pairs.
If we have minhash signatures for the items, an effective way to choose the
hashings is to divide the signature matrix into b bands consisting of r rows
each. For each band, there is a hash function that takes vectors of r integers
(the portion of one column within that band) and hashes them to some large
number of buckets. We can use the same hash function for all the bands, but
we use a separate bucket array for each band, so columns with the same vector
in different bands will not hash to the same bucket.
Example 3.11 : Figure 3.7 shows part of a signature matrix of 12 rows divided
into four bands of three rows each. The second and fourth of the explicitly
shown columns each have the column vector [0, 2, 1] in the first band, so the
3.4. LOCALITY-SENSITIVE HASHING FOR DOCUMENTS 93
1 0 0 0 2
3 2 1 2 2
0 1 3 1 1
. . . . . . band 1
band 2
band 3
band 4
Figure 3.7: Dividing a signature matrix into four bands of three rows per band
will definitely hash to the same bucket in the hashing for the first band. Thus,
regardless of what those columns look like in the other three bands, this pair
of columns will be a candidate pair. It is possible that other columns, such as
the first two shown explicitly, will also hash to the same bucket according to
the hashing of the first band. However, since their column vectors are different,
[1, 3, 0] and [0, 2, 1], and there are many buckets for each hashing, we expect the
chances of an accidental collision to be very small. We shall normally assume
that two vectors hash to the same bucket if and only if they are identical.
Two columns that do not agree in band 1 have three other chances to become
a candidate pair; they might be identical in any one of these other bands.
However, observe that the more similar two columns are, the more likely it is
that they will be identical in some band. Thus, intuitively the banding strategy
makes similar columns much more likely to be candidate pairs than dissimilar
pairs. ✷
3.4.2 Analysis of the Banding Technique
Suppose we use b bands of r rows each, and suppose that a particular pair of
documents have Jaccard similarity s. Recall from Section 3.3.3 that the probability the minhash signatures for these documents agree in any one particular
row of the signature matrix is s. We can calculate the probability that these
documents (or rather their signatures) become a candidate pair as follows:
1. The probability that the signatures agree in all rows of one particular
band is s
r
.
2. The probability that the signatures disagree in at least one row of a particular band is 1 − s
r
.
3. The probability that the signatures disagree in at least one row of each
of the bands is (1 − s
r
)
b
.
94 CHAPTER 3. FINDING SIMILAR ITEMS
4. The probability that the signatures agree in all the rows of at least one
band, and therefore become a candidate pair, is 1 − (1 − s
r
)
b
.
0 1
 of documents
Jaccard similarity
Probability
of becoming
a candidate
Figure 3.8: The S-curve
It may not be obvious, but regardless of the chosen constants b and r, this
function has the form of an S-curve, as suggested in Fig. 3.8. The threshold,
that is, the value of similarity s at which the probability of becoming a candidate is 1/2, is a function of b and r. The threshold is roughly where the rise is
the steepest, and for large b and r we find that pairs with similarity above the
threshold are very likely to become candidates, while those below the threshold
are unlikely to become candidates – exactly the situation we want. An approximation to the threshold is (1/b)
1/r. For example, if b = 16 and r = 4, then the
threshold is approximately at s = 1/2, since the 4th root of 1/16 is 1/2.
Example 3.12 : Let us consider the case b = 20 and r = 5. That is, we suppose
we have signatures of length 100, divided into twenty bands of five rows each.
Figure 3.9 tabulates some of the values of the function 1 − (1 − s
5
)
20. Notice
that the threshold, the value of s at which the curve has risen halfway, is just
slightly more than 0.5. Also notice that the curve is not exactly the ideal step
function that jumps from 0 to 1 at the threshold, but the slope of the curve
in the middle is significant. For example, it rises by more than 0.6 going from
s = 0.4 to s = 0.6, so the slope in the middle is greater than 3.
For example, at s = 0.8, 1 − (0.8)5
is about 0.672. If you raise this number
to the 20th power, you get about 0.00035. Subtracting this fraction from 1
yields 0.99965. That is, if we consider two documents with 80% similarity, then
in any one band, they have only about a 33% chance of agreeing in all five rows
and thus becoming a candidate pair. However, there are 20 bands and thus 20
3.4. LOCALITY-SENSITIVE HASHING FOR DOCUMENTS 95
s 1 − (1 − s
r
)
b
.2 .006
.3 .047
.4 .186
.5 .470
.6 .802
.7 .975
.8 .9996
Figure 3.9: Values of the S-curve for b = 20 and r = 5
chances to become a candidate. Only roughly one in 3000 pairs that are as high
as 80% similar will fail to become a candidate pair and thus be a false negative.
✷
3.4.3 Combining the Techniques
We can now give an approach to finding the set of candidate pairs for similar
documents and then discovering the truly similar documents among them. It
must be emphasized that this approach can produce false negatives – pairs of
similar documents that are not identified as such because they never become
a candidate pair. There will also be false positives – candidate pairs that are
evaluated, but are found not to be sufficiently similar.
1. Pick a value of k and construct from each document the set of k-shingles.
Optionally, hash the k-shingles to shorter bucket numbers.
2. Sort the document-shingle pairs to order them by shingle.
3. Pick a length n for the minhash signatures. Feed the sorted list to the
algorithm of Section 3.3.5 to compute the minhash signatures for all the
documents.
4. Choose a threshold t that defines how similar documents have to be in
order for them to be regarded as a desired “similar pair.” Pick a number
of bands b and a number of rows r such that br = n, and the threshold
t is approximately (1/b)
1/r. If avoidance of false negatives is important,
you may wish to select b and r to produce a threshold lower than t; if
speed is important and you wish to limit false positives, select b and r to
produce a higher threshold.
5. Construct candidate pairs by applying the LSH technique of Section 3.4.1.
6. Examine each candidate pair’s signatures and determine whether the fraction of components in which they agree is at least t.
96 CHAPTER 3. FINDING SIMILAR ITEMS
7. Optionally, if the signatures are sufficiently similar, go to the documents
themselves and check that they are truly similar, rather than documents
that, by luck, had similar signatures.
3.4.4 Exercises for Section 3.4
Exercise 3.4.1 : Evaluate the S-curve 1 − (1 − s
r
)
b
for s = 0.1, 0.2, . . . , 0.9, for
the following values of r and b:
• r = 3 and b = 10.
• r = 6 and b = 20.
• r = 5 and b = 50.
! Exercise 3.4.2 : For each of the (r, b) pairs in Exercise 3.4.1, compute the
threshold, that is, the value of s for which the value of 1−(1−s
r
)
b
is exactly 1/2.
How does this value compare with the estimate of (1/b)
1/r that was suggested
in Section 3.4.2?
! Exercise 3.4.3 : Use the techniques explained in Section 1.3.5 to approximate
the S-curve 1 − (1 − s
r
)
b when s
r
is very small.
! Exercise 3.4.4 : Suppose we wish to implement LSH by MapReduce. Specifically, assume chunks of the signature matrix consist of columns, and elements
are key-value pairs where the key is the column number and the value is the
signature itself (i.e., a vector of values).
(a) Show how to produce the buckets for all the bands as output of a single
MapReduce process. Hint: Remember that a Map function can produce
several key-value pairs from a single element.
(b) Show how another MapReduce process can convert the output of (a) to
a list of pairs that need to be compared. Specifically, for each column i,
there should be a list of those columns j > i with which i needs to be
compared.
3.5 Distance Measures
We now take a short detour to study the general notion of distance measures.
The Jaccard similarity is a measure of how close sets are, although it is not
really a distance measure. That is, the closer sets are, the higher the Jaccard
similarity. Rather, 1 minus the Jaccard similarity is a distance measure, as we
shall see; it is called the Jaccard distance.
However, Jaccard distance is not the only measure of closeness that makes
sense. We shall examine in this section some other distance measures that have
applications. Then, in Section 3.6 we see how some of these distance measures
3.5. DISTANCE MEASURES 97
also have an LSH technique that allows us to focus on nearby points without
comparing all points. Other applications of distance measures will appear when
we study clustering in Chapter 7.
3.5.1 Definition of a Distance Measure
Suppose we have a set of points, called a space. A distance measure on this
space is a function d(x, y) that takes two points in the space as arguments and
produces a real number, and satisfies the following axioms:
1. d(x, y) ≥ 0 (no negative distances).
2. d(x, y) = 0 if and only if x = y (distances are positive, except for the
distance from a point to itself).
3. d(x, y) = d(y, x) (distance is symmetric).
4. d(x, y) ≤ d(x, z) + d(z, y) (the triangle inequality).
The triangle inequality is the most complex condition. It says, intuitively, that
to travel from x to y, we cannot obtain any benefit if we are forced to travel via
some particular third point z. The triangle-inequality axiom is what makes all
distance measures behave as if distance describes the length of a shortest path
from one point to another.
3.5.2 Euclidean Distances
The most familiar distance measure is the one we normally think of as “distance.” An n-dimensional Euclidean space is one where points are vectors of n
real numbers. The conventional distance measure in this space, which we shall
refer to as the L2-norm, is defined:
d([x1, x2, . . . , xn], [y1, y2, . . . , yn]) =
vuutXn
i=1
(xi − yi)
2
That is, we square the distance in each dimension, sum the squares, and take
the positive square root.
It is easy to verify the first three requirements for a distance measure are
satisfied. The Euclidean distance between two points cannot be negative, because the positive square root is intended. Since all squares of real numbers are
nonnegative, any i such that xi 6= yi forces the distance to be strictly positive.
On the other hand, if xi = yi for all i, then the distance is clearly 0. Symmetry
follows because (xi − yi)
2 = (yi − xi)
2
. The triangle inequality requires a good
deal of algebra to verify. However, it is well understood to be a property of
Euclidean space: the sum of the lengths of any two sides of a triangle is no less
than the length of the third side.
98 CHAPTER 3. FINDING SIMILAR ITEMS
There are other distance measures that have been used for Euclidean spaces.
For any constant r, we can define the Lr-norm to be the distance measure d
defined by:
d([x1, x2, . . . , xn], [y1, y2, . . . , yn]) = (Xn
i=1
|xi − yi
|
r
)
1/r
The case r = 2 is the usual L2-norm just mentioned. Another common distance
measure is the L1-norm, or Manhattan distance. There, the distance between
two points is the sum of the magnitudes of the differences in each dimension.
It is called “Manhattan distance” because it is the distance one would have to
travel between points if one were constrained to travel along grid lines, as on
the streets of a city such as Manhattan.
Another interesting distance measure is the L∞-norm, which is the limit
as r approaches infinity of the Lr-norm. As r gets larger, only the dimension
with the largest difference matters, so formally, the L∞-norm is defined as the
maximum of |xi − yi
| over all dimensions i.
Example 3.13 : Consider the two-dimensional Euclidean space (the customary plane) and the points (2, 7) and (6, 4). The L2-norm gives a distance
of p
(2 − 6)2 + (7 − 4)2 =
√
4
2 + 32 = 5. The L1-norm gives a distance of
|2 − 6| + |7 − 4| = 4 + 3 = 7. The L∞-norm gives a distance of
max(|2 − 6|, |7 − 4|) = max(4, 3) = 4
✷
3.5.3 Jaccard Distance
As mentioned at the beginning of the section, we define the Jaccard distance
of sets by d(x, y) = 1 − SIM(x, y). That is, the Jaccard distance is 1 minus the
ratio of the sizes of the intersection and union of sets x and y. We must verify
that this function is a distance measure.
1. d(x, y) is nonnegative because the size of the intersection cannot exceed
the size of the union.
2. d(x, y) = 0 if x = y, because x ∪ x = x ∩ x = x. However, if x 6= y, then
the size of x ∩ y is strictly less than the size of x ∪ y, so d(x, y) is strictly
positive.
3. d(x, y) = d(y, x) because both union and intersection are symmetric; i.e.,
x ∪ y = y ∪ x and x ∩ y = y ∩ x.
4. For the triangle inequality, recall from Section 3.3.3 that SIM(x, y) is the
probability a random minhash function maps x and y to the same value.
Thus, the Jaccard distance d(x, y) is the probability that a random minhash function does not send x and y to the same value. We can therefore
3.5. DISTANCE MEASURES 99
translate the condition d(x, y) ≤ d(x, z) + d(z, y) to the statement that if
h is a random minhash function, then the probability that h(x) 6= h(y)
is no greater than the sum of the probability that h(x) 6= h(z) and the
probability that h(z) 6= h(y). However, this statement is true because
whenever h(x) 6= h(y), at least one of h(x) and h(y) must be different
from h(z). They could not both be h(z), because then h(x) and h(y)
would be the same.
.
3.5.4 Cosine Distance
The cosine distance makes sense in spaces that have dimensions, including Euclidean spaces and discrete versions of Euclidean spaces, such as spaces where
points are vectors with integer components or Boolean (0 or 1) components. In
such a space, points may be thought of as directions. We do not distinguish between a vector and a multiple of that vector. Then the cosine distance between
two points is the angle that the vectors to those points make. This angle will
be in the range 0 to 180 degrees, regardless of how many dimensions the space
has.
We can calculate the cosine distance by first computing the cosine of the
angle, and then applying the arc-cosine function to translate to an angle in the
0-180 degree range. Given two vectors x and y, the cosine of the angle between
them is the dot product x.y divided by the L2-norms of x and y (i.e., their
Euclidean distances from the origin). Recall that the dot product of vectors
[x1, x2, . . . , xn].[y1, y2, . . . , yn] is Pn
i=1 xiyi
.
Example 3.14 : Let our two vectors be x = [1, 2, −1] and = [2, 1, 1]. The dot
√
product x.y is 1 × 2 + 2 × 1 + (−1) × 1 = 3. The L2-norm of both vectors is
6. For example, x has L2-norm p
1
2 + 22 + (−1)2 =
√
6. Thus, the cosine of
the angle between x and y is 3/(
√
6
√
6) or 1/2. The angle whose cosine is 1/2
is 60 degrees, so that is the cosine distance between x and y. ✷
We must show that the cosine distance is indeed a distance measure. We
have defined it so the values are in the range 0 to 180, so no negative distances
are possible. Two vectors have angle 0 if and only if they are the same direction.5
Symmetry is obvious: the angle between x and y is the same as the angle
between y and x. The triangle inequality is best argued by physical reasoning.
One way to rotate from x to y is to rotate to z and thence to y. The sum of
those two rotations cannot be less than the rotation directly from x to y.
5Notice that to satisfy the second axiom, we have to treat vectors that are multiples of
one another, e.g. [1, 2] and [3, 6], as the same direction, which they are. If we regarded these
as different vectors, we would give them distance 0 and thus violate the condition that only
d(x, x) is 0.
100 CHAPTER 3. FINDING SIMILAR ITEMS
3.5.5 Edit Distance
This distance makes sense when points are strings. The distance between two
strings x = x1x2 · · · xn and y = y1y2 · · · ym is the smallest number of insertions
and deletions of single characters that will convert x to y.
Example 3.15 : The edit distance between the strings x = abcde and y =
acfdeg is 3. To convert x to y:
1. Delete b.
2. Insert f after c.
3. Insert g after e.
No sequence of fewer than three insertions and/or deletions will convert x to y.
Thus, d(x, y) = 3. ✷
Another way to define and calculate the edit distance d(x, y) is to compute
a longest common subsequence (LCS) of x and y. An LCS of x and y is a
string that is constructed by deleting positions from x and y, and that is as
long as any string that can be constructed that way. The edit distance d(x, y)
can be calculated as the length of x plus the length of y minus twice the length
of their LCS.
Example 3.16 : The strings x = abcde and y = acfdeg from Example 3.15
have a unique LCS, which is acde. We can be sure it is the longest possible,
because it contains every symbol appearing in both x and y. Fortunately, these
common symbols appear in the same order in both strings, so we are able to
use them all in an LCS. Note that the length of x is 5, the length of y is 6, and
the length of their LCS is 4. The edit distance is thus 5 + 6 − 2 × 4 = 3, which
agrees with the direct calculation in Example 3.15.
For another example, consider x = aba and y = bab. Their edit distance is
2. For example, we can convert x to y by deleting the first a and then inserting
b at the end. There are two LCS’s: ab and ba. Each can be obtained by
deleting one symbol from each string. As must be the case for multiple LCS’s
of the same pair of strings, both LCS’s have the same length. Therefore, we
may compute the edit distance as 3 + 3 − 2 × 2 = 2. ✷
Edit distance is a distance measure. Surely no edit distance can be negative,
and only two identical strings have an edit distance of 0. To see that edit
distance is symmetric, note that a sequence of insertions and deletions can be
reversed, with each insertion becoming a deletion, and vice versa. The triangle
inequality is also straightforward. One way to turn a string s into a string t
is to turn s into some string u and then turn u into t. Thus, the number of
edits made going from s to u, plus the number of edits made going from u to t
cannot be less than the smallest number of edits that will turn s into t.
3.5. DISTANCE MEASURES 101
Non-Euclidean Spaces
Notice that several of the distance measures introduced in this section are
not Euclidean spaces. A property of Euclidean spaces that we shall find
important when we take up clustering in Chapter 7 is that the average
of points in a Euclidean space always exists and is a point in the space.
However, consider the space of sets for which we defined the Jaccard distance. The notion of the “average” of two sets makes no sense. Likewise,
the space of strings, where we can use the edit distance, does not let us
take the “average” of strings.
Vector spaces, for which we suggested the cosine distance, may or may
not be Euclidean. If the components of the vectors can be any real numbers, then the space is Euclidean. However, if we restrict components to
be integers, then the space is not Euclidean. Notice that, for instance, we
cannot find an average of the vectors [1, 2] and [3, 1] in the space of vectors
with two integer components, although if we treated them as members of
the two-dimensional Euclidean space, then we could say that their average
was [2.0, 1.5].
3.5.6 Hamming Distance
Given a space of vectors, we define the Hamming distance between two vectors
to be the number of components in which they differ. It should be obvious
that Hamming distance is a distance measure. Clearly the Hamming distance
cannot be negative, and if it is zero, then the vectors are identical. The distance does not depend on which of two vectors we consider first. The triangle
inequality should also be evident. If x and z differ in m components, and z
and y differ in n components, then x and y cannot differ in more than m + n
components. Most commonly, Hamming distance is used when the vectors are
Boolean; they consist of 0’s and 1’s only. However, in principle, the vectors can
have components from any set.
Example 3.17 : The Hamming distance between the vectors 10101 and 11110
is 3. That is, these vectors differ in the second, fourth, and fifth components,
while they agree in the first and third components. ✷
3.5.7 Exercises for Section 3.5
! Exercise 3.5.1 : On the space of nonnegative integers, which of the following
functions are distance measures? If so, prove it; if not, prove that it fails to
satisfy one or more of the axioms.
(a) max(x, y) = the larger of x and y.
102 CHAPTER 3. FINDING SIMILAR ITEMS
(b) diff(x, y) = |x − y| (the absolute magnitude of the difference between x
and y).
(c) sum(x, y) = x + y.
Exercise 3.5.2 : Find the L1 and L2 distances between the points (5, 6, 7) and
(8, 2, 4).
!! Exercise 3.5.3 : Prove that if i and j are any positive integers, and i < j,
then the Li norm between any two points is greater than the Lj norm between
those same two points.
Exercise 3.5.4 : Find the Jaccard distances between the following pairs of
sets:
(a) {1, 2, 3, 4} and {2, 3, 4, 5}.
(b) {1, 2, 3} and {4, 5, 6}.
Exercise 3.5.5 : Compute the cosines of the angles between each of the following pairs of vectors.6
(a) (3, −1, 2) and (−2, 3, 1).
(b) (1, 2, 3) and (2, 4, 6).
(c) (5, 0, −4) and (−1, −6, 2).
(d) (0, 1, 1, 0, 1, 1) and (0, 0, 1, 0, 0, 0).
! Exercise 3.5.6 : Prove that the cosine distance between any two vectors of 0’s
and 1’s, of the same length, is at most 90 degrees.
Exercise 3.5.7 : Find the edit distances (using only insertions and deletions)
between the following pairs of strings.
(a) abcdef and bdaefc.
(b) abccdabc and acbdcab.
(c) abcdef and baedfc.
! Exercise 3.5.8 : There are a number of other notions of edit distance available.
For instance, we can allow, in addition to insertions and deletions, the following
operations:
6Note that what we are asking for is not precisely the cosine distance, but from the cosine
of an angle, you can compute the angle itself, perhaps with the aid of a table or library
function.
3.6. THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS 103
i. Mutation, where one symbol is replaced by another symbol. Note that a
mutation can always be performed by an insertion followed by a deletion,
but if we allow mutations, then this change counts for only 1, not 2, when
computing the edit distance.
ii. Transposition, where two adjacent symbols have their positions swapped.
Like a mutation, we can simulate a transposition by one insertion followed
by one deletion, but here we count only 1 for these two steps.
Repeat Exercise 3.5.7 if edit distance is defined to be the number of insertions,
deletions, mutations, and transpositions needed to transform one string into
another.
! Exercise 3.5.9 : Prove that the edit distance discussed in Exercise 3.5.8 is
indeed a distance measure.
Exercise 3.5.10 : Find the Hamming distances between each pair of the following vectors: 000000, 110011, 010101, and 011100.
3.6 The Theory of Locality-Sensitive Functions
The LSH technique developed in Section 3.4 is one example of a family of functions (the minhash functions) that can be combined (by the banding technique)
to distinguish strongly between pairs at a low distance from pairs at a high distance. The steepness of the S-curve in Fig. 3.8 reflects how effectively we can
avoid false positives and false negatives among the candidate pairs.
Now, we shall explore other families of functions, besides the minhash functions, that can serve to produce candidate pairs efficiently. These functions can
apply to the space of sets and the Jaccard distance, or to another space and/or
another distance measure. There are three conditions that we need for a family
of functions:
1. They must be more likely to make close pairs be candidate pairs than
distant pairs. We make this notion precise in Section 3.6.1.
2. They must be statistically independent, in the sense that it is possible to
estimate the probability that two or more functions will all give a certain
response by the product rule for independent events.
3. They must be efficient, in two ways:
(a) They must be able to identify candidate pairs in time much less
than the time it takes to look at all pairs. For example, minhash
functions have this capability, since we can hash sets to minhash
values in time proportional to the size of the data, rather than the
square of the number of sets in the data. Since sets with common
values are colocated in a bucket, we have implicitly produced the
104 CHAPTER 3. FINDING SIMILAR ITEMS
candidate pairs for a single minhash function in time much less than
the number of pairs of sets.
(b) They must be combinable to build functions that are better at avoiding false positives and negatives, and the combined functions must
also take time that is much less than the number of pairs. For example, the banding technique of Section 3.4.1 takes single minhash
functions, which satisfy condition 3a but do not, by themselves have
the S-curve behavior we want, and produces from a number of minhash functions a combined function that has the S-curve shape.
Our first step is to define “locality-sensitive functions” generally. We then
see how the idea can be applied in several applications. Finally, we discuss
how to apply the theory to arbitrary data with either a cosine distance or a
Euclidean distance measure.
3.6.1 Locality-Sensitive Functions
For the purposes of this section, we shall consider functions that take two items
and render a decision about whether these items should be a candidate pair.
In many cases, the function f will “hash” items, and the decision will be based
on whether or not the result is equal. Because it is convenient to use the
notation f(x) = f(y) to mean that f(x, y) is “yes; make x and y a candidate
pair,” we shall use f(x) = f(y) as a shorthand with this meaning. We also use
f(x) 6= f(y) to mean “do not make x and y a candidate pair unless some other
function concludes we should do so.”
A collection of functions of this form will be called a family of functions.
For example, the family of minhash functions, each based on one of the possible
permutations of rows of a characteristic matrix, form a family.
Let d1 < d2 be two distances according to some distance measure d. A
family F of functions is said to be (d1, d2, p1, p2)-sensitive if for every f in F:
1. If d(x, y) ≤ d1, then the probability that f(x) = f(y) is at least p1.
2. If d(x, y) ≥ d2, then the probability that f(x) = f(y) is at most p2.
Figure 3.10 illustrates what we expect about the probability that a given
function in a (d1, d2, p1, p2)-sensitive family will declare two items to be a candidate pair. Notice that we say nothing about what happens when the distance
between the items is strictly between d1 and d2, but we can make d1 and d2 as
close as we wish. The penalty is that typically p1 and p2 are then close as well.
As we shall see, it is possible to drive p1 and p2 apart while keeping d1 and d2
fixed.
3.6.2 Locality-Sensitive Families for Jaccard Distance
For the moment, we have only one way to find a family of locality-sensitive
functions: use the family of minhash functions, and assume that the distance
3.6. THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS 105
Probabilty
of being
declared a
candidate
d
p
d
p
1 2
1
2
Distance
Figure 3.10: Behavior of a (d1, d2, p1, p2)-sensitive function
measure is the Jaccard distance. As before, we interpret a minhash function h
to make x and y a candidate pair if and only if h(x) = h(y).
• The family of minhash functions is a (d1, d2, 1−d1, 1−d2)-sensitive family
for any d1 and d2, where 0 ≤ d1 < d2 ≤ 1.
The reason is that if d(x, y) ≤ d1, where d is the Jaccard distance, then
SIM(x, y) = 1 − d(x, y) ≥ 1 − d1. But we know that the Jaccard similarity
of x and y is equal to the probability that a minhash function will hash x and
y to the same value. A similar argument applies to d2 or any distance.
Example 3.18 : We could let d1 = 0.3 and d2 = 0.6. Then we can assert that
the family of minhash functions is a (0.3, 0.6, 0.7, 0.4)-sensitive family. That is,
if the Jaccard distance between x and y is at most 0.3 (i.e., SIM(x, y) ≥ 0.7)
then there is at least a 0.7 chance that a minhash function will send x and y to
the same value, and if the Jaccard distance between x and y is at least 0.6 (i.e.,
SIM(x, y) ≤ 0.4), then there is at most a 0.4 chance that x and y will be sent
to the same value. Note that we could make the same assertion with another
choice of d1 and d2; only d1 < d2 is required. ✷
3.6.3 Amplifying a Locality-Sensitive Family
Suppose we are given a (d1, d2, p1, p2)-sensitive family F. We can construct a
new family F
′
by the AND-construction on F, which is defined as follows. Each
member of F
′
consists of r members of F for some fixed r. If f is in F
′
, and f is
constructed from the set {f1, f2, . . . , fr} of members of F, we say f(x) = f(y)
if and only if fi(x) = fi(y) for all i = 1, 2, . . . , r. Notice that this construction
mirrors the effect of the r rows in a single band: the band makes x and y a
106 CHAPTER 3. FINDING SIMILAR ITEMS
candidate pair if every one of the r rows in the band say that x and y are equal
(and therefore a candidate pair according to that row).
Since the members of F are independently chosen to make a member of F
′
,
we can assert that F
′
is a
d1, d2,(p1)
r
,(p2)
r

-sensitive family. That is, for any
p, if p is the probability that a member of F will declare (x, y) to be a candidate
pair, then the probability that a member of F
′ will so declare is p
r
.
There is another construction, which we call the OR-construction, that turns
a (d1, d2, p1, p2)-sensitive family F into a
d1, d2, 1 − (1 − p1)
b
, 1 − (1 − p2)
b

-
sensitive family F
′
. Each member f of F
′
is constructed from b members of F,
say f1, f2, . . . , fb. We define f(x) = f(y) if and only if fi(x) = fi(y) for one or
more values of i. The OR-construction mirrors the effect of combining several
bands: x and y become a candidate pair if any band makes them a candidate
pair.
If p is the probability that a member of F will declare (x, y) to be a candidate
pair, then 1−p is the probability it will not so declare. (1−p)
b
is the probability
that none of f1, f2, . . . , fb will declare (x, y) a candidate pair, and 1 − (1 − p)
b
is the probability that at least one fi will declare (x, y) a candidate pair, and
therefore that f will declare (x, y) to be a candidate pair.
Notice that the AND-construction lowers all probabilities, but if we choose F
and r judiciously, we can make the small probability p2 get very close to 0, while
the higher probability p1 stays significantly away from 0. Similarly, the ORconstruction makes all probabilities rise, but by choosing F and b judiciously,
we can make the larger probability approach 1 while the smaller probability
remains bounded away from 1. We can cascade AND- and OR-constructions in
any order to make the low probability close to 0 and the high probability close
to 1. Of course the more constructions we use, and the higher the values of r
and b that we pick, the larger the number of functions from the original family
that we are forced to use. Thus, the better the final family of functions is, the
longer it takes to apply the functions from this family.
Example 3.19 : Suppose we start with a family F. We use the AND-construction with r = 4 to produce a family F1. We then apply the OR-construction
to F1 with b = 4 to produce a third family F2. Note that the members of F2
each are built from 16 members of F, and the situation is analogous to starting
with 16 minhash functions and treating them as four bands of four rows each.
The 4-way AND-function converts any probability p into p
4
. When we
follow it by the 4-way OR-construction, that probability is further converted
into 1−(1−p
4
)
4
. Some values of this transformation are indicated in Fig. 3.11.
This function is an S-curve, staying low for a while, then rising steeply (although
not too steeply; the slope never gets much higher than 2), and then leveling
off at high values. Like any S-curve, it has a fixedpoint, the value of p that is
left unchanged when we apply the function of the S-curve. In this case, the
fixedpoint is the value of p for which p = 1 − (1 − p
4
)
4
. We can see that the
fixedpoint is somewhere between 0.7 and 0.8. Below that value, probabilities are
decreased, and above it they are increased. Thus, if we pick a high probabili
3.6. THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS 107
p 1 − (1 − p
4
)
4
0.2 0.0064
0.3 0.0320
0.4 0.0985
0.5 0.2275
0.6 0.4260
0.7 0.6666
0.8 0.8785
0.9 0.9860
Figure 3.11: Effect of the 4-way AND-construction followed by the 4-way ORconstruction
above the fixedpoint and a low probability below it, we shall have the desired
effect that the low probability is decreased and the high probability is increased.
Suppose F is the minhash functions, regarded as a (0.2, 0.6, 0.8, 0.4)-sensitive family. Then F2, the family constructed by a 4-way AND followed by a
4-way OR, is a (0.2, 0.6, 0.8785, 0.0985)-sensitive family, as we can read from the
rows for 0.8 and 0.4 in Fig. 3.11. By replacing F by F2, we have reduced both
the false-negative and false-positive rates, at the cost of making application of
the functions take 16 times as long. ✷
p

1 − (1 − p)
4
4
0.1 0.0140
0.2 0.1215
0.3 0.3334
0.4 0.5740
0.5 0.7725
0.6 0.9015
0.7 0.9680
0.8 0.9936
Figure 3.12: Effect of the 4-way OR-construction followed by the 4-way ANDconstruction
Example 3.20 : For the same cost, we can apply a 4-way OR-construction
followed by a 4-way AND-construction. Figure 3.12 gives the transformation
on probabilities implied by this construction. For instance, suppose that F is a
(0.2, 0.6, 0.8, 0.4)-sensitive family. Then the constructed family is a
(0.2, 0.6, 0.9936, 0.5740)-sensitiv
108 CHAPTER 3. FINDING SIMILAR ITEMS
family. This choice is not necessarily the best. Although the higher probability
has moved much closer to 1, the lower probability has also raised, increasing
the number of false positives. ✷
Example 3.21 : We can cascade constructions as much as we like. For example, we could use the construction of Example 3.19 on the family of minhash
functions and then use the construction of Example 3.20 on the resulting family.
The constructed family would then have functions each built from 256 minhash
functions. It would, for instance transform a (0.2, 0.8, 0.8, 0.2)-sensitive family
into a (0.2, 0.8, 0.9991285, 0.0000004)-sensitive family. ✷
3.6.4 Exercises for Section 3.6
Exercise 3.6.1 : What is the effect on probability of starting with the family
of minhash functions and applying:
(a) A 2-way AND construction followed by a 3-way OR construction.
(b) A 3-way OR construction followed by a 2-way AND construction.
(c) A 2-way AND construction followed by a 2-way OR construction, followed
by a 2-way AND construction.
(d) A 2-way OR construction followed by a 2-way AND construction, followed
by a 2-way OR construction followed by a 2-way AND construction.
Exercise 3.6.2 : Find the fixedpoints for each of the functions constructed in
Exercise 3.6.1.
! Exercise 3.6.3 : Any function of probability p, such as that of Fig. 3.11, has
a slope given by the derivative of the function. The maximum slope is where
that derivative is a maximum. Find the value of p that gives a maximum slope
for the S-curves given by Fig. 3.11 and Fig. 3.12. What are the values of these
maximum slopes?
!! Exercise 3.6.4 : Generalize Exercise 3.6.3 to give, as a function of r and b, the
point of maximum slope and the value of that slope, for families of functions
defined from the minhash functions by:
(a) An r-way AND construction followed by a b-way OR construction.
(b) A b-way OR construction followed by an r-way AND construction.
3.7 LSH Families for Other Distance Measures
There is no guarantee that a distance measure has a locality-sensitive family of
hash functions. So far, we have only seen such families for the Jaccard distance.
In this section, we shall show how to construct locality-sensitive families for
Hamming distance, the cosine distance and for the normal Euclidean distance.
3.7. LSH FAMILIES FOR OTHER DISTANCE MEASURES 109
3.7.1 LSH Families for Hamming Distance
It is quite simple to build a locality-sensitive family of functions for the Hamming distance. Suppose we have a space of d-dimensional vectors, and h(x, y)
denotes the Hamming distance between vectors x and y. If we take any one
position of the vectors, say the ith position, we can define the function fi(x)
to be the ith bit of vector x. Then fi(x) = fi(y) if and only if vectors x and
y agree in the ith position. Then the probability that fi(x) = fi(y) for a randomly chosen i is exactly 1 − h(x, y)/d; i.e., it is the fraction of positions in
which x and y agree.
This situation is almost exactly like the one we encountered for minhashing.
Thus, the family F consisting of the functions {f1, f2, . . . , fd} is a
(d1, d2, 1 − d1/d, 1 − d2/d)-sensitive
family of hash functions, for any d1 < d2. There are only two differences
between this family and the family of minhash functions.
1. While Jaccard distance runs from 0 to 1, the Hamming distance on a
vector space of dimension d runs from 0 to d. It is therefore necessary to
scale the distances by dividing by d, to turn them into probabilities.
2. While there is essentially an unlimited supply of minhash functions, the
size of the family F for Hamming distance is only d.
The first point is of no consequence; it only requires that we divide by d at
appropriate times. The second point is more serious. If d is relatively small,
then we are limited in the number of functions that can be composed using
the AND and OR constructions, thereby limiting how steep we can make the
S-curve be.
3.7.2 Random Hyperplanes and the Cosine Distance
Recall from Section 3.5.4 that the cosine distance between two vectors is the
angle between the vectors. For instance, we see in Fig. 3.13 two vectors x
and y that make an angle θ between them. Note that these vectors may be
in a space of many dimensions, but they always define a plane, and the angle
between them is measured in this plane. Figure 3.13 is a “top-view” of the
plane containing x and y.
Suppose we pick a hyperplane through the origin. This hyperplane intersects
the plane of x and y in a line. Figure 3.13 suggests two possible hyperplanes,
one whose intersection is the dashed line and the other’s intersection is the
dotted line. To pick a random hyperplane, we actually pick the normal vector
to the hyperplane, say v. The hyperplane is then the set of points whose dot
product with v is 0.
First, consider a vector v that is normal to the hyperplane whose projection
is represented by the dashed line in Fig. 3.13; that is, x and y are on different
110 CHAPTER 3. FINDING SIMILAR ITEMS
θ
x
y
Figure 3.13: Two vectors make an angle θ
sides of the hyperplane. Then the dot products v.x and v.y will have different
signs. If we assume, for instance, that v is a vector whose projection onto the
plane of x and y is above the dashed line in Fig. 3.13, then v.x is positive,
while v.y is negative. The normal vector v instead might extend in the opposite
direction, below the dashed line. In that case v.x is negative and v.y is positive,
but the signs are still different.
On the other hand, the randomly chosen vector v could be normal to a
hyperplane like the dotted line in Fig. 3.13. In that case, both v.x and v.y
have the same sign. If the projection of v extends to the right, then both dot
products are positive, while if v extends to the left, then both are negative.
What is the probability that the randomly chosen vector is normal to a
hyperplane that looks like the dashed line rather than the dotted line? All
angles for the line that is the intersection of the random hyperplane and the
plane of x and y are equally likely. Thus, the hyperplane will look like the
dashed line with probability θ/180 and will look like the dotted line otherwise.
Thus, each hash function f in our locality-sensitive family F is built from
a randomly chosen vector vf . Given two vectors x and y, say f(x) = f(y) if
and only if the dot products vf .x and vf .y have the same sign. Then F is a
locality-sensitive family for the cosine distance. The parameters are essentially
the same as for the Jaccard-distance family described in Section 3.6.2, except
the scale of distances is 0–180 rather than 0–1. That is, F is a
(d1, d2,(180 − d1)/180,(180 − d2)/180)-sensitive
family of hash functions. From this basis, we can amplify the family as we wish,
just as for the minhash-based family.
3.7. LSH FAMILIES FOR OTHER DISTANCE MEASURES 111
3.7.3 Sketches
Instead of chosing a random vector from all possible vectors, it turns out to be
sufficiently random if we restrict our choice to vectors whose components are
+1 and −1. The dot product of any vector x with a vector v of +1’s and −1’s
is formed by adding the components of x where v is +1 and then subtracting
the other components of x – those where v is −1.
If we pick a collection of random vectors, say v1, v2, . . . , vn, then we can
apply them to an arbitrary vector x by computing v1.x, v2.x, . . . , vn.x and then
replacing any positive value by +1 and any negative value by −1. The result is
called the sketch of x. You can handle 0’s arbitrarily, e.g., by chosing a result +1
or −1 at random. Since there is only a tiny probability of a zero dot product,
the choice has essentially no effect.
Example 3.22 : Suppose our space consists of 4-dimensional vectors, and we
pick three random vectors: v1 = [+1, −1, +1, +1], v2 = [−1, +1, −1, +1], and
v3 = [+1, +1, −1, −1]. For the vector x = [3, 4, 5, 6], the sketch is [+1, +1, −1].
That is, v1.x = 3−4+5+6 = 10. Since the result is positive, the first component
of the sketch is +1. Similarly, v2.x = 2 and v3.x = −4, so the second component
of the sketch is +1 and the third component is −1.
Consider the vector y = [4, 3, 2, 1]. We can similarly compute its sketch to
be [+1, −1, +1]. Since the sketches for x and y agree in 1/3 of the positions,
we estimate that the angle between them is 120 degrees. That is, a randomly
chosen hyperplane is twice as likely to look like the dashed line in Fig. 3.13 than
like the dotted line.
The above conclusion turns out to be quite wrong. We can calculate the
cosine of the angle between x and y to be x.y, which is
6 × 1 + 5 × 2 + 4 × 3 + 3 × 4 = 40
divided by the magnitudes of the two vectors. These magnitudes are
p
6
2 + 52 + 42 + 32 = 9.274
and √
1
2 + 22 + 32 + 42 = 5.477. Thus, the cosine of the angle between x and
y is 0.7875, and this angle is about 38 degrees. However, if you look at all
16 different vectors v of length 4 that have +1 and −1 as components, you
find that there are only four of these whose dot products with x and y have
a different sign, namely v2, v3, and their complements [+1, −1, +1, −1] and
[−1, −1, +1, +1]. Thus, had we picked all sixteen of these vectors to form a
sketch, the estimate of the angle would have been 180/4 = 45 degrees. ✷
3.7.4 LSH Families for Euclidean Distance
Now, let us turn to the Euclidean distance (Section 3.5.2), and see if we can
develop a locality-sensitive family of hash functions for this distance. We shall
start with a 2-dimensional Euclidean space. Each hash function f in our family
112 CHAPTER 3. FINDING SIMILAR ITEMS
F will be associated with a randomly chosen line in this space. Pick a constant
a and divide the line into segments of length a, as suggested by Fig. 3.14, where
the “random” line has been oriented to be horizontal.
θ
Points at
distance
Bucket
width a
d
Figure 3.14: Two points at distance d ≫ a have a small chance of being hashed
to the same bucket
The segments of the line are the buckets into which function f hashes points.
A point is hashed to the bucket in which its projection onto the line lies. If the
distance d between two points is small compared with a, then there is a good
chance the two points hash to the same bucket, and thus the hash function f
will declare the two points equal. For example, if d = a/2, then there is at least
a 50% chance the two points will fall in the same bucket. In fact, if the angle
θ between the randomly chosen line and the line connecting the points is large,
then there is an even greater chance that the two points will fall in the same
bucket. For instance, if θ is 90 degrees, then the two points are certain to fall
in the same bucket.
However, suppose d is larger than a. In order for there to be any chance of
the two points falling in the same bucket, we need d cos θ ≤ a. The diagram of
Fig. 3.14 suggests why this requirement holds. Note that even if d cos θ ≪ a it
is still not certain that the two points will fall in the same bucket. However,
we can guarantee the following. If d ≥ 2a, then there is no more than a 1/3
chance the two points fall in the same bucket. The reason is that for cos θ to
be less than 1/2, we need to have θ in the range 60 to 90 degrees. If θ is in the
range 0 to 60 degrees, then cos θ is more than 1/2. But since θ is the smaller
angle between two randomly chosen lines in the plane, θ is twice as likely to be
between 0 and 60 as it is to be between 60 and 90.
We conclude that the family F just described forms a (a/2, 2a, 1/2, 1/3)-
sensitive family of hash functions. That is, for distances up to a/2 the probability is at least 1/2 that two points at that distance will fall in the same bucket,
while for distances at least 2a the probability points at that distance will fall in
3.7. LSH FAMILIES FOR OTHER DISTANCE MEASURES 113
the same bucket is at most 1/3. We can amplify this family as we like, just as
for the other examples of locality-sensitive hash functions we have discussed.
3.7.5 More LSH Families for Euclidean Spaces
There is something unsatisfying about the family of hash functions developed
in Section 3.7.4. First, the technique was only described for two-dimensional
Euclidean spaces. What happens if our data is points in a space with many
dimensions? Second, for Jaccard and cosine distances, we were able to develop
locality-sensitive families for any pair of distances d1 and d2 as long as d1 < d2.
In Section 3.7.4 we appear to need the stronger condition d1 < 4d2.
However, we claim that there is a locality-sensitive family of hash functions for any d1 < d2 and for any number of dimensions. The family’s hash
functions still derive from random lines through the space and a bucket size
a that partitions the line. We still hash points by projecting them onto the
line. Given that d1 < d2, we may not know what the probability p1 is that two
points at distance d1 hash to the same bucket, but we can be certain that it
is greater than p2, the probability that two points at distance d2 hash to the
same bucket. The reason is that this probability surely grows as the distance
shrinks. Thus, even if we cannot calculate p1 and p2 easily, we know that there
is a (d1, d2, p1, p2)-sensitive family of hash functions for any d1 < d2 and any
given number of dimensions.
Using the amplification techniques of Section 3.6.3, we can then adjust the
two probabilities to surround any particular value we like, and to be as far apart
as we like. Of course, the further apart we want the probabilities to be, the
larger the number of basic hash functions in F we must use.
3.7.6 Exercises for Section 3.7
Exercise 3.7.1 : Suppose we construct the basic family of six locality-sensitive
functions for vectors of length six. For each pair of the vectors 000000, 110011,
010101, and 011100, which of the six functions makes them candidates?
Exercise 3.7.2 : Let us compute sketches using the following four “random”
vectors:
v1 = [+1, +1, +1, −1] v2 = [+1, +1, −1, +1]
v3 = [+1, −1, +1, +1] v4 = [−1, +1, +1, +1]
Compute the sketches of the following vectors.
(a) [2, 3, 4, 5].
(b) [−2, 3, −4, 5].
(c) [2, −3, 4, −5].
114 CHAPTER 3. FINDING SIMILAR ITEMS
For each pair, what is the estimated angle between them, according to the
sketches? What are the true angles?
Exercise 3.7.3 : Suppose we form sketches by using all sixteen of the vectors
of length 4, whose components are each +1 or −1. Compute the sketches of
the three vectors in Exercise 3.7.2. How do the estimates of the angles between
each pair compare with the true angles?
Exercise 3.7.4 : Suppose we form sketches using the four vectors from Exercise 3.7.2.
! (a) What are the constraints on a, b, c, and d that will cause the sketch of
the vector [a, b, c, d] to be [+1, +1, +1, +1]?
!! (b) Consider two vectors [a, b, c, d] and [e, f, g, h]. What are the conditions on
a, b, . . . , h that will make the sketches of these two vectors be the same?
Exercise 3.7.5 : Suppose we have points in a 3-dimensional Euclidean space:
p1 = (1, 2, 3), p2 = (0, 2, 4), and p3 = (4, 3, 2). Consider the three hash functions
defined by the three axes (to make our calculations very easy). Let buckets be
of length a, with one bucket the interval [0, a) (i.e., the set of points x such that
0 ≤ x < a), the next [a, 2a), the previous one [−a, 0), and so on.
(a) For each of the three lines, assign each of the points to buckets, assuming
a = 1.
(b) Repeat part (a), assuming a = 2.
(c) What are the candidate pairs for the cases a = 1 and a = 2?
! (d) For each pair of points, for what values of a will that pair be a candidate
pair?
3.8 Applications of Locality-Sensitive Hashing
In this section, we shall explore three examples of how LSH is used in practice.
In each case, the techniques we have learned must be modified to meet certain
constraints of the problem. The three subjects we cover are:
1. Entity Resolution: This term refers to matching data records that refer to
the same real-world entity, e.g., the same person. The principal problem
addressed here is that the similarity of records does not match exactly
either the similar-sets or similar-vectors models of similarity on which the
theory is built.
2. Matching Fingerprints: It is possible to represent fingerprints as sets.
However, we shall explore a different family of locality-sensitive hash functions from the one we get by minhashing.
3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 115
3. Matching Newspaper Articles: Here, we consider a different notion of
shingling that focuses attention on the core article in an on-line newspaper’s Web page, ignoring all the extraneous material such as ads and
newspaper-specific material.
3.8.1 Entity Resolution
It is common to have several data sets available, and to know that they refer to
some of the same entities. For example, several different bibliographic sources
provide information about many of the same books or papers. In the general
case, we have records describing entities of some type, such as people or books.
The records may all have the same format, or they may have different formats,
with different kinds of information.
There are many reasons why information about an entity may vary, even if
the field in question is supposed to be the same. For example, names may be
expressed differently in different records because of misspellings, absence of a
middle initial, use of a nickname, and many other reasons. For example, “Bob
S. Jomes” and “Robert Jones Jr.” may or may not be the same person. If
records come from different sources, the fields may differ as well. One source’s
records may have an “age” field, while another does not. The second source
might have a “date of birth” field, or it may have no information at all about
when a person was born.
3.8.2 An Entity-Resolution Example
We shall examine a real example of how LSH was used to deal with an entityresolution problem. Company A was engaged by Company B to solicit customers for B. Company B would pay A a yearly fee, as long as the customer
maintained their subscription. They later quarreled and disagreed over how
many customers A had provided to B. Each had about 1,000,000 records, some
of which described the same people; those were the customers A had provided
to B. The records had different data fields, but unfortunately none of those
fields was “this is a customer that A had provided to B.” Thus, the problem
was to match records from the two sets to see if a pair represented the same
person.
Each record had fields for the name, address, and phone number of the
person. However, the values in these fields could differ for many reasons. Not
only were there the misspellings and other naming differences mentioned in
Section 3.8.1, but there were other opportunities to disagree as well. A customer
might give their home phone to A and their cell phone to B. Or they might
move, and tell B but not A (because they no longer had need for a relationship
with A). Area codes of phones sometimes change.
The strategy for identifying records involved scoring the differences in three
fields: name, address, and phone. To create a score describing the likelihood
that two records, one from A and the other from B, described the same per-
116 CHAPTER 3. FINDING SIMILAR ITEMS
son, 100 points was assigned to each of the three fields, so records with exact
matches in all three fields got a score of 300. However, there were deductions for
mismatches in each of the three fields. As a first approximation, edit-distance
(Section 3.5.5) was used, but the penalty grew quadratically with the distance.
Then, certain publicly available tables were used to reduce the penalty in appropriate situations. For example, “Bill” and “William” were treated as if they
differed in only one letter, even though their edit-distance is 5.
However, it is not feasible to score all one trillion pairs of records. Thus,
a simple LSH was used to focus on likely candidates. Three “hash functions”
were used. The first sent records to the same bucket only if they had identical
names; the second did the same but for identical addresses, and the third did
the same for phone numbers. In practice, there was no hashing; rather the
records were sorted by name, so records with identical names would appear
consecutively and get scored for overall similarity of the name, address, and
phone. Then the records were sorted by address, and those with the same
address were scored. Finally, the records were sorted a third time by phone,
and records with identical phones were scored.
This approach missed a record pair that truly represented the same person
but none of the three fields matched exactly. Since the goal was to prove in
a court of law that the persons were the same, it is unlikely that such a pair
would have been accepted by a judge as sufficiently similar anyway.
3.8.3 Validating Record Matches
What remains is to determine how high a score indicates that two records truly
represent the same individual. In the example at hand, there was an easy
way to make that decision, and the technique can be applied in many similar
situations. It was decided to look at the creation-dates for the records at hand,
and to assume that 90 days was an absolute maximum delay between the time
the service was bought at Company A and registered at B. Thus, a proposed
match between two records that were chosen at random, subject only to the
constraint that the date on the B-record was between 0 and 90 days after the
date on the A-record, would have an average delay of 45 days.
It was found that of the pairs with a perfect 300 score, the average delay was
10 days. If you assume that 300-score pairs are surely correct matches, then you
can look at the pool of pairs with any given score s, and compute the average
delay of those pairs. Suppose that the average delay is x, and the fraction of
true matches among those pairs with score s is f. Then x = 10f + 45(1 − f),
or x = 45 − 35f. Solving for f, we find that the fraction of the pairs with score
s that are truly matches is (45 − x)/35.
The same trick can be used whenever:
1. There is a scoring system used to evaluate the likelihood that two records
represent the same entity, and
3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 117
When Are Record Matches Good Enough?
While every case will be different, it may be of interest to know how the
experiment of Section 3.8.3 turned out on the data of Section 3.8.2. For
scores down to 185, the value of x was very close to 10; i.e., these scores
indicated that the likelihood of the records representing the same person
was essentially 1. Note that a score of 185 in this example represents a
situation where one field is the same (as would have to be the case, or the
records would never even be scored), one field was completely different,
and the third field had a small discrepancy. Moreover, for scores as low as
115, the value of x was noticeably less than 45, meaning that some of these
pairs did represent the same person. Note that a score of 115 represents
a case where one field is the same, but there is only a slight similarity in
the other two fields.
2. There is some field, not used in the scoring, from which we can derive a
measure that differs, on average, for true pairs and false pairs.
For instance, suppose there were a “height” field recorded by both companies
A and B in our running example. We can compute the average difference in
height for pairs of random records, and we can compute the average difference in
height for records that have a perfect score (and thus surely represent the same
entities). For a given score s, we can evaluate the average height difference of the
pairs with that score and estimate the probability of the records representing
the same entity. That is, if h0 is the average height difference for the perfect
matches, h1 is the average height difference for random pairs, and h is the
average height difference for pairs of score s, then the fraction of good pairs
with score s is (h1 − h)/(h1 − h0).
3.8.4 Matching Fingerprints
When fingerprints are matched by computer, the usual representation is not
an image, but a set of locations in which minutiae are located. A minutia,
in the context of fingerprint descriptions, is a place where something unusual
happens, such as two ridges merging or a ridge ending. If we place a grid over a
fingerprint, we can represent the fingerprint by the set of grid squares in which
minutiae are located.
Ideally, before overlaying the grid, fingerprints are normalized for size and
orientation, so that if we took two images of the same finger, we would find
minutiae lying in exactly the same grid squares. We shall not consider here
the best ways to normalize images. Let us assume that some combination of
techniques, including choice of grid size and placing a minutia in several adjacent
grid squares if it lies close to the border of the squares enables us to assume
118 CHAPTER 3. FINDING SIMILAR ITEMS
that grid squares from two images have a significantly higher probability of
agreeing in the presence or absence of a minutia than if they were from images
of different fingers.
Thus, fingerprints can be represented by sets of grid squares – those where
their minutiae are located – and compared like any sets, using the Jaccard similarity or distance. There are two versions of fingerprint comparison, however.
• The many-one problem is the one we typically expect. A fingerprint has
been found on a gun, and we want to compare it with all the fingerprints
in a large database, to see which one matches.
• The many-many version of the problem is to take the entire database, and
see if there are any pairs that represent the same individual.
While the many-many version matches the model that we have been following
for finding similar items, the same technology can be used to speed up the
many-one problem.
3.8.5 A LSH Family for Fingerprint Matching
We could minhash the sets that represent a fingerprint, and use the standard
LSH technique from Section 3.4. However, since the sets are chosen from a
relatively small set of grid points (perhaps 1000), the need to minhash them
into more succinct signatures is not clear. We shall study here another form of
locality-sensitive hashing that works well for data of the type we are discussing.
Suppose for an example that the probability of finding a minutia in a random
grid square of a random fingerprint is 20%. Also, assume that if two fingerprints
come from the same finger, and one has a minutia in a given grid square, then
the probability that the other does too is 80%. We can define a locality-sensitive
family of hash functions as follows. Each function f in this family F is defined
by three grid squares. Function f says “yes” for two fingerprints if both have
minutiae in all three grid squares, and otherwise f says “no.” Put another
way, we may imagine that f sends to a single bucket all fingerprints that have
minutiae in all three of f’s grid points, and sends each other fingerprint to a
bucket of its own. In what follows, we shall refer to the first of these buckets as
“the” bucket for f and ignore the buckets that are required to be singletons.
If we want to solve the many-one problem, we can use many functions from
the family F and precompute their buckets of fingerprints to which they answer
“yes.” Then, given a new fingerprint that we want to match, we determine
which of these buckets it belongs to and compare it with all the fingerprints
found in any of those buckets. To solve the many-many problem, we compute
the buckets for each of the functions and compare all fingerprints in each of the
buckets.
Let us consider how many functions we need to get a reasonable probability
of catching a match, without having to compare the fingerprint on the gun with
each of the millions of fingerprints in the database. First, the probability that
3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 119
two fingerprints from different fingers would be in the bucket for a function f
in F is (0.2)6 = 0.000064. The reason is that they will both go into the bucket
only if they each have a minutia in each of the three grid points associated with
f, and the probability of each of those six independent events is 0.2.
Now, consider the probability that two fingerprints from the same finger
wind up in the bucket for f. The probability that the first fingerprint has
minutiae in each of the three squares belonging to f is (0.2)3 = 0.008. However,
if it does, then the probability is (0.8)3 = 0.512 that the other fingerprint
will as well. Thus, if the fingerprints are from the same finger, there is a
0.008 × 0.512 = 0.004096 probability that they will both be in the bucket of f.
That is not much; it is about one in 200. However, if we use many functions
from F, but not too many, then we can get a good probability of matching
fingerprints from the same finger while not having too many false positives –
fingerprints that must be considered but do not match.
Example 3.23 : For a specific example, let us suppose that we use 1024
functions chosen randomly from F. Next, we shall construct a new family F1 by performing a 1024-way OR on F. Then the probability that F1
will put fingerprints from the same finger together in at least one bucket is
1 − (1 − 0.004096)1024 = 0.985. On the other hand, the probability that
two fingerprints from different fingers will be placed in the same bucket is
(1 − (1 − 0.000064)1024 = 0.063. That is, we get about 1.5% false negatives
and about 6.3% false positives. ✷
The result of Example 3.23 is not the best we can do. While it offers only a
1.5% chance that we shall fail to identify the fingerprint on the gun, it does force
us to look at 6.3% of the entire database. Increasing the number of functions
from F will increase the number of false positives, with only a small benefit
of reducing the number of false negatives below 1.5%. On the other hand, we
can also use the AND construction, and in so doing, we can greatly reduce
the probability of a false positive, while making only a small increase in the
false-negative rate. For instance, we could take 2048 functions from F in two
groups of 1024. Construct the buckets for each of the functions. However, given
a fingerprint P on the gun:
1. Find the buckets from the first group in which P belongs, and take the
union of these buckets.
2. Do the same for the second group.
3. Take the intersection of the two unions.
4. Compare P only with those fingerprints in the intersection.
Note that we still have to take unions and intersections of large sets of fingerprints, but we compare only a small fraction of those. It is the comparison of
120 CHAPTER 3. FINDING SIMILAR ITEMS
fingerprints that takes the bulk of the time; in steps (1) and (2) fingerprints
can be represented by their integer indices in the database.
If we use this scheme, the probability of detecting a matching fingerprint
is (0.985)2 = 0.970; that is, we get about 3% false negatives. However, the
probability of a false positive is (0.063)2 = 0.00397. That is, we only have to
examine about 1/250th of the database.
3.8.6 Similar News Articles
Our last case study concerns the problem of organizing a large repository of
on-line news articles by grouping together Web pages that were derived from
the same basic text. It is common for organizations like The Associated Press
to produce a news item and distribute it to many newspapers. Each newspaper
puts the story in its on-line edition, but surrounds it by information that is
special to that newspaper, such as the name and address of the newspaper,
links to related articles, and links to ads. In addition, it is common for the
newspaper to modify the article, perhaps by leaving off the last few paragraphs
or even deleting text from the middle. As a result, the same news article can
appear quite different at the Web sites of different newspapers.
The problem looks very much like the one that was suggested in Section 3.4:
find documents whose shingles have a high Jaccard similarity. Note that this
problem is different from the problem of finding news articles that tell about the
same events. The latter problem requires other techniques, typically examining
the set of important words in the documents (a concept we discussed briefly
in Section 1.3.1) and clustering them to group together different articles about
the same topic.
However, an interesting variation on the theme of shingling was found to be
more effective for data of the type described. The problem is that shingling as
we described it in Section 3.2 treats all parts of a document equally. However,
we wish to ignore parts of the document, such as ads or the headlines of other
articles to which the newspaper added a link, that are not part of the news
article. It turns out that there is a noticeable difference between text that
appears in prose and text that appears in ads or headlines. Prose has a much
greater frequency of stop words, the very frequent words such as “the” or “and.”
The total number of words that are considered stop words varies with the
application, but it is common to use a list of several hundred of the most
frequent words.
Example 3.24 : A typical ad might say simply “Buy Sudzo.” On the other
hand, a prose version of the same thought that might appear in an article is
“I recommend that you buy Sudzo for your laundry.” In the latter sentence, it
would be normal to treat “I,” “that,” “you,” “for,” and “your” as stop words.
✷
Suppose we define a shingle to be a stop word followed by the next two
words. Then the ad “Buy Sudzo” from Example 3.24 has no shingles and
3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 121
would not be reflected in the representation of the Web page containing that
ad. On the other hand, the sentence from Example 3.24 would be represented
by five shingles: “I recommend that,” “that you buy,” “you buy Sudzo,” “for
your laundry,” and “your laundry x,” where x is whatever word follows that
sentence.
Suppose we have two Web pages, each of which consists of half news text
and half ads or other material that has a low density of stop words. If the news
text is the same but the surrounding material is different, then we would expect
that a large fraction of the shingles of the two pages would be the same. They
might have a Jaccard similarity of 75%. However, if the surrounding material
is the same but the news content is different, then the number of common
shingles would be small, perhaps 25%. If we were to use the conventional
shingling, where shingles are (say) sequences of 10 consecutive characters, we
would expect the two documents to share half their shingles (i.e., a Jaccard
similarity of 1/3), regardless of whether it was the news or the surrounding
material that they shared.
3.8.7 Exercises for Section 3.8
Exercise 3.8.1 : Suppose we are trying to perform entity resolution among
bibliographic references, and we score pairs of references based on the similarities of their titles, list of authors, and place of publication. Suppose also that
all references include a year of publication, and this year is equally likely to be
any of the ten most recent years. Further, suppose that we discover that among
the pairs of references with a perfect score, there is an average difference in the
publication year of 0.1.7 Suppose that the pairs of references with a certain
score s are found to have an average difference in their publication dates of 2.
What is the fraction of pairs with score s that truly represent the same publication? Note: Do not make the mistake of assuming the average difference
in publication date between random pairs is 5 or 5.5. You need to calculate it
exactly, and you have enough information to do so.
Exercise 3.8.2 : Suppose we use the family F of functions described in Section 3.8.5, where there is a 20% chance of a minutia in an grid square, an 80%
chance of a second copy of a fingerprint having a minutia in a grid square where
the first copy does, and each function in F being formed from three grid squares.
In Example 3.23, we constructed family F1 by using the OR construction on
1024 members of F. Suppose we instead used family F2 that is a 2048-way OR
of members of F.
(a) Compute the rates of false positives and false negatives for F2.
(b) How do these rates compare with what we get if we organize the same
2048 functions into a 2-way AND of members of F1, as was discussed at
the end of Section 3.8.5?
7We might expect the average to be 0, but in practice, errors in publication year do occur.
122 CHAPTER 3. FINDING SIMILAR ITEMS
Exercise 3.8.3 : Suppose fingerprints have the same statistics outlined in Exercise 3.8.2, but we use a base family of functions F
′
defined like F, but using
only two randomly chosen grid squares. Construct another set of functions F
′
1
from F
′
by taking the n-way OR of functions from F
′
. What, as a function of
n, are the false positive and false negative rates for F
′
1
?
Exercise 3.8.4 : Suppose we use the functions F1 from Example 3.23, but we
want to solve the many-many problem.
(a) If two fingerprints are from the same finger, what is the probability that
they will not be compared (i.e., what is the false negative rate)?
(b) What fraction of the fingerprints from different fingers will be compared
(i.e., what is the false positive rate)?
! Exercise 3.8.5 : Assume we have the set of functions F as in Exercise 3.8.2,
and we construct a new set of functions F3 by an n-way OR of functions in
F. For what value of n is the sum of the false positive and false negative rates
minimized?
3.9 Methods for High Degrees of Similarity
LSH-based methods appear most effective when the degree of similarity we
accept is relatively low. When we want to find sets that are almost identical,
there are other methods that can be faster. Moreover, these methods are exact,
in that they find every pair of items with the desired degree of similarity. There
are no false negatives, as there can be with LSH.
3.9.1 Finding Identical Items
The extreme case is finding identical items, for example, Web pages that are
identical, character-for-character. It is straightforward to compare two documents and tell whether they are identical, but we still must avoid having to
compare every pair of documents. Our first thought would be to hash documents based on their first few characters, and compare only those documents
that fell into the same bucket. That scheme should work well, unless all the
documents begin with the same characters, such as an HTML header.
Our second thought would be to use a hash function that examines the
entire document. That would work, and if we use enough buckets, it would be
very rare that two documents went into the same bucket, yet were not identical.
The downside of this approach is that we must examine every character of every
document. If we limit our examination to a small number of characters, then
we never have to examine a document that is unique and falls into a bucket of
its own.
A better approach is to pick some fixed random positions for all documents,
and make the hash function depend only on these. This way, we can avoid
3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 123
a problem where there is a common prefix for all or most documents, yet we
need not examine entire documents unless they fall into a bucket with another
document. One problem with selecting fixed positions is that if some documents
are short, they may not have some of the selected positions. However, if we are
looking for highly similar documents, we never need to compare two documents
that differ significantly in their length. We exploit this idea in Section 3.9.3.
3.9.2 Representing Sets as Strings
Now, let us focus on the harder problem of finding, in a large collection of sets,
all pairs that have a high Jaccard similarity, say at least 0.9. We can represent
a set by sorting the elements of the universal set in some fixed order, and
representing any set by listing its elements in this order. The list is essentially
a string of “characters,” where the characters are the elements of the universal
set. These strings are unusual, however, in that:
1. No character appears more than once in a string, and
2. If two characters appear in two different strings, then they appear in the
same order in both strings.
Example 3.25 : Suppose the universal set consists of the 26 lower-case letters,
and we use the normal alphabetical order. Then the set {d, a, b} is represented
by the string abd. ✷
In what follows, we shall assume all strings represent sets in the manner just
described. Thus, we shall talk about the Jaccard similarity of strings, when
strictly speaking we mean the similarity of the sets that the strings represent.
Also, we shall talk of the length of a string, as a surrogate for the number of
elements in the set that the string represents.
Note that the documents discussed in Section 3.9.1 do not exactly match
this model, even though we can see documents as strings. To fit the model,
we would shingle the documents, assign an order to the shingles, and represent
each document by its list of shingles in the selected order.
3.9.3 Length-Based Filtering
The simplest way to exploit the string representation of Section 3.9.2 is to sort
the strings by length. Then, each string s is compared with those strings t that
follow s in the list, but are not too long. Suppose the lower bound on Jaccard
similarity between two strings is J. For any string x, denote its length by Lx.
Note that Ls ≤ Lt. The intersection of the sets represented by s and t cannot
have more than Ls members, while their union has at least Lt members. Thus,
the Jaccard similarity of s and t, which we denote SIM(s, t), is at most Ls/Lt.
That is, in order for s and t to require comparison, it must be that J ≤ Ls/Lt,
or equivalently, Lt ≤ Ls/J.
124 CHAPTER 3. FINDING SIMILAR ITEMS
A Better Ordering for Symbols
Instead of using the obvious order for elements of the universal set, e.g.,
lexicographic order for shingles, we can order symbols rarest first. That
is, determine how many times each element appears in the collection of
sets, and order them by this count, lowest first. The advantage of doing
so is that the symbols in prefixes will tend to be rare. Thus, they will
cause that string to be placed in index buckets that have relatively few
members. Then, when we need to examine a string for possible matches,
we shall find few other strings that are candidates for comparison.
Example 3.26 : Suppose that s is a string of length 9, and we are looking for
strings with at least 0.9 Jaccard similarity. Then we have only to compare s
with strings following it in the length-based sorted order that have length at
most 9/0.9 = 10. That is, we compare s with those strings of length 9 that
follow it in order, and all strings of length 10. We have no need to compare s
with any other string.
Suppose the length of s were 8 instead. Then s would be compared with
following strings of length up to 8/0.9 = 8.89. That is, a string of length 9
would be too long to have a Jaccard similarity of 0.9 with s, so we only have to
compare s with the strings that have length 8 but follow it in the sorted order.
✷
3.9.4 Prefix Indexing
In addition to length, there are several other features of strings that can be
exploited to limit the number of comparisons that must be made to identify
all pairs of similar strings. The simplest of these options is to create an index
for each symbol; recall a symbol of a string is any one of the elements of the
universal set. For each string s, we select a prefix of s consisting of the first p
symbols of s. How large p must be depends on Ls and J, the lower bound on
Jaccard similarity. We add string s to the index for each of its first p symbols.
In effect, the index for each symbol becomes a bucket of strings that must be
compared. We must be certain that any other string t such that SIM(s, t) ≥ J
will have at least one symbol in its prefix that also appears in the prefix of s.
Suppose not; rather SIM(s, t) ≥ J, but t has none of the first p symbols of
s. Then the highest Jaccard similarity that s and t can have occurs when t is
a suffix of s, consisting of everything but the first p symbols of s. The Jaccard
similarity of s and t would then be (Ls − p)/Ls. To be sure that we do not
have to compare s with t, we must be certain that J > (Ls − p)/Ls. That
is, p must be at least ⌊(1 − J)Ls⌋ + 1. Of course we want p to be as small as
possible, so we do not index string s in more buckets than we need to. Thus,
we shall hereafter take p = ⌊(1 − J)Ls⌋ + 1 to be the length of the prefix that
3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 125
gets indexed.
Example 3.27 : Suppose J = 0.9. If Ls = 9, then p = ⌊0.1 × 9⌋ + 1 =
⌊0.9⌋ + 1 = 1. That is, we need to index s under only its first symbol. Any
string t that does not have the first symbol of s in a position such that t is
indexed by that symbol will have Jaccard similarity with s that is less than 0.9.
Suppose s is bcdefghij. Then s is indexed under b only. Suppose t does not
begin with b. There are two cases to consider.
1. If t begins with a, and SIM(s, t) ≥ 0.9, then it can only be that t is
abcdefghij. But if that is the case, t will be indexed under both a and
b. The reason is that Lt = 10, so t will be indexed under the symbols of
its prefix of length ⌊0.1 × 10⌋ + 1 = 2.
2. If t begins with c or a later letter, then the maximum value of SIM(s, t)
occurs when t is cdefghij. But then SIM(s, t) = 8/9 < 0.9.
In general, with J = 0.9, strings of length up to 9 are indexed by their first
symbol, strings of lengths 10–19 are indexed under their first two symbols,
strings of length 20–29 are indexed under their first three symbols, and so on.
✷
We can use the indexing scheme in two ways, depending on whether we
are trying to solve the many-many problem or a many-one problem; recall the
distinction was introduced in Section 3.8.4. For the many-one problem, we
create the index for the entire database. To query for matches to a new set
S, we convert that set to a string s, which we call the probe string. Determine
the length of the prefix that must be considered, that is, ⌊(1 − J)Ls⌋ + 1. For
each symbol appearing in one of the prefix positions of s, we look in the index
bucket for that symbol, and we compare s with all the strings appearing in that
bucket.
If we want to solve the many-many problem, start with an empty database
of strings and indexes. For each set S, we treat S as a new set for the many-one
problem. We convert S to a string s, which we treat as a probe string in the
many-one problem. However, after we examine an index bucket, we also add s
to that bucket, so s will be compared with later strings that could be matches.
3.9.5 Using Position Information
Consider the strings s = acdefghijk and t = bcdefghijk, and assume J = 0.9.
Since both strings are of length 10, they are indexed under their first two
symbols. Thus, s is indexed under a and c, while t is indexed under b and c.
Whichever is added last will find the other in the bucket for c, and they will be
compared. However, since c is the second symbol of both, we know there will
be two symbols, a and b in this case, that are in the union of the two sets but
not in the intersection. Indeed, even though s and t are identical from c to the
126 CHAPTER 3. FINDING SIMILAR ITEMS
end, their intersection is 9 symbols and their union is 11; thus SIM(s, t) = 9/11,
which is less than 0.9.
If we build our index based not only on the symbol, but on the position of
the symbol within the string, we could avoid comparing s and t above. That
is, let our index have a bucket for each pair (x, i), containing the strings that
have symbol x in position i of their prefix. Given a string s, and assuming J is
the minimum desired Jaccard similarity, we look at the prefix of s, that is, the
positions 1 through ⌊(1 − J)Ls⌋ + 1. If the symbol in position i of the prefix is
x, add s to the index bucket for (x, i).
Now consider s as a probe string. With what buckets must it be compared?
We shall visit the symbols of the prefix of s from the left, and we shall take
advantage of the fact that we only need to find a possible matching string t if
none of the previous buckets we have examined for matches held t. That is, we
only need to find a candidate match once. Thus, if we find that the ith symbol
of s is x, then we need look in the bucket (x, j) for certain small values of j.
j
s
t
Symbols definitely
appearing in
only one string
i
Figure 3.15: Strings s and t begin with i − 1 and j − 1 unique symbols, respectively, and then agree beyond that
To compute the upper bound on j, suppose t is a string none of whose first
j −1 symbols matched anything in s, but the ith symbol of s is the same as the
jth symbol of t. The highest value of SIM(s, t) occurs if s and t are identical
beyond their ith and jth symbols, respectively, as suggested by Fig. 3.15. If
that is the case, the size of their intersection is Ls − i + 1, since that is the
number of symbols of s that could possibly be in t. The size of their union is
at least Ls + j − 1. That is, s surely contributes Ls symbols to the union, and
there are also at least j −1 symbols of t that are not in s. The ratio of the sizes
of the intersection and union must be at least J, so we must have:
Ls − i + 1
Ls + j − 1
≥ J
If we isolate j in this inequality, we have j ≤

Ls(1 − J) − i + 1 + J

/J.
Example 3.28 : Consider the string s = acdefghijk with J = 0.9 discussed
at the beginning of this section. Suppose s is now a probe string. We already
established that we need to consider the first two positions; that is, i can be
3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 127
or 2. Suppose i = 1. Then j ≤ (10 × 0.1 − 1 + 1 + 0.9)/0.9. That is, we only
have to compare the symbol a with strings in the bucket for (a, j) if j ≤ 2.11.
Thus, j can be 1 or 2, but nothing higher.
Now suppose i = 2. Then we require j ≤ (10 × 0.1 − 2 + 1 + 0.9)/0.9, Or
j ≤ 1. We conclude that we must look in the buckets for (a, 1), (a, 2), and (c, 1),
but in no other bucket. In comparison, using the buckets of Section 3.9.4, we
would look into the buckets for a and c, which is equivalent to looking to all
buckets (a, j) and (c, j) for any j. ✷
3.9.6 Using Position and Length in Indexes
When we considered the upper limit on j in the previous section, we assumed
that what follows positions i and j were as in Fig. 3.15, where what followed
these positions in strings s and t matched exactly. We do not want to build an
index that involves every symbol in the strings, because that makes the total
work excessive. However, we can add to our index a summary of what follows
the positions being indexed. Doing so expands the number of buckets, but not
beyond reasonable bounds, and yet enables us to eliminate many candidate
matches without comparing entire strings. The idea is to use index buckets
corresponding to a symbol, a position, and the suffix length, that is, the number
of symbols following the position in question.
Example 3.29 : The string s = acdefghijk, with J = 0.9, would be indexed
in the buckets for (a, 1, 9) and (c, 2, 8). That is, the first position of s has symbol
a, and its suffix is of length 9. The second position has symbol c and its suffix
is of length 8. ✷
Figure 3.15 assumes that the suffixes for position i of s and position j of t
have the same length. If not, then we can either get a smaller upper bound on
the size of the intersection of s and t (if t is shorter) or a larger lower bound
on the size of the union (if t is longer). Suppose s has suffix length p and t has
suffix length q.
Case 1: p ≥ q. Here, the maximum size of the intersection is
Ls − i + 1 − (p − q)
Since Ls = i + p, we can write the above expression for the intersection size as
q + 1. The minimum size of the union is Ls + j − 1, as it was when we did not
take suffix length into account. Thus, we require
q + 1
Ls + j − 1
≥ J
whenever p ≥ q.
128 CHAPTER 3. FINDING SIMILAR ITEMS
Case 2: p < q. Here, the maximum size of the intersection is Ls − i + 1, as
when suffix length was not considered. However, the minimum size of the union
is now Ls + j − 1 + q − p. If we again use the relationship Ls = i + p, we can
replace Ls − p by i and get the formula i + j − 1 + q for the size of the union.
If the Jaccard similarity is at least J, then
Ls − i + 1
i + j − 1 + q
≥ J
whenever p < q.
Example 3.30 : Let us again consider the string s = acdefghijk, but to make
the example show some details, let us choose J = 0.8 instead of 0.9. We know
that Ls = 10. Since ⌊(1 − J)Ls⌋ + 1 = 3, we must consider prefix positions
i = 1, 2, and 3 in what follows. As before, let p be the suffix length of s and q
the suffix length of t.
First, consider the case p ≥ q. The additional constraint we have on q and
j is (q + 1)/(9 + j) ≥ 0.8. We can enumerate the pairs of values of j and q for
each i between 1 and 3, as follows.
i = 1: Here, p = 9, so q ≤ 9. Let us consider the possible values of q:
q = 9: We must have 10/(9 + j) ≥ 0.8. Thus, we can have j = 1, j = 2,
or j = 3. Note that for j = 4, 10/13 > 0.8.
q = 8: We must have 9/(9 + j) ≥ 0.8. Thus, we can have j = 1 or j = 2.
For j = 3, 9/12 > 0.8.
q = 7: We must have 8/(9 + j) ≥ 0.8. Only j = 1 satisfies this inequality.
q = 6: There are no possible values of j, since 7/(9 + j) > 0.8 for every
positive integer j. The same holds for every smaller value of q.
i = 2: Here, p = 8, so we require q ≤ 8. Since the constraint (q+1)/(9+j) ≥ 0.8
does not depend on i,
8 we can use the analysis from the above case, but
exclude the case q = 9. Thus, the only possible values of j and q when
i = 2 are
1. q = 8; j = 1.
2. q = 8; j = 2.
3. q = 7; j = 1.
i = 3: Now, p = 7 and the constraints are q ≤ 7 and (q + 1)/(9 + j) ≥ 0.8. The
only option is q = 7 and j = 1.
Next, we must consider the case p < q. The additional constraint is
11 − i
i + j + q − 1
≥ 0.8
Again, consider each possible value of i.
8Note that i does influence the value of p, and through p, puts a limit on q.
3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 129
i = 1: Then p = 9, so we require q ≥ 10 and 10/(q + j) ≥ 0.8. The possible
values of q and j are
1. q = 10; j = 1.
2. q = 10; j = 2.
3. q = 11; j = 1.
i = 2: Now, p = 8, so we require q ≥ 9 and 9/(q + j + 1) ≥ 0.8. Since j must
be a positive integer, the only solution is q = 9 and j = 1, a possibility
that we already knew about.
i = 3: Here, p = 7, so we require q ≥ 8 and 8/(q + j + 2) ≥ 0.8. There are no
solutions.
q j = 1 j = 2 j = 3
7 x
8 x x
i = 1 9 x x x
10 x x
11 x
7 x
i = 2 8 x x
9 x
i = 3 7 x
Figure 3.16: The buckets that must be examined to find possible matches for
the string s = acdefghijk with J = 0.8 are marked with an x
When we accumulate the possible combinations of i, j, and q, we see that
the set of index buckets in which we must look forms a pyramid. Figure 3.16
shows the buckets in which we must search. That is, we must look in those
buckets (x, j, q) such that the ith symbol of the string s is x, j is the position
associated with the bucket and q the suffix length. ✷
3.9.7 Exercises for Section 3.9
Exercise 3.9.1 : Suppose our universal set is the lower-case letters, and the
order of elements is taken to be the vowels, in alphabetic order, followed by the
consonants in reverse alphabetic order. Represent the following sets as strings.
a {q, w, e, r, t, y}.
(b) {a, s, d, f, g, h, j, u, i}.
130 CHAPTER 3. FINDING SIMILAR ITEMS
Exercise 3.9.2 : Suppose we filter candidate pairs based only on length, as in
Section 3.9.3. If s is a string of length 20, with what strings is s compared when
J, the lower bound on Jaccard similarity has the following values: (a) J = 0.85
(b) J = 0.95 (c) J = 0.98?
Exercise 3.9.3 : Suppose we have a string s of length 15, and we wish to index
its prefix as in Section 3.9.4.
(a) How many positions are in the prefix if J = 0.85?
(b) How many positions are in the prefix if J = 0.95?
! (c) For what range of values of J will s be indexed under its first four symbols,
but no more?
Exercise 3.9.4 : Suppose s is a string of length 12. With what symbol-position
pairs will s be compared with if we use the indexing approach of Section 3.9.5,
and (a) J = 0.75 (b) J = 0.95?
! Exercise 3.9.5 : Suppose we use position information in our index, as in Section 3.9.5. Strings s and t are both chosen at random from a universal set of
100 elements. Assume J = 0.9. What is the probability that s and t will be
compared if
(a) s and t are both of length 9.
(b) s and t are both of length 10.
Exercise 3.9.6 : Suppose we use indexes based on both position and suffix
length, as in Section 3.9.6. If s is a string of length 20, with what symbolposition-length triples will s be compared with, if (a) J = 0.8 (b) J = 0.9?
3.10 Summary of Chapter 3
✦ Jaccard Similarity: The Jaccard similarity of sets is the ratio of the size
of the intersection of the sets to the size of the union. This measure of
similarity is suitable for many applications, including textual similarity of
documents and similarity of buying habits of customers.
✦ Shingling: A k-shingle is any k characters that appear consecutively in
a document. If we represent a document by its set of k-shingles, then
the Jaccard similarity of the shingle sets measures the textual similarity
of documents. Sometimes, it is useful to hash shingles to bit strings of
shorter length, and use sets of hash values to represent documents.
✦ Minhashing: A minhash function on sets is based on a permutation of the
universal set. Given any such permutation, the minhash value for a set is
that element of the set that appears first in the permuted order.
3.10. SUMMARY OF CHAPTER 3 131
✦ Minhash Signatures: We may represent sets by picking some list of permutations and computing for each set its minhash signature, which is the
sequence of minhash values obtained by applying each permutation on the
list to that set. Given two sets, the expected fraction of the permutations
that will yield the same minhash value is exactly the Jaccard similarity
of the sets.
✦ Efficient Minhashing: Since it is not really possible to generate random
permutations, it is normal to simulate a permutation by picking a random
hash function and taking the minhash value for a set to be the least hash
value of any of the set’s members. An additional efficiency can be had
by restricting the search for the smallest minhash value to only a small
subset of the universal set.
✦ Locality-Sensitive Hashing for Signatures: This technique allows us to
avoid computing the similarity of every pair of sets or their minhash signatures. If we are given signatures for the sets, we may divide them into
bands, and only measure the similarity of a pair of sets if they are identical in at least one band. By choosing the size of bands appropriately, we
can eliminate from consideration most of the pairs that do not meet our
threshold of similarity.
✦ Distance Measures: A distance measure is a function on pairs of points in
a space that satisfy certain axioms. The distance between two points is 0 if
the points are the same, but greater than 0 if the points are different. The
distance is symmetric; it does not matter in which order we consider the
two points. A distance measure must satisfy the triangle inequality: the
distance between two points is never more than the sum of the distances
between those points and some third point.
✦ Euclidean Distance: The most common notion of distance is the Euclidean
distance in an n-dimensional space. This distance, sometimes called the
L2-norm, is the square root of the sum of the squares of the differences
between the points in each dimension. Another distance suitable for Euclidean spaces, called Manhattan distance or the L1-norm is the sum of
the magnitudes of the differences between the points in each dimension.
✦ Jaccard Distance: One minus the Jaccard similarity is a distance measure,
called the Jaccard distance.
✦ Cosine Distance: The angle between vectors in a vector space is the cosine
distance measure. We can compute the cosine of that angle by taking the
dot product of the vectors and dividing by the lengths of the vectors.
✦ Edit Distance: This distance measure applies to a space of strings, and
is the number of insertions and/or deletions needed to convert one string
into the other. The edit distance can also be computed as the sum of
132 CHAPTER 3. FINDING SIMILAR ITEMS
the lengths of the strings minus twice the length of the longest common
subsequence of the strings.
✦ Hamming Distance: This distance measure applies to a space of vectors.
The Hamming distance between two vectors is the number of positions in
which the vectors differ.
✦ Generalized Locality-Sensitive Hashing: We may start with any collection
of functions, such as the minhash functions, that can render a decision
as to whether or not a pair of items should be candidates for similarity
checking. The only constraint on these functions is that they provide a
lower bound on the probability of saying “yes” if the distance (according
to some distance measure) is below a given limit, and an upper bound on
the probability of saying “yes” if the distance is above another given limit.
We can then increase the probability of saying “yes” for nearby items and
at the same time decrease the probability of saying “yes” for distant items
to as great an extent as we wish, by applying an AND construction and
an OR construction.
✦ Random Hyperplanes and LSH for Cosine Distance: We can get a set of
basis functions to start a generalized LSH for the cosine distance measure
by identifying each function with a list of randomly chosen vectors. We
apply a function to a given vector v by taking the dot product of v with
each vector on the list. The result is a sketch consisting of the signs (+1 or
−1) of the dot products. The fraction of positions in which the sketches of
two vectors agree, multiplied by 180, is an estimate of the angle between
the two vectors.
✦ LSH For Euclidean Distance: A set of basis functions to start LSH for
Euclidean distance can be obtained by choosing random lines and projecting points onto those lines. Each line is broken into fixed-length intervals,
and the function answers “yes” to a pair of points that fall into the same
interval.
✦ High-Similarity Detection by String Comparison: An alternative approach
to finding similar items, when the threshold of Jaccard similarity is close to
1, avoids using minhashing and LSH. Rather, the universal set is ordered,
and sets are represented by strings, consisting their elements in order.
The simplest way to avoid comparing all pairs of sets or their strings is to
note that highly similar sets will have strings of approximately the same
length. If we sort the strings, we can compare each string with only a
small number of the immediately following strings.
✦ Character Indexes: If we represent sets by strings, and the similarity
threshold is close to 1, we can index all strings by their first few characters.
The prefix whose characters must be indexed is approximately the length
of the string times the maximum Jaccard distance (1 minus the minimum
Jaccard similarity).
3.11. REFERENCES FOR CHAPTER 3 133
✦ Position Indexes: We can index strings not only on the characters in
their prefixes, but on the position of that character within the prefix. We
reduce the number of pairs of strings that must be compared, because
if two strings share a character that is not in the first position in both
strings, then we know that either there are some preceding characters that
are in the union but not the intersection, or there is an earlier symbol that
appears in both strings.
✦ Suffix Indexes: We can also index strings based not only on the characters
in their prefixes and the positions of those characters, but on the length
of the character’s suffix – the number of positions that follow it in the
string. This structure further reduces the number of pairs that must be
compared, because a common symbol with different suffix lengths implies
additional characters that must be in the union but not in the intersection.
3.11 References for Chapter 3
The technique we called shingling is attributed to [11]. The use in the manner
we discussed here is from [2].
Minhashing comes from [3]. The improvement that avoids looking at all
elements is from [10].
The original works on locality-sensitive hashing were [9] and [7]. [1] is a
useful summary of ideas in this field.
[4] introduces the idea of using random-hyperplanes to summarize items in
a way that respects the cosine distance. [8] suggests that random hyperplanes
plus LSH can be more accurate at detecting similar documents than minhashing
plus LSH.
Techniques for summarizing points in a Euclidean space are covered in [6].
[12] presented the shingling technique based on stop words.
The length and prefix-based indexing schemes for high-similarity matching
comes from [5]. The technique involving suffix length is from [13].
1. A. Andoni and P. Indyk, “Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions,” Comm. ACM 51:1, pp. 117–
122, 2008.
2. A.Z. Broder, “On the resemblance and containment of documents,” Proc.
Compression and Complexity of Sequences, pp. 21–29, Positano Italy,
1997.
3. A.Z. Broder, M. Charikar, A.M. Frieze, and M. Mitzenmacher, “Min-wise
independent permutations,” ACM Symposium on Theory of Computing,
pp. 327–336, 1998.
4. M.S. Charikar, “Similarity estimation techniques from rounding algorithms,” ACM Symposium on Theory of Computing, pp. 380–388, 2002.
134 CHAPTER 3. FINDING SIMILAR ITEMS
5. S. Chaudhuri, V. Ganti, and R. Kaushik, “A primitive operator for similarity joins in data cleaning,” Proc. Intl. Conf. on Data Engineering,
2006.
6. M. Datar, N. Immorlica, P. Indyk, and V.S. Mirrokni, “Locality-sensitive
hashing scheme based on p-stable distributions,” Symposium on Computational Geometry pp. 253–262, 2004.
7. A. Gionis, P. Indyk, and R. Motwani, “Similarity search in high dimensions via hashing,” Proc. Intl. Conf. on Very Large Databases, pp. 518–
529, 1999.
8. M. Henzinger, “Finding near-duplicate web pages: a large-scale evaluation
of algorithms,” Proc. 29th SIGIR Conf., pp. 284–291, 2006.
9. P. Indyk and R. Motwani. “Approximate nearest neighbor: towards removing the curse of dimensionality,” ACM Symposium on Theory of Computing, pp. 604–613, 1998.
10. P. Li, A.B. Owen, and C.H. Zhang. “One permutation hashing,” Conf.
on Neural Information Processing Systems 2012, pp. 3122–3130.
11. U. Manber, “Finding similar files in a large file system,” Proc. USENIX
Conference, pp. 1–10, 1994.
12. M. Theobald, J. Siddharth, and A. Paepcke, “SpotSigs: robust and efficient near duplicate detection in large web collections,” 31st Annual ACM
SIGIR Conference, July, 2008, Singapore.
13. C. Xiao, W. Wang, X. Lin, and J.X. Yu, “Efficient similarity joins for
near duplicate detection,” Proc. WWW Conference, pp. 131-140, 2008.

72
Chapter 3
Finding Similar Items
A fundamental data-mining problem is to examine data for “similar” items. We
shall take up applications in Section 3.1, but an example would be looking at a
collection of Web pages and finding near-duplicate pages. These pages could be
plagiarisms, for example, or they could be mirrors that have almost the same
content but differ in information about the host and about other mirrors.
The naive approach to finding pairs of similar items requires us to look at every pair of items. When we are dealing with a large dataset, looking at all pairs
of items may be prohibitive, even given an abundance of hardware resources.
For example, even a million items gives us half a trillion pairs to examine, and
a million items is considered a “small” dataset by today’s standards.
It is therefore a pleasant surprise to learn of a family of techniques called
locality-sensitive hashing, or LSH, that allows us to focus on pairs that are likely
to be similar, without having to look at all pairs. Thus, it is possible that we
can avoid the quadratic growth in computation time that is required by the
naive algorithm. There is usually a downside to locality-sensitive hashing, due
to the presence of false negatives, that is, pairs of items that are similar, yet
are not included in the set of pairs that we examine, but by careful tuning we
can reduce the fraction of false negatives by increasing the number of pairs we
consider.
The general idea behind LSH is that we hash items using many different
hash functions. These hash functions are not the conventional sort of hash
functions. Rather, they are carefully designed to have the property that pairs
are much more likely to wind up in the same bucket of a hash function if the
items are similar than if they are not similar. We then can examine only the
candidate pairs, which are pairs of items that wind up in the same bucket for
at least one of the hash functions.
We begin our discussion of LSH with an examination of the problem of finding similar documents – those that share a lot of common text. We first show
how to convert documents into sets (Section 3.2) in a way that lets us view
textual similarity of documents as sets having a large overlap. More precisely,
73
74 CHAPTER 3. FINDING SIMILAR ITEMS
we measure the similarity of sets by their Jaccard similarity, the ratio of the
sizes of their intersection and union. A second key trick we need is minhashing
(Section 3.3), which is a way to convert large sets into much smaller representations, called signatures, that still enable us to estimate closely the Jaccard
similarity of the represented sets. Finally, in Section 3.4 we see how to apply
the bucketing idea inherent in LSH to the signatures.
In Section 3.5 we begin our study of how to apply LSH to items other than
sets. We consider the general notion of a distance measure that tells to what
degree items are similar. Then, in Section 3.6 we consider the general idea of
locality-sensitive hashing, and in Section 3.7 we see how to do LSH for some data
types other than sets. Then, Section 3.8 examines in detail several applications
of the LSH idea. Finally, we consider in Section 3.9 some techniques for finding
similar sets that can be more efficient than LSH when the degree of similarity
we want is very high.
3.1 Applications of Set Similarity
We shall focus initially on a particular notion of “similarity”: the similarity of
sets by looking at the relative size of their intersection. This notion of similarity
is called Jaccard similarity, which is introduced in Section 3.1.1. We then
examine some of the uses of finding similar sets. These include finding textually
similar documents and collaborative filtering by finding similar customers and
similar products. In order to turn the problem of textual similarity of documents
into one of set intersection, we use the technique called shingling, which is the
subject of Section 3.2.
3.1.1 Jaccard Similarity of Sets
The Jaccard similarity of sets S and T is |S ∩ T |/|S ∪ T |, that is, the ratio
of the size of the intersection of S and T to the size of their union. We shall
denote the Jaccard similarity of S and T by SIM(S, T ).
Example 3.1 : In Fig. 3.1 we see two sets S and T . There are three elements
in their intersection and a total of eight elements that appear in S or T or both.
Thus, SIM(S, T ) = 3/8. ✷
3.1.2 Similarity of Documents
An important class of problems that Jaccard similarity addresses well is that
of finding textually similar documents in a large corpus such as the Web or a
collection of news articles. We should understand that the aspect of similarity
we are looking at here is character-level similarity, not “similar meaning,” which
requires us to examine the words in the documents and their uses. That problem
is also interesting but is addressed by other techniques, which we hinted at in
3.1. APPLICATIONS OF SET SIMILARITY 75
































T
S
Figure 3.1: Two sets with Jaccard similarity 3/8
Section 1.3.1. However, textual similarity also has important uses. Many of
these involve finding duplicates or near duplicates. First, let us observe that
testing whether two documents are exact duplicates is easy; just compare the
two documents character-by-character, and if they ever differ then they are not
the same. However, in many applications, the documents are not identical, yet
they share large portions of their text. Here are some examples:
Plagiarism
Finding plagiarized documents tests our ability to find textual similarity. The
plagiarizer may extract only some parts of a document for his own. He may
alter a few words and may alter the order in which sentences of the original
appear. Yet the resulting document may still contain much of the original. No
simple process of comparing documents character by character will detect a
sophisticated plagiarism.
Mirror Pages
It is common for important or popular Web sites to be duplicated at a number
of hosts, in order to share the load. The pages of these mirror sites will be
quite similar, but are rarely identical. For instance, they might each contain
information associated with their particular host, and they might each have
links to the other mirror sites but not to themselves. A related phenomenon is
the reuse of Web pages from one academic class to another. These pages might
include class notes, assignments, and lecture slides. Similar pages might change
the name of the course, year, and make small changes from year to year. It
is important to be able to detect similar pages of these kinds, because search
engines produce better results if they avoid showing two pages that are nearly
identical within the first
76 CHAPTER 3. FINDING SIMILAR ITEMS
Articles from the Same Source
It is common for one reporter to write a news article that gets distributed,
say through the Associated Press, to many newspapers, which then publish
the article on their Web sites. Each newspaper changes the article somewhat.
They may cut out paragraphs, or even add material of their own. They most
likely will surround the article by their own logo, ads, and links to other articles
at their site. However, the core of each newspaper’s page will be the original
article. News aggregators, such as Google News, try to find all versions of such
an article, in order to show only one, and that task requires finding when two
Web pages are textually similar, although not identical.1
3.1.3 Collaborative Filtering as a Similar-Sets Problem
Another class of applications where similarity of sets is very important is called
collaborative filtering, a process whereby we recommend to users items that were
liked by other users who have exhibited similar tastes. We shall investigate
collaborative filtering in detail in Section 9.3, but for the moment let us see
some common examples.
On-Line Purchases
Amazon.com has millions of customers and sells millions of items. Its database
records which items have been bought by which customers. We can say two customers are similar if their sets of purchased items have a high Jaccard similarity.
Likewise, two items that have sets of purchasers with high Jaccard similarity
will be deemed similar. Note that, while we might expect mirror sites to have
Jaccard similarity above 90%, it is unlikely that any two customers have Jaccard similarity that high (unless they have purchased only one item). Even a
Jaccard similarity like 20% might be unusual enough to identify customers with
similar tastes. The same observation holds for items; Jaccard similarities need
not be very high to be significant.
Collaborative filtering requires several tools, in addition to finding similar
customers or items, as we discuss in Chapter 9. For example, two Amazon
customers who like science-fiction might each buy many science-fiction books,
but only a few of these will be in common. However, by combining similarityfinding with clustering (Chapter 7), we might be able to discover that sciencefiction books are mutually similar and put them in one group. Then, we can
get a more powerful notion of customer-similarity by asking whether they made
purchases within many of the same groups.
1News aggregation also involves finding articles that are about the same topic, even though
not textually similar. This problem too can yield to a similarity search, but it requires
techniques other than Jaccard similarity of sets.
3.1. APPLICATIONS OF SET SIMILARITY 77
Movie Ratings
Netflix records which movies each of its customers rented, and also the ratings
assigned to those movies by the customers. We can regard movies as similar
if they were rented or rated highly by many of the same customers, and see
customers as similar if they rented or rated highly many of the same movies.
The same observations that we made for Amazon above apply in this situation:
similarities need not be high to be significant, and clustering movies by genre
will make things easier.
When our data consists of ratings rather than binary decisions (bought/did
not buy or liked/disliked), we cannot rely simply on sets as representations of
customers or items. Some options are:
1. Ignore low-rated customer/movie pairs; that is, treat these events as if
the customer never watched the movie.
2. When comparing customers, imagine two set elements for each movie,
“liked” and “hated.” If a customer rated a movie highly, put “liked” for
that movie in the customer’s set. If they gave a low rating to a movie, put
“hated” for that movie in their set. Then, we can look for high Jaccard
similarity among these sets. We can use a similar trick when comparing
movies.
3. If ratings are 1-to-5-stars, put a movie in a customer’s set n times if
they rated the movie n-stars. Then, use Jaccard similarity for bags when
measuring the similarity of customers. The Jaccard similarity for bags
B and C is defined by counting an element n times in the intersection if
n is the minimum of the number of times the element appears in B and
C. In the union, we count the element the sum of the number of times it
appears in B and in C.
2
Example 3.2 : The bag-similarity of bags {a, a, a, b} and {a, a, b, b, c} is 1/3.
The intersection counts a twice and b once, so its size is 3. The size of the
union of two bags is always the sum of the sizes of the two bags, or 9 in this
case. Since the highest possible Jaccard similarity for bags is 1/2, the score
of 1/3 indicates the two bags are quite similar, as should be apparent from an
examination of their contents. ✷
2Although the union for bags is normally (e.g., in the SQL standard) defined to have the
sum of the number of copies in each of the two bags, this definition causes some inconsistency
with the Jaccard similarity for sets. Under this definition of bag union, the maximum Jaccard
similarity is 1/2, not 1, since the union of a set with itself has twice as many elements as the
intersection of the same set with itself. If we prefer to have the Jaccard similarity of a set
with itself be 1, we can redefine the union of bags to have each element appear the maximum
number of times it appears in either of the two bags. This change also gives a reasonable
measure of bag similarity.
78 CHAPTER 3. FINDING SIMILAR ITEMS
3.1.4 Exercises for Section 3.1
Exercise 3.1.1 : Compute the Jaccard similarities of each pair of the following
three sets: {1, 2, 3, 4}, {2, 3, 5, 7}, and {2, 4, 6}.
Exercise 3.1.2 : Compute the Jaccard bag similarity of each pair of the following three bags: {1, 1, 1, 2}, {1, 1, 2, 2, 3}, and {1, 2, 3, 4}.
!! Exercise 3.1.3 : Suppose we have a universal set U of n elements, and we
choose two subsets S and T at random, each with m of the n elements. What
is the expected value of the Jaccard similarity of S and T ?
3.2 Shingling of Documents
The most effective way to represent documents as sets, for the purpose of identifying lexically similar documents is to construct from the document the set
of short strings that appear within it. If we do so, then documents that share
pieces as short as sentences or even phrases will have many common elements
in their sets, even if those sentences appear in different orders in the two documents. In this section, we introduce the simplest and most common approach,
shingling, as well as an interesting variation.
3.2.1 k-Shingles
A document is a string of characters. Define a k-shingle for a document to be
any substring of length k found within the document. Then, we may associate
with each document the set of k-shingles that appear one or more times within
that document.
Example 3.3 : Suppose our document D is the string abcdabd, and we pick
k = 2. Then the set of 2-shingles for D is {ab, bc, cd, da, bd}.
Note that the substring ab appears twice within D, but appears only once
as a shingle. A variation of shingling produces a bag, rather than a set, so each
shingle would appear in the result as many times as it appears in the document.
However, we shall not use bags of shingles here. ✷
There are several options regarding how white space (blank, tab, newline,
etc.) is treated. It probably makes sense to replace any sequence of one or more
white-space characters by a single blank. That way, we distinguish shingles that
cover two or more words from those that do not.
Example 3.4 : If we use k = 9, but eliminate whitespace altogether, then we
would see some lexical similarity in the sentences “The plane was ready for
touch down”. and “The quarterback scored a touchdown”. However, if we
retain the blanks, then the first has shingles touch dow and ouch down, while
the second has touchdown. If we eliminated the blanks, then both would have
touchdown. ✷
3.2. SHINGLING OF DOCUMENTS 79
3.2.2 Choosing the Shingle Size
We can pick k to be any constant we like. However, if we pick k too small, then
we would expect most sequences of k characters to appear in most documents.
If so, then we could have documents whose shingle-sets had high Jaccard similarity, yet the documents had none of the same sentences or even phrases. As
an extreme example, if we use k = 1, most Web pages will have most of the
common characters and few other characters, so almost all Web pages will have
high similarity.
How large k should be depends on how long typical documents are and how
large the set of typical characters is. The important thing to remember is:
• k should be picked large enough that the probability of any given shingle
appearing in any given document is low.
Thus, if our corpus of documents is emails, picking k = 5 should be fine.
To see why, suppose that only letters and a general white-space character appear in emails (although in practice, most of the printable ASCII characters
can be expected to appear occasionally). If so, then there would be 275 =
14,348,907 possible shingles. Since the typical email is much smaller than 14
million characters long, we would expect k = 5 to work well, and indeed it does.
However, the calculation is a bit more subtle. Surely, more than 27 characters appear in emails, However, all characters do not appear with equal probability. Common letters and blanks dominate, while ”z” and other letters that
have high point-value in Scrabble are rare. Thus, even short emails will have
many 5-shingles consisting of common letters, and the chances of unrelated
emails sharing these common shingles is greater than would be implied by the
calculation in the paragraph above. A good rule of thumb is to imagine that
there are only 20 characters and estimate the number of k-shingles as 20k
. For
large documents, such as research articles, choice k = 9 is considered safe.
3.2.3 Hashing Shingles
Instead of using substrings directly as shingles, we can pick a hash function
that maps strings of length k to some number of buckets and treat the resulting
bucket number as the shingle. The set representing a document is then the
set of integers that are bucket numbers of one or more k-shingles that appear
in the document. For instance, we could construct the set of 9-shingles for a
document and then map each of those 9-shingles to a bucket number in the
range 0 to 232 − 1. Thus, each shingle is represented by four bytes instead
of nine. Not only has the data been compacted, but we can now manipulate
(hashed) shingles by single-word machine operations.
Notice that we can differentiate documents better if we use 9-shingles and
hash them down to four bytes than to use 4-shingles, even though the space used
to represent a shingle is the same. The reason was touched upon in Section 3.2.2.
If we use 4-shingles, most sequences of four bytes are unlikely or impossible to
80 CHAPTER 3. FINDING SIMILAR ITEMS
find in typical documents. Thus, the effective number of different shingles is
much less than 232 −1. If, as in Section 3.2.2, we assume only 20 characters are
frequent in English text, then the number of different 4-shingles that are likely
to occur is only (20)4 = 160,000. However, if we use 9-shingles, there are many
more than 232 likely shingles. When we hash them down to four bytes, we can
expect almost any sequence of four bytes to be possible, as was discussed in
Section 1.3.2.
3.2.4 Shingles Built from Words
An alternative form of shingle has proved effective for the problem of identifying
similar news articles, mentioned in Section 3.1.2. The exploitable distinction for
this problem is that the news articles are written in a rather different style than
are other elements that typically appear on the page with the article. News
articles, and most prose, have a lot of stop words (see Section 1.3.1), the most
common words such as “and,” “you,” “to,” and so on. In many applications,
we want to ignore stop words, since they don’t tell us anything useful about
the article, such as its topic.
However, for the problem of finding similar news articles, it was found that
defining a shingle to be a stop word followed by the next two words, regardless
of whether or not they were stop words, formed a useful set of shingles. The
advantage of this approach is that the news article would then contribute more
shingles to the set representing the Web page than would the surrounding elements. Recall that the goal of the exercise is to find pages that had the same
articles, regardless of the surrounding elements. By biasing the set of shingles
in favor of the article, pages with the same article and different surrounding
material have higher Jaccard similarity than pages with the same surrounding
material but with a different article.
Example 3.5 : An ad might have the simple text “Buy Sudzo.” However, a
news article with the same idea might read something like “A spokesperson
for the Sudzo Corporation revealed today that studies have shown it is
good for people to buy Sudzo products.” Here, we have italicized all the
likely stop words, although there is no set number of the most frequent words
that should be considered stop words. The first three shingles made from a
stop word and the next two following are:
A spokesperson for
for the Sudzo
the Sudzo Corporation
There are nine shingles from the sentence, but none from the “ad.” ✷
3.2.5 Exercises for Section 3.2
Exercise 3.2.1 : What are the first ten 3-shingles in the first sentence of Section 3.2?
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 81
Exercise 3.2.2 : If we use the stop-word-based shingles of Section 3.2.4, and
we take the stop words to be all the words of three or fewer letters, then what
are the shingles in the first sentence of Section 3.2?
Exercise 3.2.3 : What is the largest number of k-shingles a document of n
bytes can have? You may assume that the size of the alphabet is large enough
that the number of possible strings of length k is at least n.
3.3 Similarity-Preserving Summaries of Sets
Sets of shingles are large. Even if we hash them to four bytes each, the space
needed to store a set is still roughly four times the space taken by the document.
If we have millions of documents, it may well not be possible to store all the
shingle-sets in main memory.3
Our goal in this section is to replace large sets by much smaller representations called “signatures.” The important property we need for signatures is
that we can compare the signatures of two sets and estimate the Jaccard similarity of the underlying sets from the signatures alone. It is not possible that
the signatures give the exact similarity of the sets they represent, but the estimates they provide are close, and the larger the signatures the more accurate
the estimates. For example, if we replace the 200,000-byte hashed-shingle sets
that derive from 50,000-byte documents by signatures of 1000 bytes, we can
usually get within a few percent.
3.3.1 Matrix Representation of Sets
Before explaining how it is possible to construct small signatures from large
sets, it is helpful to visualize a collection of sets as their characteristic matrix.
The columns of the matrix correspond to the sets, and the rows correspond to
elements of the universal set from which elements of the sets are drawn. There
is a 1 in row r and column c if the element for row r is a member of the set for
column c. Otherwise the value in position (r, c) is 0.
Element S1 S2 S3 S4
a 1 0 0 1
b 0 0 1 0
c 0 1 0 1
d 1 0 1 1
e 0 0 1 0
Figure 3.2: A matrix representing four sets
3There is another serious concern: even if the sets fit in main memory, the number of pairs
may be too great for us to evaluate the similarity of each pair. We take up the solution to
this problem in Section 3.4.
82 CHAPTER 3. FINDING SIMILAR ITEMS
Example 3.6 : In Fig. 3.2 is an example of a matrix representing sets chosen
from the universal set {a, b, c, d, e}. Here, S1 = {a, d}, S2 = {c}, S3 = {b, d, e},
and S4 = {a, c, d}. The top row and leftmost columns are not part of the matrix,
but are present only to remind us what the rows and columns represent. ✷
It is important to remember that the characteristic matrix is unlikely to be
the way the data is stored, but it is useful as a way to visualize the data. For one
reason not to store data as a matrix, these matrices are almost always sparse
(they have many more 0’s than 1’s) in practice. It saves space to represent a
sparse matrix of 0’s and 1’s by the positions in which the 1’s appear. For another
reason, the data is usually stored in some other format for other purposes.
As an example, if rows are products, and columns are customers, represented
by the set of products they bought, then this data would really appear in a
database table of purchases. A tuple in this table would list the item, the
purchaser, and probably other details about the purchase, such as the date and
the credit card used.
3.3.2 Minhashing
The signatures we desire to construct for sets are composed of the results of a
large number of calculations, say several hundred, each of which is a “minhash”
of the characteristic matrix. In this section, we shall learn how a minhash is
computed in principle, and in later sections we shall see how a good approximation to the minhash is computed in practice.
To minhash a set represented by a column of the characteristic matrix, pick
a permutation of the rows. The minhash value of any column is the number of
the first row, in the permuted order, in which the column has a 1.
Example 3.7 : Let us suppose we pick the order of rows beadc for the matrix
of Fig. 3.2. This permutation defines a minhash function h that maps sets to
rows. Let us compute the minhash value of set S1 according to h. The first
column, which is the column for set S1, has 0 in row b, so we proceed to row e,
the second in the permuted order. There is again a 0 in the column for S1, so
we proceed to row a, where we find a 1. Thus. h(S1) = a.
Element S1 S2 S3 S4
b 0 0 1 0
e 0 0 1 0
a 1 0 0 1
d 1 0 1 1
c 0 1 0 1
Figure 3.3: A permutation of the rows of Fig. 3.2
Although it is not physically possible to permute very large characteristic
matrices, the minhash function h implicitly reorders the rows of the matrix of
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 83
Fig. 3.2 so it becomes the matrix of Fig. 3.3. In this matrix, we can read off
the values of h by scanning from the top until we come to a 1. Thus, we see
that h(S2) = c, h(S3) = b, and h(S4) = a. ✷
3.3.3 Minhashing and Jaccard Similarity
There is a remarkable connection between minhashing and Jaccard similarity
of the sets that are minhashed.
• The probability that the minhash function for a random permutation of
rows produces the same value for two sets equals the Jaccard similarity
of those sets.
To see why, we need to picture the columns for those two sets. If we restrict
ourselves to the columns for sets S1 and S2, then rows can be divided into three
classes:
1. Type X rows have 1 in both columns.
2. Type Y rows have 1 in one of the columns and 0 in the other.
3. Type Z rows have 0 in both columns.
Since the matrix is sparse, most rows are of type Z. However, it is the ratio
of the numbers of type X and type Y rows that determine both SIM(S1, S2)
and the probability that h(S1) = h(S2). Let there be x rows of type X and y
rows of type Y . Then SIM(S1, S2) = x/(x + y). The reason is that x is the size
of S1 ∩ S2 and x + y is the size of S1 ∪ S2.
Now, consider the probability that h(S1) = h(S2). If we imagine the rows
permuted randomly, and we proceed from the top, the probability that we shall
meet a type X row before we meet a type Y row is x/(x + y). But if the
first row from the top other than type Z rows is a type X row, then surely
h(S1) = h(S2). On the other hand, if the first row other than a type Z row
that we meet is a type Y row, then the set with a 1 gets that row as its minhash
value. However the set with a 0 in that row surely gets some row further down
the permuted list. Thus, we know h(S1) 6= h(S2) if we first meet a type Y row.
We conclude the probability that h(S1) = h(S2) is x/(x + y), which is also the
Jaccard similarity of S1 and S2.
3.3.4 Minhash Signatures
Again think of a collection of sets represented by their characteristic matrix M.
To represent sets, we pick at random some number n of permutations of the
rows of M. Perhaps 100 permutations or several hundred permutations will do.
Call the minhash functions determined by these permutations h1, h2, . . . , hn.
From the column representing set S, construct the minhash signature for S, the
vector [h1(S), h2(S), . . . , hn(S)]. We normally represent this list of hash-values
84 CHAPTER 3. FINDING SIMILAR ITEMS
as a column. Thus, we can form from matrix M a signature matrix, in which
the ith column of M is replaced by the minhash signature for (the set of) the
ith column.
Note that the signature matrix has the same number of columns as M but
only n rows. Even if M is not represented explicitly, but in some compressed
form suitable for a sparse matrix (e.g., by the locations of its 1’s), it is normal
for the signature matrix to be much smaller than M.
The remarkable thing about signature matrices is that we can use their
columns to estimate the Jaccard similarity of the sets that correspond to the
columns of signature matrix. By the theorem proved in Section 3.3.3, we know
that the probability that two columns have the same value in a given row of
the signature matrix equals the Jaccard similarity of the sets corresponding to
those columns. Moreover, since the permutations on which the minhash values
are based were chosen independently, we can think of each row of the signature
matrix as an independent experiment. Thus, the expected number of rows in
which two columns agree equals the Jaccard similarity of their corresponding
sets. Moreover, the more minhashings we use, i.e., the more rows in the signature matrix, the smaller the expected error in the estimate of the Jaccard
similarity will be.
3.3.5 Computing Minhash Signatures in Practice
It is not feasible to permute a large characteristic matrix explicitly. Even picking
a random permutation of millions or billions of rows is time-consuming, and
the necessary sorting of the rows would take even more time. Thus, permuted
matrices like that suggested by Fig. 3.3, while conceptually appealing, are not
implementable.
Fortunately, it is possible to simulate the effect of a random permutation by
a random hash function that maps row numbers to as many buckets as there
are rows. A hash function that maps integers 0, 1, . . . , k − 1 to bucket numbers
0 through k−1 typically will map some pairs of integers to the same bucket and
leave other buckets unfilled. However, the difference is unimportant as long as
k is large and there are not too many collisions. We can maintain the fiction
that our hash function h “permutes” row r to position h(r) in the permuted
order.
Thus, instead of picking n random permutations of rows, we pick n randomly
chosen hash functions h1, h2, . . . , hn on the rows. We construct the signature
matrix by considering each row in their given order. Let SIG(i, c) be the element
of the signature matrix for the ith hash function and column c. Initially, set
SIG(i, c) to ∞ for all i and c. We handle row r by doing the following:
1. Compute h1(r), h2(r), . . . , hn(r).
2. For each column c do the following:
(a) If c has 0 in row r, do nothing.
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 85
(b) However, if c has 1 in row r, then for each i = 1, 2, . . . , n set SIG(i, c)
to the smaller of the current value of SIG(i, c) and hi(r).
Row S1 S2 S3 S4 x + 1 mod 5 3x + 1 mod 5
0 1 0 0 1 1 1
1 0 0 1 0 2 4
2 0 1 0 1 3 2
3 1 0 1 1 4 0
4 0 0 1 0 0 3
Figure 3.4: Hash functions computed for the matrix of Fig. 3.2
Example 3.8 : Let us reconsider the characteristic matrix of Fig. 3.2, which
we reproduce with some additional data as Fig. 3.4. We have replaced the
letters naming the rows by integers 0 through 4. We have also chosen two hash
functions: h1(x) = x+1 mod 5 and h2(x) = 3x+1 mod 5. The values of these
two functions applied to the row numbers are given in the last two columns of
Fig. 3.4. Notice that these simple hash functions are true permutations of the
rows, but a true permutation is only possible because the number of rows, 5, is
a prime. In general, there will be collisions, where two rows get the same hash
value.
Now, let us simulate the algorithm for computing the signature matrix.
Initially, this matrix consists of all ∞’s:
S1 S2 S3 S4
h1 ∞ ∞ ∞ ∞
h2 ∞ ∞ ∞ ∞
First, we consider row 0 of Fig. 3.4. We see that the values of h1(0) and
h2(0) are both 1. The row numbered 0 has 1’s in the columns for sets S1 and
S4, so only these columns of the signature matrix can change. As 1 is less than
∞, we do in fact change both values in the columns for S1 and S4. The current
estimate of the signature matrix is thus:
S1 S2 S3 S4
h1 1 ∞ ∞ 1
h2 1 ∞ ∞ 1
Now, we move to the row numbered 1 in Fig. 3.4. This row has 1 only in
S3, and its hash values are h1(1) = 2 and h2(1) = 4. Thus, we set SIG(1, 3) to 2
and SIG(2, 3) to 4. All other signature entries remain as they are because their
columns have 0 in the row numbered 1. The new signature matrix:
S1 S2 S3 S4
h1 1 ∞ 2 1
h2 1 ∞ 4 1
86 CHAPTER 3. FINDING SIMILAR ITEMS
The row of Fig. 3.4 numbered 2 has 1’s in the columns for S2 and S4, and
its hash values are h1(2) = 3 and h2(2) = 2. We could change the values in the
signature for S4, but the values in this column of the signature matrix, [1, 1], are
each less than the corresponding hash values [3, 2]. However, since the column
for S2 still has ∞’s, we replace it by [3, 2], resulting in:
S1 S2 S3 S4
h1 1 3 2 1
h2 1 2 4 1
Next comes the row numbered 3 in Fig. 3.4. Here, all columns but S2 have
1, and the hash values are h1(3) = 4 and h2(3) = 0. The value 4 for h1 exceeds
what is already in the signature matrix for all the columns, so we shall not
change any values in the first row of the signature matrix. However, the value
0 for h2 is less than what is already present, so we lower SIG(2, 1), SIG(2, 3) and
SIG(2, 4) to 0. Note that we cannot lower SIG(2, 2) because the column for S2 in
Fig. 3.4 has 0 in the row we are currently considering. The resulting signature
matrix:
S1 S2 S3 S4
h1 1 3 2 1
h2 0 2 0 0
Finally, consider the row of Fig. 3.4 numbered 4. h1(4) = 0 and h2(4) = 3.
Since row 4 has 1 only in the column for S3, we only compare the current
signature column for that set, [2, 0] with the hash values [0, 3]. Since 0 < 2, we
change SIG(1, 3) to 0, but since 3 > 0 we do not change SIG(2, 3). The final
signature matrix is:
S1 S2 S3 S4
h1 1 3 0 1
h2 0 2 0 0
We can estimate the Jaccard similarities of the underlying sets from this
signature matrix. Notice that columns 1 and 4 are identical, so we guess that
SIM(S1, S4) = 1.0. If we look at Fig. 3.4, we see that the true Jaccard similarity
of S1 and S4 is 2/3. Remember that the fraction of rows that agree in the
signature matrix is only an estimate of the true Jaccard similarity, and this
example is much too small for the law of large numbers to assure that the
estimates are close. For additional examples, the signature columns for S1 and
S3 agree in half the rows (true similarity 1/4), while the signatures of S1 and
S2 estimate 0 as their Jaccard similarity (the correct value). ✷
3.3.6 Speeding Up Minhashing
The process of minhashing is time-consuming, since we need to examine the
entire k-row matrix M for each minhash function we want. Let us first return
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 87
to the model of Section 3.3.2, where we imagine rows are actually permuted.
But to compute one minhash function on all the columns, we shall not go all
the way to the end of the permutation, but only look at the first m out of k
rows. If we make m small compared with k, we reduce the work by a large
factor, k/m.
However, there is a downside to making m small. As long as each column
has at least one 1 in the first m rows in permuted order, the rows after the mth
have no effect on any minhash value and may as well not be looked at. But
what if some columns are all-0’s in the first m rows? We have no minhash value
for those columns, and will instead have to use a special symbol, for which we
shall use ∞.
When we examine the minhash signatures of two columns in order to estimate the Jaccard similarity of their underlying sets, as in Section 3.3.4, we have
to take into account the possibility that one or both columns have ∞ as their
minhash value for some components of the signature. There are three cases:
1. If neither column has ∞ in a given row, then there is no change needed.
Count this row as an example of equal values if the two values are the
same, and as an example of unequal values if not.
2. One column has ∞ and the other does not. In this case, had we used
all the rows of the original permuted matrix M, the column that has the
∞ would eventually have been given some row number, and that number
will surely not be one of the first m rows in the permuted order. But the
other column does have a value that is one of the first m rows. Thus, we
surely have an example of unequal minhash values, and we count this row
of the signature matrix as such an example.
3. Now, suppose both columns have ∞ in row. Then in the original permuted
matrix M, the first m rows of both columns were all 0’s. We thus have no
information about the Jaccard similarity of the corresponding sets; that
similarity is only a function of the last k − m rows, which we have chosen
not to look at. We therefore count this row of the signature matrix as
neither an example of equal values nor of unequal values.
As long as the third case, where both columns have ∞, is rare, we get
almost as many examples to average as there are rows in the signature matrix.
That effect will reduce the accuracy of our estimates of the Jaccard distance
somewhat, but not much. And since we are now able to compute minhash
values for all the columns much faster than if we examined all the rows of M,
we can afford the time to apply a few more minhash functions. We get even
better accuracy than originally, and we do so faster than before.
3.3.7 Speedup Using Hash Functions
As before, there are reasons not to physically permute rows in the manner
assumed in Section 3.3.6. However, the idea of true permutations makes more
88 CHAPTER 3. FINDING SIMILAR ITEMS
sense in the context of Section 3.3.6 than it did in Section 3.3.2. The reason
is that we do not need to construct a full permutation of k elements, but only
pick a small number m out of the k rows and then pick a random permutation
of those rows. Depending on the value of m and how the matrix M is stored, it
might make sense to follow the algorithm suggested by Section 3.3.6 literally.
However, it is more likely that a strategy akin to Section 3.3.5 is needed.
Now, the rows of M are fixed, and not permuted. We choose a hash function
that hashes row numbers, and compute hash values for only the first m rows.
That is, we follow the algorithm of Section 3.3.5, but only until we reach the
mth row, whereupon we stop and, and for each columns, we take the minimum
hash value seen so far as the minhash value for that column.
Since some column may have 0 in all m rows, it is possible that some of the
minhash values will be ∞. Assuming m is sufficiently large that ∞ minhash
values are rare, we still get a good estimate of the Jaccard similarity of sets by
comparing columns of the signature matrix. Suppose T is the set of elements
of the universal set that are represented by the first m rows of matrix M. Let
S1 and S2 be the sets represented by two columns of M. Then the first m rows
of M represent the sets S1 ∩ T and S2 ∩ T . If both these sets are empty (i.e.,
both columns are all-0 in their first m rows), then this minhash function will be
∞ in both columns and will be ignored when estimating the Jaccard similarity
of the columns’ underlying sets.
If at least one of the sets S1 ∩ T and S2 ∩ T is nonempty, then the probability of the two columns having equal values for this minhash function is the
Jaccard similarity of these two sets, that is
|S1 ∩ S2 ∩ T |
|(S1 ∪ S2) ∩ T |
As long as T is chosen to be a random subset of the universal set, the expected
value of this fraction will be the same as the Jaccard similarity of S1 and S2.
However, there will be some random variation, since depending on T , we could
find more or less than an average number of type X rows (1’s in both columns)
and/or type Y rows (1 in one column and 0 in the other) among the first m
rows of matrix M.
To mitigate this variation, we do not use the same set T for each minhashing
that we do. Rather, we divide the rows of M into k/m groups.4 Then for each
hash function, we compute one minhash value by examining only the first m
rows of M, a different minhash value by examining only the second m rows,
and so on. We thus get k/m minhash values from a single hash function and
a single pass over all the rows of M. In fact, if k/m is large enough, we may
get all the rows of the signature matrix that we need by a single hash function
applied to each of the subsets of rows of M.
4
In what follows, we assume m divides k evenly, for convenience. It is unimportant, as
long as k/m is large, if some rows are not included in any group because k is not an integer
multiple of m.
3.3. SIMILARITY-PRESERVING SUMMARIES OF SETS 89
Moreover, by using each of the rows of M to compute one of these minhash
values, we tend to balance out the errors in estimation of the Jaccard similarity
due to any one particular subset of the rows. That is, the Jaccard similarity
of S1 and S2 determines the ratio of type X and type Y rows. All the type X
rows are distributed among the k/m sets of rows, and likewise the type Y rows.
Thus, while one set of m rows may have more of one type of row than average,
there must then be some other set of m rows with fewer than average of that
same type.
Example 3.9 : In Fig. 3.5 we see a matrix representing three sets S1, S2, and
S3, with a universal set of eight elements; i.e., k = 8. Let us pick m = 4, so one
pass through the rows yields two minhash values, one based on the first four
rows and the other on the second four rows.
S1 S2 S3
0 0 0
0 0 0
0 0 1
0 1 1
1 1 1
1 1 0
1 0 0
0 0 0
Figure 3.5: A Boolean matrix representing three sets
First, note that the Jaccard similarities of the three sets are SIM(S1, S2) =
1/2, SIM(S1, S3) = 1/5, and SIM(S2, S3) = 1/2. Now, look at the first four
rows only. Whatever hash function we use, the minhash value for S1 will be
∞, the minhash value for S2 will be the hash value of the 4th row, and the
minhash value for S3 will be the smaller of the hash values for the third and
fourth rows. Thus, the minhash values for S1 and S2 will never agree. That
makes sense, since if T is the set of elements represented by the first four rows,
then S1 ∩ T = ∅, and therefore SIM(S1 ∩ T, S2 ∩ T ) = 0. However, in the
second four rows, the Jaccard similarity of S1 and S2 restricted to the elements
represented by the last four rows is 2/3.
We conclude that if we generate signatures consisting of two minhash values
using this hash function, one based on the first four rows and the second based
on the last four rows, the expected number of matches we get between the
signatures for S1 and S2 is the average of 0 and 2/3, or 1/3. Since the actual
Jaccard similarity of S1 and S2 is 1/2, there is an error, but not too great an
error. In larger examples, where minhash values are based on far more than
four rows, the expected error will approach zero.
Similarly, we can see the effect of splitting the rows on the other two pairs
of columns. Between S1 and S3, the top half represents sets with a Jaccard
90 CHAPTER 3. FINDING SIMILAR ITEMS
similarity of 0, while the bottom half represents sets with a Jaccard similarity
1/3. The expected number of matches in the signatures of S1 and S3 is therefore
the average of these, or 1/6. That compares with the true Jaccard similarity
SIM(S1, S3) = 1/5. Finally, when we compare S2 and S3, we note that the
Jaccard similarity of these columns in the first four rows is 1/2, and so is their
Jaccard similarity in the bottom four rows. The average, 1/2, also agrees exactly
with SIM(S2, S3) = 1/2. ✷
3.3.8 Exercises for Section 3.3
Exercise 3.3.1 : Verify the theorem from Section 3.3.3, which relates the Jaccard similarity to the probability of minhashing to equal values, for the particular case of Fig. 3.2.
(a) Compute the Jaccard similarity of each of the pairs of columns in Fig. 3.2.
! (b) Compute, for each pair of columns of that figure, the fraction of the 120
permutations of the rows that make the two columns hash to the same
value.
Exercise 3.3.2 : Using the data from Fig. 3.4, add to the signatures of the
columns the values of the following hash functions:
(a) h3(x) = 2x + 4 mod 5.
(b) h4(x) = 3x − 1 mod 5.
Element S1 S2 S3 S4
0 0 1 0 1
1 0 1 0 0
2 1 0 0 1
3 0 0 1 0
4 0 0 1 1
5 1 0 0 0
Figure 3.6: Matrix for Exercise 3.3.3
Exercise 3.3.3 : In Fig. 3.6 is a matrix with six rows.
(a) Compute the minhash signature for each column if we use the following
three hash functions: h1(x) = 2x + 1 mod 6; h2(x) = 3x + 2 mod 6;
h3(x) = 5x + 2 mod 6.
(b) Which of these hash functions are true permutations?
3.4. LOCALITY-SENSITIVE HASHING FOR DOCUMENTS 91
(c) How close are the estimated Jaccard similarities for the six pairs of columns
to the true Jaccard similarities?
! Exercise 3.3.4 : Now that we know Jaccard similarity is related to the probability that two sets minhash to the same value, reconsider Exercise 3.1.3. Can
you use this relationship to simplify the problem of computing the expected
Jaccard similarity of randomly chosen sets?
! Exercise 3.3.5 : Prove that if the Jaccard similarity of two columns is 0, then
minhashing always gives a correct estimate of the Jaccard similarity.
!! Exercise 3.3.6 : One might expect that we could estimate the Jaccard similarity of columns without using all possible permutations of rows. For example,
we could only allow cyclic permutations; i.e., start at a randomly chosen row
r, which becomes the first in the order, followed by rows r + 1, r + 2, and so
on, down to the last row, and then continuing with the first row, second row,
and so on, down to row r − 1. There are only n such permutations if there are
n rows. However, these permutations are not sufficient to estimate the Jaccard
similarity correctly. Give an example of a two-column matrix where averaging
over all the cyclic permutations does not give the Jaccard similarity.
! Exercise 3.3.7 : Suppose we want to use a MapReduce framework to compute
minhash signatures. If the matrix is stored in chunks that correspond to some
columns, then it is quite easy to exploit parallelism. Each Map task gets some
of the columns and all the hash functions, and computes the minhash signatures
of its given columns. However, suppose the matrix were chunked by rows, so
that a Map task is given the hash functions and a set of rows to work on. Design
Map and Reduce functions to exploit MapReduce with data in this form.
! Exercise 3.3.8 : As we noticed in Section 3.3.6, we have problems when a
column has only 0’s. If we compute a minhash function using entire columns
(as in Section 3.3.2), then the only time we get all 0’s in a column is if that
column represents the empty set. How should we handle the empty set to make
sure no errors in Jaccard-similarity estimation are introduced?
!! Exercise 3.3.9 : In Example 3.9, each of the three estimates of Jaccard similarity we obtained was either smaller than or the same as the true Jaccard
similarity. Is it possible that for another pair of columns, the average of the
Jaccard similarities of the upper and lower halves will exceed the actual Jaccard
similarity of the columns?
3.4 Locality-Sensitive Hashing for Documents
Even though we can use minhashing to compress large documents into small
signatures and preserve the expected similarity of any pair of documents, it
still may be impossible to find the pairs with greatest similarity efficiently. The
92 CHAPTER 3. FINDING SIMILAR ITEMS
reason is that the number of pairs of documents may be too large, even if there
are not too many documents.
Example 3.10 : Suppose we have a million documents, and we use signatures
of length 250. Then we use 1000 bytes per document for the signatures, and
the entire data fits in a gigabyte – less than a typical main memory of a laptop.
However, there are
1,000,000
2

or half a trillion pairs of documents. If it takes a
microsecond to compute the similarity of two signatures, then it takes almost
six days to compute all the similarities on that laptop. ✷
If our goal is to compute the similarity of every pair, there is nothing we
can do to reduce the work, although parallelism can reduce the elapsed time.
However, often we want only the most similar pairs or all pairs that are above
some lower bound in similarity. If so, then we need to focus our attention only
on pairs that are likely to be similar, without investigating every pair. There is
a general theory of how to provide such focus, called locality-sensitive hashing
(LSH) or near-neighbor search. In this section we shall consider a specific form
of LSH, designed for the particular problem we have been studying: documents,
represented by shingle-sets, then minhashed to short signatures. In Section 3.6
we present the general theory of locality-sensitive hashing and a number of
applications and related techniques.
3.4.1 LSH for Minhash Signatures
One general approach to LSH is to “hash” items several times, in such a way that
similar items are more likely to be hashed to the same bucket than dissimilar
items are. We then consider any pair that hashed to the same bucket for any
of the hashings to be a candidate pair. We check only the candidate pairs for
similarity. The hope is that most of the dissimilar pairs will never hash to the
same bucket, and therefore will never be checked. Those dissimilar pairs that
do hash to the same bucket are false positives; we hope these will be only a
small fraction of all pairs. We also hope that most of the truly similar pairs
will hash to the same bucket under at least one of the hash functions. Those
that do not are false negatives; we hope these will be only a small fraction of
the truly similar pairs.
If we have minhash signatures for the items, an effective way to choose the
hashings is to divide the signature matrix into b bands consisting of r rows
each. For each band, there is a hash function that takes vectors of r integers
(the portion of one column within that band) and hashes them to some large
number of buckets. We can use the same hash function for all the bands, but
we use a separate bucket array for each band, so columns with the same vector
in different bands will not hash to the same bucket.
Example 3.11 : Figure 3.7 shows part of a signature matrix of 12 rows divided
into four bands of three rows each. The second and fourth of the explicitly
shown columns each have the column vector [0, 2, 1] in the first band, so the
3.4. LOCALITY-SENSITIVE HASHING FOR DOCUMENTS 93
1 0 0 0 2
3 2 1 2 2
0 1 3 1 1
. . . . . . band 1
band 2
band 3
band 4
Figure 3.7: Dividing a signature matrix into four bands of three rows per band
will definitely hash to the same bucket in the hashing for the first band. Thus,
regardless of what those columns look like in the other three bands, this pair
of columns will be a candidate pair. It is possible that other columns, such as
the first two shown explicitly, will also hash to the same bucket according to
the hashing of the first band. However, since their column vectors are different,
[1, 3, 0] and [0, 2, 1], and there are many buckets for each hashing, we expect the
chances of an accidental collision to be very small. We shall normally assume
that two vectors hash to the same bucket if and only if they are identical.
Two columns that do not agree in band 1 have three other chances to become
a candidate pair; they might be identical in any one of these other bands.
However, observe that the more similar two columns are, the more likely it is
that they will be identical in some band. Thus, intuitively the banding strategy
makes similar columns much more likely to be candidate pairs than dissimilar
pairs. ✷
3.4.2 Analysis of the Banding Technique
Suppose we use b bands of r rows each, and suppose that a particular pair of
documents have Jaccard similarity s. Recall from Section 3.3.3 that the probability the minhash signatures for these documents agree in any one particular
row of the signature matrix is s. We can calculate the probability that these
documents (or rather their signatures) become a candidate pair as follows:
1. The probability that the signatures agree in all rows of one particular
band is s
r
.
2. The probability that the signatures disagree in at least one row of a particular band is 1 − s
r
.
3. The probability that the signatures disagree in at least one row of each
of the bands is (1 − s
r
)
b
.
94 CHAPTER 3. FINDING SIMILAR ITEMS
4. The probability that the signatures agree in all the rows of at least one
band, and therefore become a candidate pair, is 1 − (1 − s
r
)
b
.
0 1
 of documents
Jaccard similarity
Probability
of becoming
a candidate
Figure 3.8: The S-curve
It may not be obvious, but regardless of the chosen constants b and r, this
function has the form of an S-curve, as suggested in Fig. 3.8. The threshold,
that is, the value of similarity s at which the probability of becoming a candidate is 1/2, is a function of b and r. The threshold is roughly where the rise is
the steepest, and for large b and r we find that pairs with similarity above the
threshold are very likely to become candidates, while those below the threshold
are unlikely to become candidates – exactly the situation we want. An approximation to the threshold is (1/b)
1/r. For example, if b = 16 and r = 4, then the
threshold is approximately at s = 1/2, since the 4th root of 1/16 is 1/2.
Example 3.12 : Let us consider the case b = 20 and r = 5. That is, we suppose
we have signatures of length 100, divided into twenty bands of five rows each.
Figure 3.9 tabulates some of the values of the function 1 − (1 − s
5
)
20. Notice
that the threshold, the value of s at which the curve has risen halfway, is just
slightly more than 0.5. Also notice that the curve is not exactly the ideal step
function that jumps from 0 to 1 at the threshold, but the slope of the curve
in the middle is significant. For example, it rises by more than 0.6 going from
s = 0.4 to s = 0.6, so the slope in the middle is greater than 3.
For example, at s = 0.8, 1 − (0.8)5
is about 0.672. If you raise this number
to the 20th power, you get about 0.00035. Subtracting this fraction from 1
yields 0.99965. That is, if we consider two documents with 80% similarity, then
in any one band, they have only about a 33% chance of agreeing in all five rows
and thus becoming a candidate pair. However, there are 20 bands and thus 20
3.4. LOCALITY-SENSITIVE HASHING FOR DOCUMENTS 95
s 1 − (1 − s
r
)
b
.2 .006
.3 .047
.4 .186
.5 .470
.6 .802
.7 .975
.8 .9996
Figure 3.9: Values of the S-curve for b = 20 and r = 5
chances to become a candidate. Only roughly one in 3000 pairs that are as high
as 80% similar will fail to become a candidate pair and thus be a false negative.
✷
3.4.3 Combining the Techniques
We can now give an approach to finding the set of candidate pairs for similar
documents and then discovering the truly similar documents among them. It
must be emphasized that this approach can produce false negatives – pairs of
similar documents that are not identified as such because they never become
a candidate pair. There will also be false positives – candidate pairs that are
evaluated, but are found not to be sufficiently similar.
1. Pick a value of k and construct from each document the set of k-shingles.
Optionally, hash the k-shingles to shorter bucket numbers.
2. Sort the document-shingle pairs to order them by shingle.
3. Pick a length n for the minhash signatures. Feed the sorted list to the
algorithm of Section 3.3.5 to compute the minhash signatures for all the
documents.
4. Choose a threshold t that defines how similar documents have to be in
order for them to be regarded as a desired “similar pair.” Pick a number
of bands b and a number of rows r such that br = n, and the threshold
t is approximately (1/b)
1/r. If avoidance of false negatives is important,
you may wish to select b and r to produce a threshold lower than t; if
speed is important and you wish to limit false positives, select b and r to
produce a higher threshold.
5. Construct candidate pairs by applying the LSH technique of Section 3.4.1.
6. Examine each candidate pair’s signatures and determine whether the fraction of components in which they agree is at least t.
96 CHAPTER 3. FINDING SIMILAR ITEMS
7. Optionally, if the signatures are sufficiently similar, go to the documents
themselves and check that they are truly similar, rather than documents
that, by luck, had similar signatures.
3.4.4 Exercises for Section 3.4
Exercise 3.4.1 : Evaluate the S-curve 1 − (1 − s
r
)
b
for s = 0.1, 0.2, . . . , 0.9, for
the following values of r and b:
• r = 3 and b = 10.
• r = 6 and b = 20.
• r = 5 and b = 50.
! Exercise 3.4.2 : For each of the (r, b) pairs in Exercise 3.4.1, compute the
threshold, that is, the value of s for which the value of 1−(1−s
r
)
b
is exactly 1/2.
How does this value compare with the estimate of (1/b)
1/r that was suggested
in Section 3.4.2?
! Exercise 3.4.3 : Use the techniques explained in Section 1.3.5 to approximate
the S-curve 1 − (1 − s
r
)
b when s
r
is very small.
! Exercise 3.4.4 : Suppose we wish to implement LSH by MapReduce. Specifically, assume chunks of the signature matrix consist of columns, and elements
are key-value pairs where the key is the column number and the value is the
signature itself (i.e., a vector of values).
(a) Show how to produce the buckets for all the bands as output of a single
MapReduce process. Hint: Remember that a Map function can produce
several key-value pairs from a single element.
(b) Show how another MapReduce process can convert the output of (a) to
a list of pairs that need to be compared. Specifically, for each column i,
there should be a list of those columns j > i with which i needs to be
compared.
3.5 Distance Measures
We now take a short detour to study the general notion of distance measures.
The Jaccard similarity is a measure of how close sets are, although it is not
really a distance measure. That is, the closer sets are, the higher the Jaccard
similarity. Rather, 1 minus the Jaccard similarity is a distance measure, as we
shall see; it is called the Jaccard distance.
However, Jaccard distance is not the only measure of closeness that makes
sense. We shall examine in this section some other distance measures that have
applications. Then, in Section 3.6 we see how some of these distance measures
3.5. DISTANCE MEASURES 97
also have an LSH technique that allows us to focus on nearby points without
comparing all points. Other applications of distance measures will appear when
we study clustering in Chapter 7.
3.5.1 Definition of a Distance Measure
Suppose we have a set of points, called a space. A distance measure on this
space is a function d(x, y) that takes two points in the space as arguments and
produces a real number, and satisfies the following axioms:
1. d(x, y) ≥ 0 (no negative distances).
2. d(x, y) = 0 if and only if x = y (distances are positive, except for the
distance from a point to itself).
3. d(x, y) = d(y, x) (distance is symmetric).
4. d(x, y) ≤ d(x, z) + d(z, y) (the triangle inequality).
The triangle inequality is the most complex condition. It says, intuitively, that
to travel from x to y, we cannot obtain any benefit if we are forced to travel via
some particular third point z. The triangle-inequality axiom is what makes all
distance measures behave as if distance describes the length of a shortest path
from one point to another.
3.5.2 Euclidean Distances
The most familiar distance measure is the one we normally think of as “distance.” An n-dimensional Euclidean space is one where points are vectors of n
real numbers. The conventional distance measure in this space, which we shall
refer to as the L2-norm, is defined:
d([x1, x2, . . . , xn], [y1, y2, . . . , yn]) =
vuutXn
i=1
(xi − yi)
2
That is, we square the distance in each dimension, sum the squares, and take
the positive square root.
It is easy to verify the first three requirements for a distance measure are
satisfied. The Euclidean distance between two points cannot be negative, because the positive square root is intended. Since all squares of real numbers are
nonnegative, any i such that xi 6= yi forces the distance to be strictly positive.
On the other hand, if xi = yi for all i, then the distance is clearly 0. Symmetry
follows because (xi − yi)
2 = (yi − xi)
2
. The triangle inequality requires a good
deal of algebra to verify. However, it is well understood to be a property of
Euclidean space: the sum of the lengths of any two sides of a triangle is no less
than the length of the third side.
98 CHAPTER 3. FINDING SIMILAR ITEMS
There are other distance measures that have been used for Euclidean spaces.
For any constant r, we can define the Lr-norm to be the distance measure d
defined by:
d([x1, x2, . . . , xn], [y1, y2, . . . , yn]) = (Xn
i=1
|xi − yi
|
r
)
1/r
The case r = 2 is the usual L2-norm just mentioned. Another common distance
measure is the L1-norm, or Manhattan distance. There, the distance between
two points is the sum of the magnitudes of the differences in each dimension.
It is called “Manhattan distance” because it is the distance one would have to
travel between points if one were constrained to travel along grid lines, as on
the streets of a city such as Manhattan.
Another interesting distance measure is the L∞-norm, which is the limit
as r approaches infinity of the Lr-norm. As r gets larger, only the dimension
with the largest difference matters, so formally, the L∞-norm is defined as the
maximum of |xi − yi
| over all dimensions i.
Example 3.13 : Consider the two-dimensional Euclidean space (the customary plane) and the points (2, 7) and (6, 4). The L2-norm gives a distance
of p
(2 − 6)2 + (7 − 4)2 =
√
4
2 + 32 = 5. The L1-norm gives a distance of
|2 − 6| + |7 − 4| = 4 + 3 = 7. The L∞-norm gives a distance of
max(|2 − 6|, |7 − 4|) = max(4, 3) = 4
✷
3.5.3 Jaccard Distance
As mentioned at the beginning of the section, we define the Jaccard distance
of sets by d(x, y) = 1 − SIM(x, y). That is, the Jaccard distance is 1 minus the
ratio of the sizes of the intersection and union of sets x and y. We must verify
that this function is a distance measure.
1. d(x, y) is nonnegative because the size of the intersection cannot exceed
the size of the union.
2. d(x, y) = 0 if x = y, because x ∪ x = x ∩ x = x. However, if x 6= y, then
the size of x ∩ y is strictly less than the size of x ∪ y, so d(x, y) is strictly
positive.
3. d(x, y) = d(y, x) because both union and intersection are symmetric; i.e.,
x ∪ y = y ∪ x and x ∩ y = y ∩ x.
4. For the triangle inequality, recall from Section 3.3.3 that SIM(x, y) is the
probability a random minhash function maps x and y to the same value.
Thus, the Jaccard distance d(x, y) is the probability that a random minhash function does not send x and y to the same value. We can therefore
3.5. DISTANCE MEASURES 99
translate the condition d(x, y) ≤ d(x, z) + d(z, y) to the statement that if
h is a random minhash function, then the probability that h(x) 6= h(y)
is no greater than the sum of the probability that h(x) 6= h(z) and the
probability that h(z) 6= h(y). However, this statement is true because
whenever h(x) 6= h(y), at least one of h(x) and h(y) must be different
from h(z). They could not both be h(z), because then h(x) and h(y)
would be the same.
.
3.5.4 Cosine Distance
The cosine distance makes sense in spaces that have dimensions, including Euclidean spaces and discrete versions of Euclidean spaces, such as spaces where
points are vectors with integer components or Boolean (0 or 1) components. In
such a space, points may be thought of as directions. We do not distinguish between a vector and a multiple of that vector. Then the cosine distance between
two points is the angle that the vectors to those points make. This angle will
be in the range 0 to 180 degrees, regardless of how many dimensions the space
has.
We can calculate the cosine distance by first computing the cosine of the
angle, and then applying the arc-cosine function to translate to an angle in the
0-180 degree range. Given two vectors x and y, the cosine of the angle between
them is the dot product x.y divided by the L2-norms of x and y (i.e., their
Euclidean distances from the origin). Recall that the dot product of vectors
[x1, x2, . . . , xn].[y1, y2, . . . , yn] is Pn
i=1 xiyi
.
Example 3.14 : Let our two vectors be x = [1, 2, −1] and = [2, 1, 1]. The dot
√
product x.y is 1 × 2 + 2 × 1 + (−1) × 1 = 3. The L2-norm of both vectors is
6. For example, x has L2-norm p
1
2 + 22 + (−1)2 =
√
6. Thus, the cosine of
the angle between x and y is 3/(
√
6
√
6) or 1/2. The angle whose cosine is 1/2
is 60 degrees, so that is the cosine distance between x and y. ✷
We must show that the cosine distance is indeed a distance measure. We
have defined it so the values are in the range 0 to 180, so no negative distances
are possible. Two vectors have angle 0 if and only if they are the same direction.5
Symmetry is obvious: the angle between x and y is the same as the angle
between y and x. The triangle inequality is best argued by physical reasoning.
One way to rotate from x to y is to rotate to z and thence to y. The sum of
those two rotations cannot be less than the rotation directly from x to y.
5Notice that to satisfy the second axiom, we have to treat vectors that are multiples of
one another, e.g. [1, 2] and [3, 6], as the same direction, which they are. If we regarded these
as different vectors, we would give them distance 0 and thus violate the condition that only
d(x, x) is 0.
100 CHAPTER 3. FINDING SIMILAR ITEMS
3.5.5 Edit Distance
This distance makes sense when points are strings. The distance between two
strings x = x1x2 · · · xn and y = y1y2 · · · ym is the smallest number of insertions
and deletions of single characters that will convert x to y.
Example 3.15 : The edit distance between the strings x = abcde and y =
acfdeg is 3. To convert x to y:
1. Delete b.
2. Insert f after c.
3. Insert g after e.
No sequence of fewer than three insertions and/or deletions will convert x to y.
Thus, d(x, y) = 3. ✷
Another way to define and calculate the edit distance d(x, y) is to compute
a longest common subsequence (LCS) of x and y. An LCS of x and y is a
string that is constructed by deleting positions from x and y, and that is as
long as any string that can be constructed that way. The edit distance d(x, y)
can be calculated as the length of x plus the length of y minus twice the length
of their LCS.
Example 3.16 : The strings x = abcde and y = acfdeg from Example 3.15
have a unique LCS, which is acde. We can be sure it is the longest possible,
because it contains every symbol appearing in both x and y. Fortunately, these
common symbols appear in the same order in both strings, so we are able to
use them all in an LCS. Note that the length of x is 5, the length of y is 6, and
the length of their LCS is 4. The edit distance is thus 5 + 6 − 2 × 4 = 3, which
agrees with the direct calculation in Example 3.15.
For another example, consider x = aba and y = bab. Their edit distance is
2. For example, we can convert x to y by deleting the first a and then inserting
b at the end. There are two LCS’s: ab and ba. Each can be obtained by
deleting one symbol from each string. As must be the case for multiple LCS’s
of the same pair of strings, both LCS’s have the same length. Therefore, we
may compute the edit distance as 3 + 3 − 2 × 2 = 2. ✷
Edit distance is a distance measure. Surely no edit distance can be negative,
and only two identical strings have an edit distance of 0. To see that edit
distance is symmetric, note that a sequence of insertions and deletions can be
reversed, with each insertion becoming a deletion, and vice versa. The triangle
inequality is also straightforward. One way to turn a string s into a string t
is to turn s into some string u and then turn u into t. Thus, the number of
edits made going from s to u, plus the number of edits made going from u to t
cannot be less than the smallest number of edits that will turn s into t.
3.5. DISTANCE MEASURES 101
Non-Euclidean Spaces
Notice that several of the distance measures introduced in this section are
not Euclidean spaces. A property of Euclidean spaces that we shall find
important when we take up clustering in Chapter 7 is that the average
of points in a Euclidean space always exists and is a point in the space.
However, consider the space of sets for which we defined the Jaccard distance. The notion of the “average” of two sets makes no sense. Likewise,
the space of strings, where we can use the edit distance, does not let us
take the “average” of strings.
Vector spaces, for which we suggested the cosine distance, may or may
not be Euclidean. If the components of the vectors can be any real numbers, then the space is Euclidean. However, if we restrict components to
be integers, then the space is not Euclidean. Notice that, for instance, we
cannot find an average of the vectors [1, 2] and [3, 1] in the space of vectors
with two integer components, although if we treated them as members of
the two-dimensional Euclidean space, then we could say that their average
was [2.0, 1.5].
3.5.6 Hamming Distance
Given a space of vectors, we define the Hamming distance between two vectors
to be the number of components in which they differ. It should be obvious
that Hamming distance is a distance measure. Clearly the Hamming distance
cannot be negative, and if it is zero, then the vectors are identical. The distance does not depend on which of two vectors we consider first. The triangle
inequality should also be evident. If x and z differ in m components, and z
and y differ in n components, then x and y cannot differ in more than m + n
components. Most commonly, Hamming distance is used when the vectors are
Boolean; they consist of 0’s and 1’s only. However, in principle, the vectors can
have components from any set.
Example 3.17 : The Hamming distance between the vectors 10101 and 11110
is 3. That is, these vectors differ in the second, fourth, and fifth components,
while they agree in the first and third components. ✷
3.5.7 Exercises for Section 3.5
! Exercise 3.5.1 : On the space of nonnegative integers, which of the following
functions are distance measures? If so, prove it; if not, prove that it fails to
satisfy one or more of the axioms.
(a) max(x, y) = the larger of x and y.
102 CHAPTER 3. FINDING SIMILAR ITEMS
(b) diff(x, y) = |x − y| (the absolute magnitude of the difference between x
and y).
(c) sum(x, y) = x + y.
Exercise 3.5.2 : Find the L1 and L2 distances between the points (5, 6, 7) and
(8, 2, 4).
!! Exercise 3.5.3 : Prove that if i and j are any positive integers, and i < j,
then the Li norm between any two points is greater than the Lj norm between
those same two points.
Exercise 3.5.4 : Find the Jaccard distances between the following pairs of
sets:
(a) {1, 2, 3, 4} and {2, 3, 4, 5}.
(b) {1, 2, 3} and {4, 5, 6}.
Exercise 3.5.5 : Compute the cosines of the angles between each of the following pairs of vectors.6
(a) (3, −1, 2) and (−2, 3, 1).
(b) (1, 2, 3) and (2, 4, 6).
(c) (5, 0, −4) and (−1, −6, 2).
(d) (0, 1, 1, 0, 1, 1) and (0, 0, 1, 0, 0, 0).
! Exercise 3.5.6 : Prove that the cosine distance between any two vectors of 0’s
and 1’s, of the same length, is at most 90 degrees.
Exercise 3.5.7 : Find the edit distances (using only insertions and deletions)
between the following pairs of strings.
(a) abcdef and bdaefc.
(b) abccdabc and acbdcab.
(c) abcdef and baedfc.
! Exercise 3.5.8 : There are a number of other notions of edit distance available.
For instance, we can allow, in addition to insertions and deletions, the following
operations:
6Note that what we are asking for is not precisely the cosine distance, but from the cosine
of an angle, you can compute the angle itself, perhaps with the aid of a table or library
function.
3.6. THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS 103
i. Mutation, where one symbol is replaced by another symbol. Note that a
mutation can always be performed by an insertion followed by a deletion,
but if we allow mutations, then this change counts for only 1, not 2, when
computing the edit distance.
ii. Transposition, where two adjacent symbols have their positions swapped.
Like a mutation, we can simulate a transposition by one insertion followed
by one deletion, but here we count only 1 for these two steps.
Repeat Exercise 3.5.7 if edit distance is defined to be the number of insertions,
deletions, mutations, and transpositions needed to transform one string into
another.
! Exercise 3.5.9 : Prove that the edit distance discussed in Exercise 3.5.8 is
indeed a distance measure.
Exercise 3.5.10 : Find the Hamming distances between each pair of the following vectors: 000000, 110011, 010101, and 011100.
3.6 The Theory of Locality-Sensitive Functions
The LSH technique developed in Section 3.4 is one example of a family of functions (the minhash functions) that can be combined (by the banding technique)
to distinguish strongly between pairs at a low distance from pairs at a high distance. The steepness of the S-curve in Fig. 3.8 reflects how effectively we can
avoid false positives and false negatives among the candidate pairs.
Now, we shall explore other families of functions, besides the minhash functions, that can serve to produce candidate pairs efficiently. These functions can
apply to the space of sets and the Jaccard distance, or to another space and/or
another distance measure. There are three conditions that we need for a family
of functions:
1. They must be more likely to make close pairs be candidate pairs than
distant pairs. We make this notion precise in Section 3.6.1.
2. They must be statistically independent, in the sense that it is possible to
estimate the probability that two or more functions will all give a certain
response by the product rule for independent events.
3. They must be efficient, in two ways:
(a) They must be able to identify candidate pairs in time much less
than the time it takes to look at all pairs. For example, minhash
functions have this capability, since we can hash sets to minhash
values in time proportional to the size of the data, rather than the
square of the number of sets in the data. Since sets with common
values are colocated in a bucket, we have implicitly produced the
104 CHAPTER 3. FINDING SIMILAR ITEMS
candidate pairs for a single minhash function in time much less than
the number of pairs of sets.
(b) They must be combinable to build functions that are better at avoiding false positives and negatives, and the combined functions must
also take time that is much less than the number of pairs. For example, the banding technique of Section 3.4.1 takes single minhash
functions, which satisfy condition 3a but do not, by themselves have
the S-curve behavior we want, and produces from a number of minhash functions a combined function that has the S-curve shape.
Our first step is to define “locality-sensitive functions” generally. We then
see how the idea can be applied in several applications. Finally, we discuss
how to apply the theory to arbitrary data with either a cosine distance or a
Euclidean distance measure.
3.6.1 Locality-Sensitive Functions
For the purposes of this section, we shall consider functions that take two items
and render a decision about whether these items should be a candidate pair.
In many cases, the function f will “hash” items, and the decision will be based
on whether or not the result is equal. Because it is convenient to use the
notation f(x) = f(y) to mean that f(x, y) is “yes; make x and y a candidate
pair,” we shall use f(x) = f(y) as a shorthand with this meaning. We also use
f(x) 6= f(y) to mean “do not make x and y a candidate pair unless some other
function concludes we should do so.”
A collection of functions of this form will be called a family of functions.
For example, the family of minhash functions, each based on one of the possible
permutations of rows of a characteristic matrix, form a family.
Let d1 < d2 be two distances according to some distance measure d. A
family F of functions is said to be (d1, d2, p1, p2)-sensitive if for every f in F:
1. If d(x, y) ≤ d1, then the probability that f(x) = f(y) is at least p1.
2. If d(x, y) ≥ d2, then the probability that f(x) = f(y) is at most p2.
Figure 3.10 illustrates what we expect about the probability that a given
function in a (d1, d2, p1, p2)-sensitive family will declare two items to be a candidate pair. Notice that we say nothing about what happens when the distance
between the items is strictly between d1 and d2, but we can make d1 and d2 as
close as we wish. The penalty is that typically p1 and p2 are then close as well.
As we shall see, it is possible to drive p1 and p2 apart while keeping d1 and d2
fixed.
3.6.2 Locality-Sensitive Families for Jaccard Distance
For the moment, we have only one way to find a family of locality-sensitive
functions: use the family of minhash functions, and assume that the distance
3.6. THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS 105
Probabilty
of being
declared a
candidate
d
p
d
p
1 2
1
2
Distance
Figure 3.10: Behavior of a (d1, d2, p1, p2)-sensitive function
measure is the Jaccard distance. As before, we interpret a minhash function h
to make x and y a candidate pair if and only if h(x) = h(y).
• The family of minhash functions is a (d1, d2, 1−d1, 1−d2)-sensitive family
for any d1 and d2, where 0 ≤ d1 < d2 ≤ 1.
The reason is that if d(x, y) ≤ d1, where d is the Jaccard distance, then
SIM(x, y) = 1 − d(x, y) ≥ 1 − d1. But we know that the Jaccard similarity
of x and y is equal to the probability that a minhash function will hash x and
y to the same value. A similar argument applies to d2 or any distance.
Example 3.18 : We could let d1 = 0.3 and d2 = 0.6. Then we can assert that
the family of minhash functions is a (0.3, 0.6, 0.7, 0.4)-sensitive family. That is,
if the Jaccard distance between x and y is at most 0.3 (i.e., SIM(x, y) ≥ 0.7)
then there is at least a 0.7 chance that a minhash function will send x and y to
the same value, and if the Jaccard distance between x and y is at least 0.6 (i.e.,
SIM(x, y) ≤ 0.4), then there is at most a 0.4 chance that x and y will be sent
to the same value. Note that we could make the same assertion with another
choice of d1 and d2; only d1 < d2 is required. ✷
3.6.3 Amplifying a Locality-Sensitive Family
Suppose we are given a (d1, d2, p1, p2)-sensitive family F. We can construct a
new family F
′
by the AND-construction on F, which is defined as follows. Each
member of F
′
consists of r members of F for some fixed r. If f is in F
′
, and f is
constructed from the set {f1, f2, . . . , fr} of members of F, we say f(x) = f(y)
if and only if fi(x) = fi(y) for all i = 1, 2, . . . , r. Notice that this construction
mirrors the effect of the r rows in a single band: the band makes x and y a
106 CHAPTER 3. FINDING SIMILAR ITEMS
candidate pair if every one of the r rows in the band say that x and y are equal
(and therefore a candidate pair according to that row).
Since the members of F are independently chosen to make a member of F
′
,
we can assert that F
′
is a
d1, d2,(p1)
r
,(p2)
r

-sensitive family. That is, for any
p, if p is the probability that a member of F will declare (x, y) to be a candidate
pair, then the probability that a member of F
′ will so declare is p
r
.
There is another construction, which we call the OR-construction, that turns
a (d1, d2, p1, p2)-sensitive family F into a
d1, d2, 1 − (1 − p1)
b
, 1 − (1 − p2)
b

-
sensitive family F
′
. Each member f of F
′
is constructed from b members of F,
say f1, f2, . . . , fb. We define f(x) = f(y) if and only if fi(x) = fi(y) for one or
more values of i. The OR-construction mirrors the effect of combining several
bands: x and y become a candidate pair if any band makes them a candidate
pair.
If p is the probability that a member of F will declare (x, y) to be a candidate
pair, then 1−p is the probability it will not so declare. (1−p)
b
is the probability
that none of f1, f2, . . . , fb will declare (x, y) a candidate pair, and 1 − (1 − p)
b
is the probability that at least one fi will declare (x, y) a candidate pair, and
therefore that f will declare (x, y) to be a candidate pair.
Notice that the AND-construction lowers all probabilities, but if we choose F
and r judiciously, we can make the small probability p2 get very close to 0, while
the higher probability p1 stays significantly away from 0. Similarly, the ORconstruction makes all probabilities rise, but by choosing F and b judiciously,
we can make the larger probability approach 1 while the smaller probability
remains bounded away from 1. We can cascade AND- and OR-constructions in
any order to make the low probability close to 0 and the high probability close
to 1. Of course the more constructions we use, and the higher the values of r
and b that we pick, the larger the number of functions from the original family
that we are forced to use. Thus, the better the final family of functions is, the
longer it takes to apply the functions from this family.
Example 3.19 : Suppose we start with a family F. We use the AND-construction with r = 4 to produce a family F1. We then apply the OR-construction
to F1 with b = 4 to produce a third family F2. Note that the members of F2
each are built from 16 members of F, and the situation is analogous to starting
with 16 minhash functions and treating them as four bands of four rows each.
The 4-way AND-function converts any probability p into p
4
. When we
follow it by the 4-way OR-construction, that probability is further converted
into 1−(1−p
4
)
4
. Some values of this transformation are indicated in Fig. 3.11.
This function is an S-curve, staying low for a while, then rising steeply (although
not too steeply; the slope never gets much higher than 2), and then leveling
off at high values. Like any S-curve, it has a fixedpoint, the value of p that is
left unchanged when we apply the function of the S-curve. In this case, the
fixedpoint is the value of p for which p = 1 − (1 − p
4
)
4
. We can see that the
fixedpoint is somewhere between 0.7 and 0.8. Below that value, probabilities are
decreased, and above it they are increased. Thus, if we pick a high probabili
3.6. THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS 107
p 1 − (1 − p
4
)
4
0.2 0.0064
0.3 0.0320
0.4 0.0985
0.5 0.2275
0.6 0.4260
0.7 0.6666
0.8 0.8785
0.9 0.9860
Figure 3.11: Effect of the 4-way AND-construction followed by the 4-way ORconstruction
above the fixedpoint and a low probability below it, we shall have the desired
effect that the low probability is decreased and the high probability is increased.
Suppose F is the minhash functions, regarded as a (0.2, 0.6, 0.8, 0.4)-sensitive family. Then F2, the family constructed by a 4-way AND followed by a
4-way OR, is a (0.2, 0.6, 0.8785, 0.0985)-sensitive family, as we can read from the
rows for 0.8 and 0.4 in Fig. 3.11. By replacing F by F2, we have reduced both
the false-negative and false-positive rates, at the cost of making application of
the functions take 16 times as long. ✷
p

1 − (1 − p)
4
4
0.1 0.0140
0.2 0.1215
0.3 0.3334
0.4 0.5740
0.5 0.7725
0.6 0.9015
0.7 0.9680
0.8 0.9936
Figure 3.12: Effect of the 4-way OR-construction followed by the 4-way ANDconstruction
Example 3.20 : For the same cost, we can apply a 4-way OR-construction
followed by a 4-way AND-construction. Figure 3.12 gives the transformation
on probabilities implied by this construction. For instance, suppose that F is a
(0.2, 0.6, 0.8, 0.4)-sensitive family. Then the constructed family is a
(0.2, 0.6, 0.9936, 0.5740)-sensitiv
108 CHAPTER 3. FINDING SIMILAR ITEMS
family. This choice is not necessarily the best. Although the higher probability
has moved much closer to 1, the lower probability has also raised, increasing
the number of false positives. ✷
Example 3.21 : We can cascade constructions as much as we like. For example, we could use the construction of Example 3.19 on the family of minhash
functions and then use the construction of Example 3.20 on the resulting family.
The constructed family would then have functions each built from 256 minhash
functions. It would, for instance transform a (0.2, 0.8, 0.8, 0.2)-sensitive family
into a (0.2, 0.8, 0.9991285, 0.0000004)-sensitive family. ✷
3.6.4 Exercises for Section 3.6
Exercise 3.6.1 : What is the effect on probability of starting with the family
of minhash functions and applying:
(a) A 2-way AND construction followed by a 3-way OR construction.
(b) A 3-way OR construction followed by a 2-way AND construction.
(c) A 2-way AND construction followed by a 2-way OR construction, followed
by a 2-way AND construction.
(d) A 2-way OR construction followed by a 2-way AND construction, followed
by a 2-way OR construction followed by a 2-way AND construction.
Exercise 3.6.2 : Find the fixedpoints for each of the functions constructed in
Exercise 3.6.1.
! Exercise 3.6.3 : Any function of probability p, such as that of Fig. 3.11, has
a slope given by the derivative of the function. The maximum slope is where
that derivative is a maximum. Find the value of p that gives a maximum slope
for the S-curves given by Fig. 3.11 and Fig. 3.12. What are the values of these
maximum slopes?
!! Exercise 3.6.4 : Generalize Exercise 3.6.3 to give, as a function of r and b, the
point of maximum slope and the value of that slope, for families of functions
defined from the minhash functions by:
(a) An r-way AND construction followed by a b-way OR construction.
(b) A b-way OR construction followed by an r-way AND construction.
3.7 LSH Families for Other Distance Measures
There is no guarantee that a distance measure has a locality-sensitive family of
hash functions. So far, we have only seen such families for the Jaccard distance.
In this section, we shall show how to construct locality-sensitive families for
Hamming distance, the cosine distance and for the normal Euclidean distance.
3.7. LSH FAMILIES FOR OTHER DISTANCE MEASURES 109
3.7.1 LSH Families for Hamming Distance
It is quite simple to build a locality-sensitive family of functions for the Hamming distance. Suppose we have a space of d-dimensional vectors, and h(x, y)
denotes the Hamming distance between vectors x and y. If we take any one
position of the vectors, say the ith position, we can define the function fi(x)
to be the ith bit of vector x. Then fi(x) = fi(y) if and only if vectors x and
y agree in the ith position. Then the probability that fi(x) = fi(y) for a randomly chosen i is exactly 1 − h(x, y)/d; i.e., it is the fraction of positions in
which x and y agree.
This situation is almost exactly like the one we encountered for minhashing.
Thus, the family F consisting of the functions {f1, f2, . . . , fd} is a
(d1, d2, 1 − d1/d, 1 − d2/d)-sensitive
family of hash functions, for any d1 < d2. There are only two differences
between this family and the family of minhash functions.
1. While Jaccard distance runs from 0 to 1, the Hamming distance on a
vector space of dimension d runs from 0 to d. It is therefore necessary to
scale the distances by dividing by d, to turn them into probabilities.
2. While there is essentially an unlimited supply of minhash functions, the
size of the family F for Hamming distance is only d.
The first point is of no consequence; it only requires that we divide by d at
appropriate times. The second point is more serious. If d is relatively small,
then we are limited in the number of functions that can be composed using
the AND and OR constructions, thereby limiting how steep we can make the
S-curve be.
3.7.2 Random Hyperplanes and the Cosine Distance
Recall from Section 3.5.4 that the cosine distance between two vectors is the
angle between the vectors. For instance, we see in Fig. 3.13 two vectors x
and y that make an angle θ between them. Note that these vectors may be
in a space of many dimensions, but they always define a plane, and the angle
between them is measured in this plane. Figure 3.13 is a “top-view” of the
plane containing x and y.
Suppose we pick a hyperplane through the origin. This hyperplane intersects
the plane of x and y in a line. Figure 3.13 suggests two possible hyperplanes,
one whose intersection is the dashed line and the other’s intersection is the
dotted line. To pick a random hyperplane, we actually pick the normal vector
to the hyperplane, say v. The hyperplane is then the set of points whose dot
product with v is 0.
First, consider a vector v that is normal to the hyperplane whose projection
is represented by the dashed line in Fig. 3.13; that is, x and y are on different
110 CHAPTER 3. FINDING SIMILAR ITEMS
θ
x
y
Figure 3.13: Two vectors make an angle θ
sides of the hyperplane. Then the dot products v.x and v.y will have different
signs. If we assume, for instance, that v is a vector whose projection onto the
plane of x and y is above the dashed line in Fig. 3.13, then v.x is positive,
while v.y is negative. The normal vector v instead might extend in the opposite
direction, below the dashed line. In that case v.x is negative and v.y is positive,
but the signs are still different.
On the other hand, the randomly chosen vector v could be normal to a
hyperplane like the dotted line in Fig. 3.13. In that case, both v.x and v.y
have the same sign. If the projection of v extends to the right, then both dot
products are positive, while if v extends to the left, then both are negative.
What is the probability that the randomly chosen vector is normal to a
hyperplane that looks like the dashed line rather than the dotted line? All
angles for the line that is the intersection of the random hyperplane and the
plane of x and y are equally likely. Thus, the hyperplane will look like the
dashed line with probability θ/180 and will look like the dotted line otherwise.
Thus, each hash function f in our locality-sensitive family F is built from
a randomly chosen vector vf . Given two vectors x and y, say f(x) = f(y) if
and only if the dot products vf .x and vf .y have the same sign. Then F is a
locality-sensitive family for the cosine distance. The parameters are essentially
the same as for the Jaccard-distance family described in Section 3.6.2, except
the scale of distances is 0–180 rather than 0–1. That is, F is a
(d1, d2,(180 − d1)/180,(180 − d2)/180)-sensitive
family of hash functions. From this basis, we can amplify the family as we wish,
just as for the minhash-based family.
3.7. LSH FAMILIES FOR OTHER DISTANCE MEASURES 111
3.7.3 Sketches
Instead of chosing a random vector from all possible vectors, it turns out to be
sufficiently random if we restrict our choice to vectors whose components are
+1 and −1. The dot product of any vector x with a vector v of +1’s and −1’s
is formed by adding the components of x where v is +1 and then subtracting
the other components of x – those where v is −1.
If we pick a collection of random vectors, say v1, v2, . . . , vn, then we can
apply them to an arbitrary vector x by computing v1.x, v2.x, . . . , vn.x and then
replacing any positive value by +1 and any negative value by −1. The result is
called the sketch of x. You can handle 0’s arbitrarily, e.g., by chosing a result +1
or −1 at random. Since there is only a tiny probability of a zero dot product,
the choice has essentially no effect.
Example 3.22 : Suppose our space consists of 4-dimensional vectors, and we
pick three random vectors: v1 = [+1, −1, +1, +1], v2 = [−1, +1, −1, +1], and
v3 = [+1, +1, −1, −1]. For the vector x = [3, 4, 5, 6], the sketch is [+1, +1, −1].
That is, v1.x = 3−4+5+6 = 10. Since the result is positive, the first component
of the sketch is +1. Similarly, v2.x = 2 and v3.x = −4, so the second component
of the sketch is +1 and the third component is −1.
Consider the vector y = [4, 3, 2, 1]. We can similarly compute its sketch to
be [+1, −1, +1]. Since the sketches for x and y agree in 1/3 of the positions,
we estimate that the angle between them is 120 degrees. That is, a randomly
chosen hyperplane is twice as likely to look like the dashed line in Fig. 3.13 than
like the dotted line.
The above conclusion turns out to be quite wrong. We can calculate the
cosine of the angle between x and y to be x.y, which is
6 × 1 + 5 × 2 + 4 × 3 + 3 × 4 = 40
divided by the magnitudes of the two vectors. These magnitudes are
p
6
2 + 52 + 42 + 32 = 9.274
and √
1
2 + 22 + 32 + 42 = 5.477. Thus, the cosine of the angle between x and
y is 0.7875, and this angle is about 38 degrees. However, if you look at all
16 different vectors v of length 4 that have +1 and −1 as components, you
find that there are only four of these whose dot products with x and y have
a different sign, namely v2, v3, and their complements [+1, −1, +1, −1] and
[−1, −1, +1, +1]. Thus, had we picked all sixteen of these vectors to form a
sketch, the estimate of the angle would have been 180/4 = 45 degrees. ✷
3.7.4 LSH Families for Euclidean Distance
Now, let us turn to the Euclidean distance (Section 3.5.2), and see if we can
develop a locality-sensitive family of hash functions for this distance. We shall
start with a 2-dimensional Euclidean space. Each hash function f in our family
112 CHAPTER 3. FINDING SIMILAR ITEMS
F will be associated with a randomly chosen line in this space. Pick a constant
a and divide the line into segments of length a, as suggested by Fig. 3.14, where
the “random” line has been oriented to be horizontal.
θ
Points at
distance
Bucket
width a
d
Figure 3.14: Two points at distance d ≫ a have a small chance of being hashed
to the same bucket
The segments of the line are the buckets into which function f hashes points.
A point is hashed to the bucket in which its projection onto the line lies. If the
distance d between two points is small compared with a, then there is a good
chance the two points hash to the same bucket, and thus the hash function f
will declare the two points equal. For example, if d = a/2, then there is at least
a 50% chance the two points will fall in the same bucket. In fact, if the angle
θ between the randomly chosen line and the line connecting the points is large,
then there is an even greater chance that the two points will fall in the same
bucket. For instance, if θ is 90 degrees, then the two points are certain to fall
in the same bucket.
However, suppose d is larger than a. In order for there to be any chance of
the two points falling in the same bucket, we need d cos θ ≤ a. The diagram of
Fig. 3.14 suggests why this requirement holds. Note that even if d cos θ ≪ a it
is still not certain that the two points will fall in the same bucket. However,
we can guarantee the following. If d ≥ 2a, then there is no more than a 1/3
chance the two points fall in the same bucket. The reason is that for cos θ to
be less than 1/2, we need to have θ in the range 60 to 90 degrees. If θ is in the
range 0 to 60 degrees, then cos θ is more than 1/2. But since θ is the smaller
angle between two randomly chosen lines in the plane, θ is twice as likely to be
between 0 and 60 as it is to be between 60 and 90.
We conclude that the family F just described forms a (a/2, 2a, 1/2, 1/3)-
sensitive family of hash functions. That is, for distances up to a/2 the probability is at least 1/2 that two points at that distance will fall in the same bucket,
while for distances at least 2a the probability points at that distance will fall in
3.7. LSH FAMILIES FOR OTHER DISTANCE MEASURES 113
the same bucket is at most 1/3. We can amplify this family as we like, just as
for the other examples of locality-sensitive hash functions we have discussed.
3.7.5 More LSH Families for Euclidean Spaces
There is something unsatisfying about the family of hash functions developed
in Section 3.7.4. First, the technique was only described for two-dimensional
Euclidean spaces. What happens if our data is points in a space with many
dimensions? Second, for Jaccard and cosine distances, we were able to develop
locality-sensitive families for any pair of distances d1 and d2 as long as d1 < d2.
In Section 3.7.4 we appear to need the stronger condition d1 < 4d2.
However, we claim that there is a locality-sensitive family of hash functions for any d1 < d2 and for any number of dimensions. The family’s hash
functions still derive from random lines through the space and a bucket size
a that partitions the line. We still hash points by projecting them onto the
line. Given that d1 < d2, we may not know what the probability p1 is that two
points at distance d1 hash to the same bucket, but we can be certain that it
is greater than p2, the probability that two points at distance d2 hash to the
same bucket. The reason is that this probability surely grows as the distance
shrinks. Thus, even if we cannot calculate p1 and p2 easily, we know that there
is a (d1, d2, p1, p2)-sensitive family of hash functions for any d1 < d2 and any
given number of dimensions.
Using the amplification techniques of Section 3.6.3, we can then adjust the
two probabilities to surround any particular value we like, and to be as far apart
as we like. Of course, the further apart we want the probabilities to be, the
larger the number of basic hash functions in F we must use.
3.7.6 Exercises for Section 3.7
Exercise 3.7.1 : Suppose we construct the basic family of six locality-sensitive
functions for vectors of length six. For each pair of the vectors 000000, 110011,
010101, and 011100, which of the six functions makes them candidates?
Exercise 3.7.2 : Let us compute sketches using the following four “random”
vectors:
v1 = [+1, +1, +1, −1] v2 = [+1, +1, −1, +1]
v3 = [+1, −1, +1, +1] v4 = [−1, +1, +1, +1]
Compute the sketches of the following vectors.
(a) [2, 3, 4, 5].
(b) [−2, 3, −4, 5].
(c) [2, −3, 4, −5].
114 CHAPTER 3. FINDING SIMILAR ITEMS
For each pair, what is the estimated angle between them, according to the
sketches? What are the true angles?
Exercise 3.7.3 : Suppose we form sketches by using all sixteen of the vectors
of length 4, whose components are each +1 or −1. Compute the sketches of
the three vectors in Exercise 3.7.2. How do the estimates of the angles between
each pair compare with the true angles?
Exercise 3.7.4 : Suppose we form sketches using the four vectors from Exercise 3.7.2.
! (a) What are the constraints on a, b, c, and d that will cause the sketch of
the vector [a, b, c, d] to be [+1, +1, +1, +1]?
!! (b) Consider two vectors [a, b, c, d] and [e, f, g, h]. What are the conditions on
a, b, . . . , h that will make the sketches of these two vectors be the same?
Exercise 3.7.5 : Suppose we have points in a 3-dimensional Euclidean space:
p1 = (1, 2, 3), p2 = (0, 2, 4), and p3 = (4, 3, 2). Consider the three hash functions
defined by the three axes (to make our calculations very easy). Let buckets be
of length a, with one bucket the interval [0, a) (i.e., the set of points x such that
0 ≤ x < a), the next [a, 2a), the previous one [−a, 0), and so on.
(a) For each of the three lines, assign each of the points to buckets, assuming
a = 1.
(b) Repeat part (a), assuming a = 2.
(c) What are the candidate pairs for the cases a = 1 and a = 2?
! (d) For each pair of points, for what values of a will that pair be a candidate
pair?
3.8 Applications of Locality-Sensitive Hashing
In this section, we shall explore three examples of how LSH is used in practice.
In each case, the techniques we have learned must be modified to meet certain
constraints of the problem. The three subjects we cover are:
1. Entity Resolution: This term refers to matching data records that refer to
the same real-world entity, e.g., the same person. The principal problem
addressed here is that the similarity of records does not match exactly
either the similar-sets or similar-vectors models of similarity on which the
theory is built.
2. Matching Fingerprints: It is possible to represent fingerprints as sets.
However, we shall explore a different family of locality-sensitive hash functions from the one we get by minhashing.
3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 115
3. Matching Newspaper Articles: Here, we consider a different notion of
shingling that focuses attention on the core article in an on-line newspaper’s Web page, ignoring all the extraneous material such as ads and
newspaper-specific material.
3.8.1 Entity Resolution
It is common to have several data sets available, and to know that they refer to
some of the same entities. For example, several different bibliographic sources
provide information about many of the same books or papers. In the general
case, we have records describing entities of some type, such as people or books.
The records may all have the same format, or they may have different formats,
with different kinds of information.
There are many reasons why information about an entity may vary, even if
the field in question is supposed to be the same. For example, names may be
expressed differently in different records because of misspellings, absence of a
middle initial, use of a nickname, and many other reasons. For example, “Bob
S. Jomes” and “Robert Jones Jr.” may or may not be the same person. If
records come from different sources, the fields may differ as well. One source’s
records may have an “age” field, while another does not. The second source
might have a “date of birth” field, or it may have no information at all about
when a person was born.
3.8.2 An Entity-Resolution Example
We shall examine a real example of how LSH was used to deal with an entityresolution problem. Company A was engaged by Company B to solicit customers for B. Company B would pay A a yearly fee, as long as the customer
maintained their subscription. They later quarreled and disagreed over how
many customers A had provided to B. Each had about 1,000,000 records, some
of which described the same people; those were the customers A had provided
to B. The records had different data fields, but unfortunately none of those
fields was “this is a customer that A had provided to B.” Thus, the problem
was to match records from the two sets to see if a pair represented the same
person.
Each record had fields for the name, address, and phone number of the
person. However, the values in these fields could differ for many reasons. Not
only were there the misspellings and other naming differences mentioned in
Section 3.8.1, but there were other opportunities to disagree as well. A customer
might give their home phone to A and their cell phone to B. Or they might
move, and tell B but not A (because they no longer had need for a relationship
with A). Area codes of phones sometimes change.
The strategy for identifying records involved scoring the differences in three
fields: name, address, and phone. To create a score describing the likelihood
that two records, one from A and the other from B, described the same per-
116 CHAPTER 3. FINDING SIMILAR ITEMS
son, 100 points was assigned to each of the three fields, so records with exact
matches in all three fields got a score of 300. However, there were deductions for
mismatches in each of the three fields. As a first approximation, edit-distance
(Section 3.5.5) was used, but the penalty grew quadratically with the distance.
Then, certain publicly available tables were used to reduce the penalty in appropriate situations. For example, “Bill” and “William” were treated as if they
differed in only one letter, even though their edit-distance is 5.
However, it is not feasible to score all one trillion pairs of records. Thus,
a simple LSH was used to focus on likely candidates. Three “hash functions”
were used. The first sent records to the same bucket only if they had identical
names; the second did the same but for identical addresses, and the third did
the same for phone numbers. In practice, there was no hashing; rather the
records were sorted by name, so records with identical names would appear
consecutively and get scored for overall similarity of the name, address, and
phone. Then the records were sorted by address, and those with the same
address were scored. Finally, the records were sorted a third time by phone,
and records with identical phones were scored.
This approach missed a record pair that truly represented the same person
but none of the three fields matched exactly. Since the goal was to prove in
a court of law that the persons were the same, it is unlikely that such a pair
would have been accepted by a judge as sufficiently similar anyway.
3.8.3 Validating Record Matches
What remains is to determine how high a score indicates that two records truly
represent the same individual. In the example at hand, there was an easy
way to make that decision, and the technique can be applied in many similar
situations. It was decided to look at the creation-dates for the records at hand,
and to assume that 90 days was an absolute maximum delay between the time
the service was bought at Company A and registered at B. Thus, a proposed
match between two records that were chosen at random, subject only to the
constraint that the date on the B-record was between 0 and 90 days after the
date on the A-record, would have an average delay of 45 days.
It was found that of the pairs with a perfect 300 score, the average delay was
10 days. If you assume that 300-score pairs are surely correct matches, then you
can look at the pool of pairs with any given score s, and compute the average
delay of those pairs. Suppose that the average delay is x, and the fraction of
true matches among those pairs with score s is f. Then x = 10f + 45(1 − f),
or x = 45 − 35f. Solving for f, we find that the fraction of the pairs with score
s that are truly matches is (45 − x)/35.
The same trick can be used whenever:
1. There is a scoring system used to evaluate the likelihood that two records
represent the same entity, and
3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 117
When Are Record Matches Good Enough?
While every case will be different, it may be of interest to know how the
experiment of Section 3.8.3 turned out on the data of Section 3.8.2. For
scores down to 185, the value of x was very close to 10; i.e., these scores
indicated that the likelihood of the records representing the same person
was essentially 1. Note that a score of 185 in this example represents a
situation where one field is the same (as would have to be the case, or the
records would never even be scored), one field was completely different,
and the third field had a small discrepancy. Moreover, for scores as low as
115, the value of x was noticeably less than 45, meaning that some of these
pairs did represent the same person. Note that a score of 115 represents
a case where one field is the same, but there is only a slight similarity in
the other two fields.
2. There is some field, not used in the scoring, from which we can derive a
measure that differs, on average, for true pairs and false pairs.
For instance, suppose there were a “height” field recorded by both companies
A and B in our running example. We can compute the average difference in
height for pairs of random records, and we can compute the average difference in
height for records that have a perfect score (and thus surely represent the same
entities). For a given score s, we can evaluate the average height difference of the
pairs with that score and estimate the probability of the records representing
the same entity. That is, if h0 is the average height difference for the perfect
matches, h1 is the average height difference for random pairs, and h is the
average height difference for pairs of score s, then the fraction of good pairs
with score s is (h1 − h)/(h1 − h0).
3.8.4 Matching Fingerprints
When fingerprints are matched by computer, the usual representation is not
an image, but a set of locations in which minutiae are located. A minutia,
in the context of fingerprint descriptions, is a place where something unusual
happens, such as two ridges merging or a ridge ending. If we place a grid over a
fingerprint, we can represent the fingerprint by the set of grid squares in which
minutiae are located.
Ideally, before overlaying the grid, fingerprints are normalized for size and
orientation, so that if we took two images of the same finger, we would find
minutiae lying in exactly the same grid squares. We shall not consider here
the best ways to normalize images. Let us assume that some combination of
techniques, including choice of grid size and placing a minutia in several adjacent
grid squares if it lies close to the border of the squares enables us to assume
118 CHAPTER 3. FINDING SIMILAR ITEMS
that grid squares from two images have a significantly higher probability of
agreeing in the presence or absence of a minutia than if they were from images
of different fingers.
Thus, fingerprints can be represented by sets of grid squares – those where
their minutiae are located – and compared like any sets, using the Jaccard similarity or distance. There are two versions of fingerprint comparison, however.
• The many-one problem is the one we typically expect. A fingerprint has
been found on a gun, and we want to compare it with all the fingerprints
in a large database, to see which one matches.
• The many-many version of the problem is to take the entire database, and
see if there are any pairs that represent the same individual.
While the many-many version matches the model that we have been following
for finding similar items, the same technology can be used to speed up the
many-one problem.
3.8.5 A LSH Family for Fingerprint Matching
We could minhash the sets that represent a fingerprint, and use the standard
LSH technique from Section 3.4. However, since the sets are chosen from a
relatively small set of grid points (perhaps 1000), the need to minhash them
into more succinct signatures is not clear. We shall study here another form of
locality-sensitive hashing that works well for data of the type we are discussing.
Suppose for an example that the probability of finding a minutia in a random
grid square of a random fingerprint is 20%. Also, assume that if two fingerprints
come from the same finger, and one has a minutia in a given grid square, then
the probability that the other does too is 80%. We can define a locality-sensitive
family of hash functions as follows. Each function f in this family F is defined
by three grid squares. Function f says “yes” for two fingerprints if both have
minutiae in all three grid squares, and otherwise f says “no.” Put another
way, we may imagine that f sends to a single bucket all fingerprints that have
minutiae in all three of f’s grid points, and sends each other fingerprint to a
bucket of its own. In what follows, we shall refer to the first of these buckets as
“the” bucket for f and ignore the buckets that are required to be singletons.
If we want to solve the many-one problem, we can use many functions from
the family F and precompute their buckets of fingerprints to which they answer
“yes.” Then, given a new fingerprint that we want to match, we determine
which of these buckets it belongs to and compare it with all the fingerprints
found in any of those buckets. To solve the many-many problem, we compute
the buckets for each of the functions and compare all fingerprints in each of the
buckets.
Let us consider how many functions we need to get a reasonable probability
of catching a match, without having to compare the fingerprint on the gun with
each of the millions of fingerprints in the database. First, the probability that
3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 119
two fingerprints from different fingers would be in the bucket for a function f
in F is (0.2)6 = 0.000064. The reason is that they will both go into the bucket
only if they each have a minutia in each of the three grid points associated with
f, and the probability of each of those six independent events is 0.2.
Now, consider the probability that two fingerprints from the same finger
wind up in the bucket for f. The probability that the first fingerprint has
minutiae in each of the three squares belonging to f is (0.2)3 = 0.008. However,
if it does, then the probability is (0.8)3 = 0.512 that the other fingerprint
will as well. Thus, if the fingerprints are from the same finger, there is a
0.008 × 0.512 = 0.004096 probability that they will both be in the bucket of f.
That is not much; it is about one in 200. However, if we use many functions
from F, but not too many, then we can get a good probability of matching
fingerprints from the same finger while not having too many false positives –
fingerprints that must be considered but do not match.
Example 3.23 : For a specific example, let us suppose that we use 1024
functions chosen randomly from F. Next, we shall construct a new family F1 by performing a 1024-way OR on F. Then the probability that F1
will put fingerprints from the same finger together in at least one bucket is
1 − (1 − 0.004096)1024 = 0.985. On the other hand, the probability that
two fingerprints from different fingers will be placed in the same bucket is
(1 − (1 − 0.000064)1024 = 0.063. That is, we get about 1.5% false negatives
and about 6.3% false positives. ✷
The result of Example 3.23 is not the best we can do. While it offers only a
1.5% chance that we shall fail to identify the fingerprint on the gun, it does force
us to look at 6.3% of the entire database. Increasing the number of functions
from F will increase the number of false positives, with only a small benefit
of reducing the number of false negatives below 1.5%. On the other hand, we
can also use the AND construction, and in so doing, we can greatly reduce
the probability of a false positive, while making only a small increase in the
false-negative rate. For instance, we could take 2048 functions from F in two
groups of 1024. Construct the buckets for each of the functions. However, given
a fingerprint P on the gun:
1. Find the buckets from the first group in which P belongs, and take the
union of these buckets.
2. Do the same for the second group.
3. Take the intersection of the two unions.
4. Compare P only with those fingerprints in the intersection.
Note that we still have to take unions and intersections of large sets of fingerprints, but we compare only a small fraction of those. It is the comparison of
120 CHAPTER 3. FINDING SIMILAR ITEMS
fingerprints that takes the bulk of the time; in steps (1) and (2) fingerprints
can be represented by their integer indices in the database.
If we use this scheme, the probability of detecting a matching fingerprint
is (0.985)2 = 0.970; that is, we get about 3% false negatives. However, the
probability of a false positive is (0.063)2 = 0.00397. That is, we only have to
examine about 1/250th of the database.
3.8.6 Similar News Articles
Our last case study concerns the problem of organizing a large repository of
on-line news articles by grouping together Web pages that were derived from
the same basic text. It is common for organizations like The Associated Press
to produce a news item and distribute it to many newspapers. Each newspaper
puts the story in its on-line edition, but surrounds it by information that is
special to that newspaper, such as the name and address of the newspaper,
links to related articles, and links to ads. In addition, it is common for the
newspaper to modify the article, perhaps by leaving off the last few paragraphs
or even deleting text from the middle. As a result, the same news article can
appear quite different at the Web sites of different newspapers.
The problem looks very much like the one that was suggested in Section 3.4:
find documents whose shingles have a high Jaccard similarity. Note that this
problem is different from the problem of finding news articles that tell about the
same events. The latter problem requires other techniques, typically examining
the set of important words in the documents (a concept we discussed briefly
in Section 1.3.1) and clustering them to group together different articles about
the same topic.
However, an interesting variation on the theme of shingling was found to be
more effective for data of the type described. The problem is that shingling as
we described it in Section 3.2 treats all parts of a document equally. However,
we wish to ignore parts of the document, such as ads or the headlines of other
articles to which the newspaper added a link, that are not part of the news
article. It turns out that there is a noticeable difference between text that
appears in prose and text that appears in ads or headlines. Prose has a much
greater frequency of stop words, the very frequent words such as “the” or “and.”
The total number of words that are considered stop words varies with the
application, but it is common to use a list of several hundred of the most
frequent words.
Example 3.24 : A typical ad might say simply “Buy Sudzo.” On the other
hand, a prose version of the same thought that might appear in an article is
“I recommend that you buy Sudzo for your laundry.” In the latter sentence, it
would be normal to treat “I,” “that,” “you,” “for,” and “your” as stop words.
✷
Suppose we define a shingle to be a stop word followed by the next two
words. Then the ad “Buy Sudzo” from Example 3.24 has no shingles and
3.8. APPLICATIONS OF LOCALITY-SENSITIVE HASHING 121
would not be reflected in the representation of the Web page containing that
ad. On the other hand, the sentence from Example 3.24 would be represented
by five shingles: “I recommend that,” “that you buy,” “you buy Sudzo,” “for
your laundry,” and “your laundry x,” where x is whatever word follows that
sentence.
Suppose we have two Web pages, each of which consists of half news text
and half ads or other material that has a low density of stop words. If the news
text is the same but the surrounding material is different, then we would expect
that a large fraction of the shingles of the two pages would be the same. They
might have a Jaccard similarity of 75%. However, if the surrounding material
is the same but the news content is different, then the number of common
shingles would be small, perhaps 25%. If we were to use the conventional
shingling, where shingles are (say) sequences of 10 consecutive characters, we
would expect the two documents to share half their shingles (i.e., a Jaccard
similarity of 1/3), regardless of whether it was the news or the surrounding
material that they shared.
3.8.7 Exercises for Section 3.8
Exercise 3.8.1 : Suppose we are trying to perform entity resolution among
bibliographic references, and we score pairs of references based on the similarities of their titles, list of authors, and place of publication. Suppose also that
all references include a year of publication, and this year is equally likely to be
any of the ten most recent years. Further, suppose that we discover that among
the pairs of references with a perfect score, there is an average difference in the
publication year of 0.1.7 Suppose that the pairs of references with a certain
score s are found to have an average difference in their publication dates of 2.
What is the fraction of pairs with score s that truly represent the same publication? Note: Do not make the mistake of assuming the average difference
in publication date between random pairs is 5 or 5.5. You need to calculate it
exactly, and you have enough information to do so.
Exercise 3.8.2 : Suppose we use the family F of functions described in Section 3.8.5, where there is a 20% chance of a minutia in an grid square, an 80%
chance of a second copy of a fingerprint having a minutia in a grid square where
the first copy does, and each function in F being formed from three grid squares.
In Example 3.23, we constructed family F1 by using the OR construction on
1024 members of F. Suppose we instead used family F2 that is a 2048-way OR
of members of F.
(a) Compute the rates of false positives and false negatives for F2.
(b) How do these rates compare with what we get if we organize the same
2048 functions into a 2-way AND of members of F1, as was discussed at
the end of Section 3.8.5?
7We might expect the average to be 0, but in practice, errors in publication year do occur.
122 CHAPTER 3. FINDING SIMILAR ITEMS
Exercise 3.8.3 : Suppose fingerprints have the same statistics outlined in Exercise 3.8.2, but we use a base family of functions F
′
defined like F, but using
only two randomly chosen grid squares. Construct another set of functions F
′
1
from F
′
by taking the n-way OR of functions from F
′
. What, as a function of
n, are the false positive and false negative rates for F
′
1
?
Exercise 3.8.4 : Suppose we use the functions F1 from Example 3.23, but we
want to solve the many-many problem.
(a) If two fingerprints are from the same finger, what is the probability that
they will not be compared (i.e., what is the false negative rate)?
(b) What fraction of the fingerprints from different fingers will be compared
(i.e., what is the false positive rate)?
! Exercise 3.8.5 : Assume we have the set of functions F as in Exercise 3.8.2,
and we construct a new set of functions F3 by an n-way OR of functions in
F. For what value of n is the sum of the false positive and false negative rates
minimized?
3.9 Methods for High Degrees of Similarity
LSH-based methods appear most effective when the degree of similarity we
accept is relatively low. When we want to find sets that are almost identical,
there are other methods that can be faster. Moreover, these methods are exact,
in that they find every pair of items with the desired degree of similarity. There
are no false negatives, as there can be with LSH.
3.9.1 Finding Identical Items
The extreme case is finding identical items, for example, Web pages that are
identical, character-for-character. It is straightforward to compare two documents and tell whether they are identical, but we still must avoid having to
compare every pair of documents. Our first thought would be to hash documents based on their first few characters, and compare only those documents
that fell into the same bucket. That scheme should work well, unless all the
documents begin with the same characters, such as an HTML header.
Our second thought would be to use a hash function that examines the
entire document. That would work, and if we use enough buckets, it would be
very rare that two documents went into the same bucket, yet were not identical.
The downside of this approach is that we must examine every character of every
document. If we limit our examination to a small number of characters, then
we never have to examine a document that is unique and falls into a bucket of
its own.
A better approach is to pick some fixed random positions for all documents,
and make the hash function depend only on these. This way, we can avoid
3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 123
a problem where there is a common prefix for all or most documents, yet we
need not examine entire documents unless they fall into a bucket with another
document. One problem with selecting fixed positions is that if some documents
are short, they may not have some of the selected positions. However, if we are
looking for highly similar documents, we never need to compare two documents
that differ significantly in their length. We exploit this idea in Section 3.9.3.
3.9.2 Representing Sets as Strings
Now, let us focus on the harder problem of finding, in a large collection of sets,
all pairs that have a high Jaccard similarity, say at least 0.9. We can represent
a set by sorting the elements of the universal set in some fixed order, and
representing any set by listing its elements in this order. The list is essentially
a string of “characters,” where the characters are the elements of the universal
set. These strings are unusual, however, in that:
1. No character appears more than once in a string, and
2. If two characters appear in two different strings, then they appear in the
same order in both strings.
Example 3.25 : Suppose the universal set consists of the 26 lower-case letters,
and we use the normal alphabetical order. Then the set {d, a, b} is represented
by the string abd. ✷
In what follows, we shall assume all strings represent sets in the manner just
described. Thus, we shall talk about the Jaccard similarity of strings, when
strictly speaking we mean the similarity of the sets that the strings represent.
Also, we shall talk of the length of a string, as a surrogate for the number of
elements in the set that the string represents.
Note that the documents discussed in Section 3.9.1 do not exactly match
this model, even though we can see documents as strings. To fit the model,
we would shingle the documents, assign an order to the shingles, and represent
each document by its list of shingles in the selected order.
3.9.3 Length-Based Filtering
The simplest way to exploit the string representation of Section 3.9.2 is to sort
the strings by length. Then, each string s is compared with those strings t that
follow s in the list, but are not too long. Suppose the lower bound on Jaccard
similarity between two strings is J. For any string x, denote its length by Lx.
Note that Ls ≤ Lt. The intersection of the sets represented by s and t cannot
have more than Ls members, while their union has at least Lt members. Thus,
the Jaccard similarity of s and t, which we denote SIM(s, t), is at most Ls/Lt.
That is, in order for s and t to require comparison, it must be that J ≤ Ls/Lt,
or equivalently, Lt ≤ Ls/J.
124 CHAPTER 3. FINDING SIMILAR ITEMS
A Better Ordering for Symbols
Instead of using the obvious order for elements of the universal set, e.g.,
lexicographic order for shingles, we can order symbols rarest first. That
is, determine how many times each element appears in the collection of
sets, and order them by this count, lowest first. The advantage of doing
so is that the symbols in prefixes will tend to be rare. Thus, they will
cause that string to be placed in index buckets that have relatively few
members. Then, when we need to examine a string for possible matches,
we shall find few other strings that are candidates for comparison.
Example 3.26 : Suppose that s is a string of length 9, and we are looking for
strings with at least 0.9 Jaccard similarity. Then we have only to compare s
with strings following it in the length-based sorted order that have length at
most 9/0.9 = 10. That is, we compare s with those strings of length 9 that
follow it in order, and all strings of length 10. We have no need to compare s
with any other string.
Suppose the length of s were 8 instead. Then s would be compared with
following strings of length up to 8/0.9 = 8.89. That is, a string of length 9
would be too long to have a Jaccard similarity of 0.9 with s, so we only have to
compare s with the strings that have length 8 but follow it in the sorted order.
✷
3.9.4 Prefix Indexing
In addition to length, there are several other features of strings that can be
exploited to limit the number of comparisons that must be made to identify
all pairs of similar strings. The simplest of these options is to create an index
for each symbol; recall a symbol of a string is any one of the elements of the
universal set. For each string s, we select a prefix of s consisting of the first p
symbols of s. How large p must be depends on Ls and J, the lower bound on
Jaccard similarity. We add string s to the index for each of its first p symbols.
In effect, the index for each symbol becomes a bucket of strings that must be
compared. We must be certain that any other string t such that SIM(s, t) ≥ J
will have at least one symbol in its prefix that also appears in the prefix of s.
Suppose not; rather SIM(s, t) ≥ J, but t has none of the first p symbols of
s. Then the highest Jaccard similarity that s and t can have occurs when t is
a suffix of s, consisting of everything but the first p symbols of s. The Jaccard
similarity of s and t would then be (Ls − p)/Ls. To be sure that we do not
have to compare s with t, we must be certain that J > (Ls − p)/Ls. That
is, p must be at least ⌊(1 − J)Ls⌋ + 1. Of course we want p to be as small as
possible, so we do not index string s in more buckets than we need to. Thus,
we shall hereafter take p = ⌊(1 − J)Ls⌋ + 1 to be the length of the prefix that
3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 125
gets indexed.
Example 3.27 : Suppose J = 0.9. If Ls = 9, then p = ⌊0.1 × 9⌋ + 1 =
⌊0.9⌋ + 1 = 1. That is, we need to index s under only its first symbol. Any
string t that does not have the first symbol of s in a position such that t is
indexed by that symbol will have Jaccard similarity with s that is less than 0.9.
Suppose s is bcdefghij. Then s is indexed under b only. Suppose t does not
begin with b. There are two cases to consider.
1. If t begins with a, and SIM(s, t) ≥ 0.9, then it can only be that t is
abcdefghij. But if that is the case, t will be indexed under both a and
b. The reason is that Lt = 10, so t will be indexed under the symbols of
its prefix of length ⌊0.1 × 10⌋ + 1 = 2.
2. If t begins with c or a later letter, then the maximum value of SIM(s, t)
occurs when t is cdefghij. But then SIM(s, t) = 8/9 < 0.9.
In general, with J = 0.9, strings of length up to 9 are indexed by their first
symbol, strings of lengths 10–19 are indexed under their first two symbols,
strings of length 20–29 are indexed under their first three symbols, and so on.
✷
We can use the indexing scheme in two ways, depending on whether we
are trying to solve the many-many problem or a many-one problem; recall the
distinction was introduced in Section 3.8.4. For the many-one problem, we
create the index for the entire database. To query for matches to a new set
S, we convert that set to a string s, which we call the probe string. Determine
the length of the prefix that must be considered, that is, ⌊(1 − J)Ls⌋ + 1. For
each symbol appearing in one of the prefix positions of s, we look in the index
bucket for that symbol, and we compare s with all the strings appearing in that
bucket.
If we want to solve the many-many problem, start with an empty database
of strings and indexes. For each set S, we treat S as a new set for the many-one
problem. We convert S to a string s, which we treat as a probe string in the
many-one problem. However, after we examine an index bucket, we also add s
to that bucket, so s will be compared with later strings that could be matches.
3.9.5 Using Position Information
Consider the strings s = acdefghijk and t = bcdefghijk, and assume J = 0.9.
Since both strings are of length 10, they are indexed under their first two
symbols. Thus, s is indexed under a and c, while t is indexed under b and c.
Whichever is added last will find the other in the bucket for c, and they will be
compared. However, since c is the second symbol of both, we know there will
be two symbols, a and b in this case, that are in the union of the two sets but
not in the intersection. Indeed, even though s and t are identical from c to the
126 CHAPTER 3. FINDING SIMILAR ITEMS
end, their intersection is 9 symbols and their union is 11; thus SIM(s, t) = 9/11,
which is less than 0.9.
If we build our index based not only on the symbol, but on the position of
the symbol within the string, we could avoid comparing s and t above. That
is, let our index have a bucket for each pair (x, i), containing the strings that
have symbol x in position i of their prefix. Given a string s, and assuming J is
the minimum desired Jaccard similarity, we look at the prefix of s, that is, the
positions 1 through ⌊(1 − J)Ls⌋ + 1. If the symbol in position i of the prefix is
x, add s to the index bucket for (x, i).
Now consider s as a probe string. With what buckets must it be compared?
We shall visit the symbols of the prefix of s from the left, and we shall take
advantage of the fact that we only need to find a possible matching string t if
none of the previous buckets we have examined for matches held t. That is, we
only need to find a candidate match once. Thus, if we find that the ith symbol
of s is x, then we need look in the bucket (x, j) for certain small values of j.
j
s
t
Symbols definitely
appearing in
only one string
i
Figure 3.15: Strings s and t begin with i − 1 and j − 1 unique symbols, respectively, and then agree beyond that
To compute the upper bound on j, suppose t is a string none of whose first
j −1 symbols matched anything in s, but the ith symbol of s is the same as the
jth symbol of t. The highest value of SIM(s, t) occurs if s and t are identical
beyond their ith and jth symbols, respectively, as suggested by Fig. 3.15. If
that is the case, the size of their intersection is Ls − i + 1, since that is the
number of symbols of s that could possibly be in t. The size of their union is
at least Ls + j − 1. That is, s surely contributes Ls symbols to the union, and
there are also at least j −1 symbols of t that are not in s. The ratio of the sizes
of the intersection and union must be at least J, so we must have:
Ls − i + 1
Ls + j − 1
≥ J
If we isolate j in this inequality, we have j ≤

Ls(1 − J) − i + 1 + J

/J.
Example 3.28 : Consider the string s = acdefghijk with J = 0.9 discussed
at the beginning of this section. Suppose s is now a probe string. We already
established that we need to consider the first two positions; that is, i can be
3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 127
or 2. Suppose i = 1. Then j ≤ (10 × 0.1 − 1 + 1 + 0.9)/0.9. That is, we only
have to compare the symbol a with strings in the bucket for (a, j) if j ≤ 2.11.
Thus, j can be 1 or 2, but nothing higher.
Now suppose i = 2. Then we require j ≤ (10 × 0.1 − 2 + 1 + 0.9)/0.9, Or
j ≤ 1. We conclude that we must look in the buckets for (a, 1), (a, 2), and (c, 1),
but in no other bucket. In comparison, using the buckets of Section 3.9.4, we
would look into the buckets for a and c, which is equivalent to looking to all
buckets (a, j) and (c, j) for any j. ✷
3.9.6 Using Position and Length in Indexes
When we considered the upper limit on j in the previous section, we assumed
that what follows positions i and j were as in Fig. 3.15, where what followed
these positions in strings s and t matched exactly. We do not want to build an
index that involves every symbol in the strings, because that makes the total
work excessive. However, we can add to our index a summary of what follows
the positions being indexed. Doing so expands the number of buckets, but not
beyond reasonable bounds, and yet enables us to eliminate many candidate
matches without comparing entire strings. The idea is to use index buckets
corresponding to a symbol, a position, and the suffix length, that is, the number
of symbols following the position in question.
Example 3.29 : The string s = acdefghijk, with J = 0.9, would be indexed
in the buckets for (a, 1, 9) and (c, 2, 8). That is, the first position of s has symbol
a, and its suffix is of length 9. The second position has symbol c and its suffix
is of length 8. ✷
Figure 3.15 assumes that the suffixes for position i of s and position j of t
have the same length. If not, then we can either get a smaller upper bound on
the size of the intersection of s and t (if t is shorter) or a larger lower bound
on the size of the union (if t is longer). Suppose s has suffix length p and t has
suffix length q.
Case 1: p ≥ q. Here, the maximum size of the intersection is
Ls − i + 1 − (p − q)
Since Ls = i + p, we can write the above expression for the intersection size as
q + 1. The minimum size of the union is Ls + j − 1, as it was when we did not
take suffix length into account. Thus, we require
q + 1
Ls + j − 1
≥ J
whenever p ≥ q.
128 CHAPTER 3. FINDING SIMILAR ITEMS
Case 2: p < q. Here, the maximum size of the intersection is Ls − i + 1, as
when suffix length was not considered. However, the minimum size of the union
is now Ls + j − 1 + q − p. If we again use the relationship Ls = i + p, we can
replace Ls − p by i and get the formula i + j − 1 + q for the size of the union.
If the Jaccard similarity is at least J, then
Ls − i + 1
i + j − 1 + q
≥ J
whenever p < q.
Example 3.30 : Let us again consider the string s = acdefghijk, but to make
the example show some details, let us choose J = 0.8 instead of 0.9. We know
that Ls = 10. Since ⌊(1 − J)Ls⌋ + 1 = 3, we must consider prefix positions
i = 1, 2, and 3 in what follows. As before, let p be the suffix length of s and q
the suffix length of t.
First, consider the case p ≥ q. The additional constraint we have on q and
j is (q + 1)/(9 + j) ≥ 0.8. We can enumerate the pairs of values of j and q for
each i between 1 and 3, as follows.
i = 1: Here, p = 9, so q ≤ 9. Let us consider the possible values of q:
q = 9: We must have 10/(9 + j) ≥ 0.8. Thus, we can have j = 1, j = 2,
or j = 3. Note that for j = 4, 10/13 > 0.8.
q = 8: We must have 9/(9 + j) ≥ 0.8. Thus, we can have j = 1 or j = 2.
For j = 3, 9/12 > 0.8.
q = 7: We must have 8/(9 + j) ≥ 0.8. Only j = 1 satisfies this inequality.
q = 6: There are no possible values of j, since 7/(9 + j) > 0.8 for every
positive integer j. The same holds for every smaller value of q.
i = 2: Here, p = 8, so we require q ≤ 8. Since the constraint (q+1)/(9+j) ≥ 0.8
does not depend on i,
8 we can use the analysis from the above case, but
exclude the case q = 9. Thus, the only possible values of j and q when
i = 2 are
1. q = 8; j = 1.
2. q = 8; j = 2.
3. q = 7; j = 1.
i = 3: Now, p = 7 and the constraints are q ≤ 7 and (q + 1)/(9 + j) ≥ 0.8. The
only option is q = 7 and j = 1.
Next, we must consider the case p < q. The additional constraint is
11 − i
i + j + q − 1
≥ 0.8
Again, consider each possible value of i.
8Note that i does influence the value of p, and through p, puts a limit on q.
3.9. METHODS FOR HIGH DEGREES OF SIMILARITY 129
i = 1: Then p = 9, so we require q ≥ 10 and 10/(q + j) ≥ 0.8. The possible
values of q and j are
1. q = 10; j = 1.
2. q = 10; j = 2.
3. q = 11; j = 1.
i = 2: Now, p = 8, so we require q ≥ 9 and 9/(q + j + 1) ≥ 0.8. Since j must
be a positive integer, the only solution is q = 9 and j = 1, a possibility
that we already knew about.
i = 3: Here, p = 7, so we require q ≥ 8 and 8/(q + j + 2) ≥ 0.8. There are no
solutions.
q j = 1 j = 2 j = 3
7 x
8 x x
i = 1 9 x x x
10 x x
11 x
7 x
i = 2 8 x x
9 x
i = 3 7 x
Figure 3.16: The buckets that must be examined to find possible matches for
the string s = acdefghijk with J = 0.8 are marked with an x
When we accumulate the possible combinations of i, j, and q, we see that
the set of index buckets in which we must look forms a pyramid. Figure 3.16
shows the buckets in which we must search. That is, we must look in those
buckets (x, j, q) such that the ith symbol of the string s is x, j is the position
associated with the bucket and q the suffix length. ✷
3.9.7 Exercises for Section 3.9
Exercise 3.9.1 : Suppose our universal set is the lower-case letters, and the
order of elements is taken to be the vowels, in alphabetic order, followed by the
consonants in reverse alphabetic order. Represent the following sets as strings.
a {q, w, e, r, t, y}.
(b) {a, s, d, f, g, h, j, u, i}.
130 CHAPTER 3. FINDING SIMILAR ITEMS
Exercise 3.9.2 : Suppose we filter candidate pairs based only on length, as in
Section 3.9.3. If s is a string of length 20, with what strings is s compared when
J, the lower bound on Jaccard similarity has the following values: (a) J = 0.85
(b) J = 0.95 (c) J = 0.98?
Exercise 3.9.3 : Suppose we have a string s of length 15, and we wish to index
its prefix as in Section 3.9.4.
(a) How many positions are in the prefix if J = 0.85?
(b) How many positions are in the prefix if J = 0.95?
! (c) For what range of values of J will s be indexed under its first four symbols,
but no more?
Exercise 3.9.4 : Suppose s is a string of length 12. With what symbol-position
pairs will s be compared with if we use the indexing approach of Section 3.9.5,
and (a) J = 0.75 (b) J = 0.95?
! Exercise 3.9.5 : Suppose we use position information in our index, as in Section 3.9.5. Strings s and t are both chosen at random from a universal set of
100 elements. Assume J = 0.9. What is the probability that s and t will be
compared if
(a) s and t are both of length 9.
(b) s and t are both of length 10.
Exercise 3.9.6 : Suppose we use indexes based on both position and suffix
length, as in Section 3.9.6. If s is a string of length 20, with what symbolposition-length triples will s be compared with, if (a) J = 0.8 (b) J = 0.9?
3.10 Summary of Chapter 3
✦ Jaccard Similarity: The Jaccard similarity of sets is the ratio of the size
of the intersection of the sets to the size of the union. This measure of
similarity is suitable for many applications, including textual similarity of
documents and similarity of buying habits of customers.
✦ Shingling: A k-shingle is any k characters that appear consecutively in
a document. If we represent a document by its set of k-shingles, then
the Jaccard similarity of the shingle sets measures the textual similarity
of documents. Sometimes, it is useful to hash shingles to bit strings of
shorter length, and use sets of hash values to represent documents.
✦ Minhashing: A minhash function on sets is based on a permutation of the
universal set. Given any such permutation, the minhash value for a set is
that element of the set that appears first in the permuted order.
3.10. SUMMARY OF CHAPTER 3 131
✦ Minhash Signatures: We may represent sets by picking some list of permutations and computing for each set its minhash signature, which is the
sequence of minhash values obtained by applying each permutation on the
list to that set. Given two sets, the expected fraction of the permutations
that will yield the same minhash value is exactly the Jaccard similarity
of the sets.
✦ Efficient Minhashing: Since it is not really possible to generate random
permutations, it is normal to simulate a permutation by picking a random
hash function and taking the minhash value for a set to be the least hash
value of any of the set’s members. An additional efficiency can be had
by restricting the search for the smallest minhash value to only a small
subset of the universal set.
✦ Locality-Sensitive Hashing for Signatures: This technique allows us to
avoid computing the similarity of every pair of sets or their minhash signatures. If we are given signatures for the sets, we may divide them into
bands, and only measure the similarity of a pair of sets if they are identical in at least one band. By choosing the size of bands appropriately, we
can eliminate from consideration most of the pairs that do not meet our
threshold of similarity.
✦ Distance Measures: A distance measure is a function on pairs of points in
a space that satisfy certain axioms. The distance between two points is 0 if
the points are the same, but greater than 0 if the points are different. The
distance is symmetric; it does not matter in which order we consider the
two points. A distance measure must satisfy the triangle inequality: the
distance between two points is never more than the sum of the distances
between those points and some third point.
✦ Euclidean Distance: The most common notion of distance is the Euclidean
distance in an n-dimensional space. This distance, sometimes called the
L2-norm, is the square root of the sum of the squares of the differences
between the points in each dimension. Another distance suitable for Euclidean spaces, called Manhattan distance or the L1-norm is the sum of
the magnitudes of the differences between the points in each dimension.
✦ Jaccard Distance: One minus the Jaccard similarity is a distance measure,
called the Jaccard distance.
✦ Cosine Distance: The angle between vectors in a vector space is the cosine
distance measure. We can compute the cosine of that angle by taking the
dot product of the vectors and dividing by the lengths of the vectors.
✦ Edit Distance: This distance measure applies to a space of strings, and
is the number of insertions and/or deletions needed to convert one string
into the other. The edit distance can also be computed as the sum of
132 CHAPTER 3. FINDING SIMILAR ITEMS
the lengths of the strings minus twice the length of the longest common
subsequence of the strings.
✦ Hamming Distance: This distance measure applies to a space of vectors.
The Hamming distance between two vectors is the number of positions in
which the vectors differ.
✦ Generalized Locality-Sensitive Hashing: We may start with any collection
of functions, such as the minhash functions, that can render a decision
as to whether or not a pair of items should be candidates for similarity
checking. The only constraint on these functions is that they provide a
lower bound on the probability of saying “yes” if the distance (according
to some distance measure) is below a given limit, and an upper bound on
the probability of saying “yes” if the distance is above another given limit.
We can then increase the probability of saying “yes” for nearby items and
at the same time decrease the probability of saying “yes” for distant items
to as great an extent as we wish, by applying an AND construction and
an OR construction.
✦ Random Hyperplanes and LSH for Cosine Distance: We can get a set of
basis functions to start a generalized LSH for the cosine distance measure
by identifying each function with a list of randomly chosen vectors. We
apply a function to a given vector v by taking the dot product of v with
each vector on the list. The result is a sketch consisting of the signs (+1 or
−1) of the dot products. The fraction of positions in which the sketches of
two vectors agree, multiplied by 180, is an estimate of the angle between
the two vectors.
✦ LSH For Euclidean Distance: A set of basis functions to start LSH for
Euclidean distance can be obtained by choosing random lines and projecting points onto those lines. Each line is broken into fixed-length intervals,
and the function answers “yes” to a pair of points that fall into the same
interval.
✦ High-Similarity Detection by String Comparison: An alternative approach
to finding similar items, when the threshold of Jaccard similarity is close to
1, avoids using minhashing and LSH. Rather, the universal set is ordered,
and sets are represented by strings, consisting their elements in order.
The simplest way to avoid comparing all pairs of sets or their strings is to
note that highly similar sets will have strings of approximately the same
length. If we sort the strings, we can compare each string with only a
small number of the immediately following strings.
✦ Character Indexes: If we represent sets by strings, and the similarity
threshold is close to 1, we can index all strings by their first few characters.
The prefix whose characters must be indexed is approximately the length
of the string times the maximum Jaccard distance (1 minus the minimum
Jaccard similarity).
3.11. REFERENCES FOR CHAPTER 3 133
✦ Position Indexes: We can index strings not only on the characters in
their prefixes, but on the position of that character within the prefix. We
reduce the number of pairs of strings that must be compared, because
if two strings share a character that is not in the first position in both
strings, then we know that either there are some preceding characters that
are in the union but not the intersection, or there is an earlier symbol that
appears in both strings.
✦ Suffix Indexes: We can also index strings based not only on the characters
in their prefixes and the positions of those characters, but on the length
of the character’s suffix – the number of positions that follow it in the
string. This structure further reduces the number of pairs that must be
compared, because a common symbol with different suffix lengths implies
additional characters that must be in the union but not in the intersection.
3.11 References for Chapter 3
The technique we called shingling is attributed to [11]. The use in the manner
we discussed here is from [2].
Minhashing comes from [3]. The improvement that avoids looking at all
elements is from [10].
The original works on locality-sensitive hashing were [9] and [7]. [1] is a
useful summary of ideas in this field.
[4] introduces the idea of using random-hyperplanes to summarize items in
a way that respects the cosine distance. [8] suggests that random hyperplanes
plus LSH can be more accurate at detecting similar documents than minhashing
plus LSH.
Techniques for summarizing points in a Euclidean space are covered in [6].
[12] presented the shingling technique based on stop words.
The length and prefix-based indexing schemes for high-similarity matching
comes from [5]. The technique involving suffix length is from [13].
1. A. Andoni and P. Indyk, “Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions,” Comm. ACM 51:1, pp. 117–
122, 2008.
2. A.Z. Broder, “On the resemblance and containment of documents,” Proc.
Compression and Complexity of Sequences, pp. 21–29, Positano Italy,
1997.
3. A.Z. Broder, M. Charikar, A.M. Frieze, and M. Mitzenmacher, “Min-wise
independent permutations,” ACM Symposium on Theory of Computing,
pp. 327–336, 1998.
4. M.S. Charikar, “Similarity estimation techniques from rounding algorithms,” ACM Symposium on Theory of Computing, pp. 380–388, 2002.
134 CHAPTER 3. FINDING SIMILAR ITEMS
5. S. Chaudhuri, V. Ganti, and R. Kaushik, “A primitive operator for similarity joins in data cleaning,” Proc. Intl. Conf. on Data Engineering,
2006.
6. M. Datar, N. Immorlica, P. Indyk, and V.S. Mirrokni, “Locality-sensitive
hashing scheme based on p-stable distributions,” Symposium on Computational Geometry pp. 253–262, 2004.
7. A. Gionis, P. Indyk, and R. Motwani, “Similarity search in high dimensions via hashing,” Proc. Intl. Conf. on Very Large Databases, pp. 518–
529, 1999.
8. M. Henzinger, “Finding near-duplicate web pages: a large-scale evaluation
of algorithms,” Proc. 29th SIGIR Conf., pp. 284–291, 2006.
9. P. Indyk and R. Motwani. “Approximate nearest neighbor: towards removing the curse of dimensionality,” ACM Symposium on Theory of Computing, pp. 604–613, 1998.
10. P. Li, A.B. Owen, and C.H. Zhang. “One permutation hashing,” Conf.
on Neural Information Processing Systems 2012, pp. 3122–3130.
11. U. Manber, “Finding similar files in a large file system,” Proc. USENIX
Conference, pp. 1–10, 1994.
12. M. Theobald, J. Siddharth, and A. Paepcke, “SpotSigs: robust and efficient near duplicate detection in large web collections,” 31st Annual ACM
SIGIR Conference, July, 2008, Singapore.
13. C. Xiao, W. Wang, X. Lin, and J.X. Yu, “Efficient similarity joins for
near duplicate detection,” Proc. WWW Conference, pp. 131-140, 2008.