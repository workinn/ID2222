1 Introduction
In 2001, in an attempt to characterize and visualize the changes that are likely
to emerge in the future, Douglas Laney [271] of META Group (Gartner now)
proposed three dimensions that characterize the challenges and opportunities of
increasingly large data: Volume, Velocity, and Variety, known as the 3 Vs of big
data. Thus, according to Gartner
“Big data” is high-volume, velocity, and variety information assets that
demand cost-effective, innovative forms of information processing for
enhanced insight and decision making.
According to Manyika et al. [297] this definition is intentionally subjective
and incorporates a moving definition of how big a dataset needs to be in order
to be considered big data. Along this lines, big data to Amazon or Google (see
Table 1) is quite different from big data to a medium-sized insurance or telecommunications organization. Hence, many different definitions have emerged over
time (see Chap. 3), but in general, it refers to “datasets whose size is beyond the
ability of typical database software tools to capture, store, manage, and analyze”
c The Author(s) 2020
V. Janev et al. (Eds.): Knowledge Graphs and Big Data Processing, LNCS 12072, pp. 3–19, 2020.
https://doi.org/10.1007/978-3-030-53199-7_1
4 V. Janev
[297] and technologies that address “data management challenges” and process
and analyze data to uncover valuable information that can benefit businesses
and organizations. Additional “Vs” of data have been added over the years, but
Volume, Velocity and Variety are the tree main dimensions that characterize the
data.
The volume dimension refers to the largeness of the data. The data size in
a big data ecosystem can range from dozens of terabytes to a few zettabytes
and is still growing [484]. In 2010, the McKinsey Global Institute estimated that
enterprises globally stored more than 7 exabytes of new data on disk drives,
while consumers stored more than 6 exabytes of new data on devices such as
PCs and notebooks. While more than 800,000 Petabytes (1 PB = 1015 bytes) of
data were stored in the year 2000, according to International Data Corporation
expectations [346] this volume will exceed 175 zettabytes (1 PB = 1021 bytes) by
2025 [85].
The velocity dimension refers to the increasing speed at which big data is created and the increasing speed at which the data need to be stored and analysed,
while the variety dimension refers to increased diversity of data types.
Variety introduces additional complexity to data processing as more kinds
of data need to be processed, combined and stored. While the 3 Vs have been
continuously used to describe big data, the additional dimensions of veracity
and value have been added to describe data integrity and quality, in what is
called the 5 Vs of big data. More Vs have been introduced, including validity,
vulnerability, volatility, and visualization, which sums up to the 10 Vs of big
data [138] (see Table 1). Regardless of how many descriptors are isolated when
describing the nature of big data, it is abundantly clear that the nature of big
data is highly complex and that it, as such, requires special technical solutions
for every step in the data workflow.
2 Big Data Ecosystem
The term Ecosystem is defined in scientific literature as a complex network or
interconnected systems (see Table 2). While in the past corporations used to deal
with static, centrally stored data collected from various sources, with the birth of
the web and cloud services, cloud computing is rapidly overtaking the traditional
in-house system as a reliable, scalable and cost-effective IT solution. Thus, large
datasets – log files, social media sentiments, click-streams – are no longer expected
to reside within a central server or within a fixed place in the cloud. To handle the
copious amounts of data, advanced analytical tools are needed which can process
and store billions of bytes of real-time data, with hundreds of thousands of transactions per second. Hence, the goal of this book is to introduce definitions, methods,
tools, frameworks and solutions for big data processing starting from the process
of information extraction, via knowledge processing and knowledge representation
to storing and visualization, sense-making, and practical applications.
Chapter 1 Ecosystem of Big Data 5
Table 1. Big data characteristics
3 Vs Volume Vast amount of data that has to be captured, stored, processed and
displayed
Velocity Rate at which the data is being generated, or analyzed
Variety Differences in data structure (format) or differences in data sources
themselves (text, images, voice, geospacial data)
5 Vs Veracity Truthfulness (uncertainty) of data, authenticity, provenance,
accountability
Validity Suitability of the selected dataset for a given application, accuracy
and correctness of the data for its intended use
7 Vs Volatility Temporal validity and fluency of the data, data currency and
availability, and ensures rapid retrieval of information as required
Value Usefulness and relevance of the extracted data in making decisions
and capacity in turning information into action
10 Vs Visualization Data representation and understandability of methods (data clustering
or using tree maps, sunbursts, parallel coordinates, circular network
diagrams, or cone trees)
Vulnerability Security and privacy concerns associated with data processing
Variability the changing meaning of data, inconsistencies in the data, biases,
ambiguities, and noise in data
3 Components of the Big Data Ecosystem
In order to depict the information processing flow in just a few phases, in Fig. 1,
from left to right, we have divided the processing workflow into three layers:
– Data sources;
– Data management (integration, storage and processing);
– Data analytics, Business intelligence (BI) and knowledge discovery (KD).
Table 2. Examples of big data ecosystems
Facebook Facebook (2018) has more than two billion users on millions of servers, running thousands
of configuration changes every day involving trillions of configuration checks [310]
LinkedIn It takes a lot of horsepower to support LinkedIn’s 467 million members worldwide (in 2017),
especially when you consider that each member is getting a personalized experience and a
web page that includes only their contacts. Supporting the load are some 100,000 servers
spread across multiple data centers [215]
Alibaba The 402,000 web-facing computers that Alibaba hosts (2017) from China-allocated IP
addresses would alone be sufficient to make Alibaba the second largest hosting company in
the world today [321]
Google There’s no official data on how many servers there are in Google’s data centers, but Gartner
estimated in a July 2016 report that Google at the time had 2.5 million servers. Google
data centers process an average of 40 million searches per second, resulting in 3.5 billion
searches per day and 1.2 trillion searches per year, Internet Live Stats reports [390]
Amazon ... an estimate of 87 AWS datacenters in total and a range of somewhere between 2.8 and 5.6
million servers in Amazon’s cloud (2014) [301]
Twitter Twitter (2013) now has 150M worldwide active users, handles 300K queries per second
(QPS) to generate timelines, and a firehose that churns out 22 MB/s. Some 400 million
tweets a day flow through the system and it can take up to 5 min for a tweet to flow from
Lady Gaga’s fingers to her 31 million followers [197]
Such partition will allow the authors of this book to discuss big data topics
from different perspectives. For computer scientists and engineers, big data poses
6 V. Janev
problems of data storage and management, communication, and computation.
For data scientists and statisticians responsible for machine learning models
development, the issues are how to get usable information out of datasets that are
too huge and complex for many traditional or classical methods to handle. From
an organizational viewpoint, business analysts are expected to select and deploy
analytics services and solutions that contribute mostly to the organizational
strategic goals, for instance, taking into consideration a framework for measuring
the organizational performance.
Data Sources. In a modern data ecosystem, the data sources layer is composed of both private and public data sources – see the left side of Fig. 2. The
corporate data originates from internal systems, cloud-based systems, as well
as external data provided from partners and third parties. Within a modern
data architecture, any type of data can be acquired and stored; however, the
most challenging task is to capture the heterogeneous datasets from various service providers. In order to allow developers to create new applications on top of
open datasets (see examples below), machine-readable formats are needed. As
such, XML and JSON have quickly become the de facto format for the web and
mobile applications due to their ease of integration into browser technologies and
server technologies that support Javascript. Once the data has been acquired, the
interlinking of diverse data sources is quite a complex and challenging process,
especially for the acquired unstructured data. That is the reason why semantic
technologies and Linked Data principles [51] have become popular over the last
decade [222]. Using Linked Data principles and a set of agreed vocabularies for
a domain, the input data is modeled in the form of resources, while the existing
relationships are modeled as a set of (named) relationships between resources.
In order to represent the knowledge of a specific domain, conceptual schemas
are applied (also called ontologies). Automatic procedures are used to map the
data to the target ontology, while standard languages are used to represent the
mappings (see Chap. 4). Furthermore, in order to unify the knowledge representation and data processing, standardized hierarchical and multilingual schemas
are used called taxonomies. Over the last decade, thousands of data repositories emerged on the web [48] that companies can use to improve their products and/or processes. The public data sources (statistics, trends, conversations,
images, videos, audios, and podcasts for instance from Google Trends, Twitter, Instagram, and others [299]) provide real-time information and on-demand
insights that enable businesses to analyse user interactions, draw patterns and
conclusions. IoT devices have also created significant challenges in many industries and enabled the development of new business models. However, one of the
main challenges associated with these repositories is automatically understanding the underlying structures and patterns of the data. Such an understanding
is a prerequisite to the application of advanced analytics to the retrieved data
[143]. Examples of Open Data Sources from different domains are:
Chapter 1 Ecosystem of Big Data 7
Fig. 1. From data to applications
– Facebook Graph API, curated by Facebook, is the primary way for apps to
read and write to the Facebook social graph. It is essentially a representation
of all information on Facebook now and in the past. For more info see here1.
– Open Corporates is one of the largest open databases of companies in the
world and holds hundreds of millions of datasets in essentially any country.
For more info, see here2.
– Global Financial Data’s API is recommended for analysts who require
large amounts of data for broad research needs. It enables researchers to study
the interaction between different data series, sectors, and genres of data. The
API supports R and Python so that the data can be directly uploaded to the
target application. For more info, see here3.
– Open Street Map is a map of the world, created by people free to use under
an open license. It powers map data on thousands of websites, mobile apps,
and hardware devices. For more info, see here4.
– The National Centers for Environmental Information (NCEI) is
responsible for hosting and providing access to one of the most significant
archives on Earth, with comprehensive oceanic, atmospheric, and geophysical data. For more info about the data access, see here5.
1 https://developers.facebook.com/docs/graph-api. 2 https://opencorporates.com/. 3 https://www.globalfinancialdata.com/. 4 https://www.openstreetmap.org/. 5 https://www.ncdc.noaa.gov/data-access.
8 V. Janev
– DBPedia is a semantic version of Wikipedia. It has helped companies like
Apple, Google, and IBM to support artificial intelligence projects. DBpedia is
in the center of the Linked Data cloud presented in Fig. 2, top-right quadrant6.
For more info, see here7.
Data Management. As data become increasingly available (from social media,
web logs, IoT sensors etc.), the challenge of managing (selecting, combining, storing) and analyzing large and growing data sets is growing more urgent. From a
data analytics point of view, that means that data processing has to be designed
taking into consideration the diversity and scalability requirements of targeted
data analytics applications. In modern settings, data acquisition via near realtime data streams in addition to batch loads is managed by different automated
processes (see Fig. 2, top-left quadrant presents an example of monitoring and
control of electric power facilities with the Supervisory, Control and Data Acquisition Systems8 developed by the Mihajlo Pupin Institute. The novel architecture [471] is ’flexible enough to support different service levels as well as optimal
algorithms and techniques for the different query workloads’ [426].
Over the last two decades, the emerging challenges in the design of end-toend data processing pipelines were addressed by computer scientists and software
providers in the following ways:
– In addition to operational database management systems (present on the
market since 1970s), different NoSQL stores appeared that lack adherence
to the time-honored SQL principles of ACID (atomicity, consistency, isolation,
durability), see Table 3.
– Cloud computing emerged as a paradigm that focuses on sharing data and
computations over a scalable network of nodes including end user computers,
data centers (see Fig. 2, bottom-left quadrant), and web services [23].
– The Data Lake concept as a new storage architecture was promoted where
raw data can be stored regardless of source, structure and (usually) size. The
data warehousing approach (based on a repository of structured, filtered data
that has already been processed for a specific purpose) is thus perceived as
outdated as it creates certain issues with respect to data integration and the
addition of new data sources.
The wide availability of big data also means that there are many quality
issues that need to be dealt with before using such data. For instance, data
inherently contains a lot of noise and uncertainty or is compromised because of
sensor malfunctioning or interferences, which may result in missing or conflicting
data. Therefore, quality assessment approaches and methods applicable in open
big data ecosystems have been developed [481].
6 www.lod-cloud.net. 7 https://wiki.dbpedia.org/. 8 http://www.pupin.rs/en/products-services/process-management/.
Chapter 1 Ecosystem of Big Data 9
Furthermore, in order to ensure interoperability between different processes
and interconnected systems, the semantic representation of data sources/processes
was introduced where a knowledge graph, from one side, meaningfully describes the
data pipeline, and from the other, is used to generate new knowledge (see Chap. 4).
Fig. 2. Components of big data ecosystem
Data Analytics. Data analytics refers to technologies that are grounded mostly
in data mining and statistical analysis [76]. The selection of an appropriate processing model and analytical solution is a challenging problem and depends on
the business issues of the targeted domain [221], for instance e-commerce [416],
market intelligence, e-government [220], healthcare, energy efficiency [47], emergency management [309], production management, and/or security (see Fig. 2,
bottom-right quadrant, example of Simulators and training aids developed by the
Mihajlo Pupin Institute). Depending on the class of problem that is being solved
(e.g. risk assessment in banks and the financial sector, predictive maintenance
of wind farms, sensing and cognition in production plants, automatic response
in control rooms, etc.), the data analytics solution also relies on text/web/network/mobile analytical services. Here various machine learning techniques such
as association rule mining, decision trees, regression, support vector machines,
and others are used.
While simple reporting and business intelligence applications that generate
aggregated measurements across different predefined dimensions based on the
data-warehousing concept were enough in 1990s, since 1995 the focus has been
on introducing parallelism into machine learning [435].
10 V. Janev
4 Using Semantics in Big Data Processing
Variety of Data Sources. In order to design and implement an adequate
big data processing architecture, as well as volume and velocity companies also
have to consider their ability to intercept the various available data sources. In
addition to the existing enterprise resource management systems, data produced
by a multitude of sources like sensors, smart devices and social media in raw,
semi-structured, unstructured and rich media formats further complicate the
processing and storage of data. Hence, different solutions for distributed storage, cloud computing, and data fusion are needed [286]. In order to make the
data useful for data analysis, companies use different methods to reduce complexity, downsize the data scale (e.g. dimensional reduction, sampling, coding)
and pre-process the data (data extraction, data cleaning, data integration, data
transformation) [456]. The heterogeneity of data can thus be characterized across
several dimensions:
– Structural variety refers to data representation; for instance, the satellite
images format is very different from the format used to store tweets generated
on the web;
– Media variety refers to the medium in which data gets delivered; for
instance, the audio of a speech versus the transcript of the speech may represent the same information in two different media;
– Semantic variety refers to the meaning of the units (terms) used to measure
or describe the data that are needed to interpret or operate on the data; for
instance, a ‘high’ salary from a service in Ethiopia is very different from a
‘high’ salary from a similar service in the United States;
– Availability variations mean that the data can be accessed continuously;
for instance, from traffic cameras, or intermediately, for instance, only when
the satellite is over the region of interest.
Semantic Variety and the Need for Standards. Attempts to explain the
uses of semantics in logic and computing date from the middle of the last century. In the information processing domain, semantics refers to the “meaning
and meaningful use of data” [472], i.e., the effective use of a data object for
representing a concept or object in the real world. Since 1980, the Artificial
Intelligence community has been promoting the idea of feeding intelligent systems and agents with general, formalized knowledge of the world (see also the
panel report from 1997 Data Semantics: what, where and how?) [398]. In 2001,
Sir Tim Berners-Lee, the Director of the Wide Web Consortium, outlined his
vision for the Semantic Web as an extension of the conventional Web and as a
world-wide distributed architecture where data and services easily interoperate.
Additionally, in 2006, Berners-Lee proposed the basic (Linked Data) principles
for interlinking linking datasets on the Web through references to common concepts [51]. The standard for the representation of the information that describes
the concepts is RDF (Resource Description Framework). In parallel, the wider
adoption of standards for representing and querying semantic information, such
Chapter 1 Ecosystem of Big Data 11
as RDF(s) and SPARQL, along with increased functionalities and improved
robustness of modern RDF stores, have established Linked Data and semantic technologies in the areas of data and knowledge management. As part of
the EC’Interoperability Solutions for European Public Administrations’ (ISA)9
program, with cooperation with W3C, core vocabularies have been adopted to
represent high-value datasets relevant for boosting innovative services.
Knowledge Engineering. Additionally, the scientific community has put a
great deal of effort into showcasing how knowledge engineering [26,92,221] can
take advantages from semantics-aware methods [222], which exploit knowledge
kept in (big) data to better reasoning on data beyond the possibilities offered by
more traditional data-instance-oriented approaches. With the announcement of
the Google Knowledge Graph in 2012, representations of general world knowledge as graphs have drawn a lot of attention again [347].
To summarize, semantics principles can be used in big data processing for
– Representing (schema and schema-less) data;
– Representing metadata (about documentation, provenance, trust, accuracy,
and other quality properties);
– Modeling data processes and flows, i.e., representing the entire pipeline making data representation shareable and verifiable.
The semantic representation of data in knowledge graphs (see Chap. 2), the
semantic processing pipeline (see Chap. 3, Chap. 5, Chap. 8), reasoning in knowledge graphs (Chap. 6) and the semantic analysis of big data (Chap. 7) are the
main topics of this book and will be explained in more detail in the subsequent
chapters.
5 Big Data, Standards and Interoperability
Interoperability remains a major burden for the developers of the big data ecosystem. In its EU 2030 vision, the European Union has set out the creation of an
internal single market through a standardised system of laws that apply in all
member states and a single European data [85] space – a genuine single market for data where businesses have easy access to an almost infinite amount of
high-quality industrial data. The vision is also supported by the EU Rolling
Plan for ICT Standardisation [86] that identifies 170 actions organised around
five priority domains—5G, cloud, cybersecurity, big data and Internet of Things.
In order to enable broad data integration, data exchange and interoperability
with the overall goal of fostering innovation based on data, standardisation at
different levels (such as metadata schemata, data representation formats and
licensing conditions of open data) is needed. This refers to all types of (multilingual) data, including both structured and unstructured data, and data from
9 https://ec.europa.eu/isa2/.
12 V. Janev
different domains as diverse as geospatial data, statistical data, weather data,
public sector information (PSI) and research data, to name just a few.
In the domain of big data, five different actions have been requested that also
involve the following standardization organizations:
– CEN, the European Committee for Standardization, to support and assist
the standardisation process and to coordinate with the relevant W3C groups
on preventing incompatible changes and on the conditions for availability of
the standard(s). The work will be in particular focused on the interoperability
needs of data portals in Europe while providing semantic interoperability with
other applications on the basis of reuse of established controlled vocabularies
(e.g. EuroVoc) and mappings to existing metadata vocabularies (e.g. SDMX,
INSPIRE metadata, Dublin Core, etc.);
– CENELEC (the European Committee for Electrotechnical Standardization)
in particular in relation to personal data management and the protection of
individuals’ fundamental rights;
– ETSI (the European Telecommunications Standards Institute) to coordinate
stakeholders and produce a detailed map of the necessary standards (e.g.
for security, interoperability, data portability and reversibility) and together
with CEN to work on various standardisation deliverables needed for the
completion of the rationalised framework of e-signatures standards;
– IEEE has a series of new standards projects related to big data (mobile
health, energy-efficient processing, personal agency and privacy) as well as
pre-standardisation activities on big data and open data;
– ISO/IEC JTC1, WG 9—Big Data, formed at the November 2014 in relation
to requirements, use cases, vocabulary and a reference architecture for big
data;
– OASIS, in relation to querying and sharing data across disparate applications
and multiple stakeholders for reuse in enterprise, cloud, and mobile devices.
Specification development in the OASIS OData TC builds on the core OData
Protocol V4 released in 2014 and addresses additional requirements identified
as extensions in four directional white papers: data aggregation, temporal
data, JSON documents, and XML documents as streams;
– OGC, the Open Geospatial Consortium defines and maintains standards for
location-based, spatio-temporal data and services. The work includes, for
instance, schema allowing descriptions of spatio-temporal sensors, images,
simulations, and statistics data (such as “datacubes”), a modular suite of
standards for Web services allowing ingestion, extraction, fusion, and (with
the web coverage processing service (WCPS) component standard) analytics
of massive spatio-temporal data like satellite and climate archives. OGC also
contributes to the INSPIRE project;
– W3C, the W3C Semantic Web Activity Group has accepted numerous Web
technologies as standards or recommendations for building semantic applications including RDF (Resource Description Framework) as a general-purpose
language; RDF Schema as a meta-language or vocabulary to define properties
Chapter 1 Ecosystem of Big Data 13
and classes of RDF resources; SPARQL as a standard language for querying
RDF data: OWL, Web Ontology Language for effective reasoning. More about
semantic standards can be found in [223].
Table 3. History of big data
Year Description
1911 Computing-Tabulating-Recording Company was founded and renamed “International Business
Machines” (IBM) in 1924
1929 The term “Super Computing” was first used in the New York World to refer to large
custom-built tabulators that IBM had made for Columbia University
1937 Social security was introduced in the United States of America and the requirement arose for
data management of 26 million residents
1945 John Von Neumann published a paper on the Electronic Discrete Variable Automatic Computer
(EDVAC), the first “documented” discussion on program storage, and laid the foundations of
computer architecture today
1957 A group of engineers established the Control Data Corporation (CDC) in Minneapolis,
Minnesota
1960 Seymour Cray (CDC) completed the CDC 1604, one of the first solid-state computers, and the
fastest computer in the world at a time when vacuum tubes were found in most large computers
1965 The first data center in the world was planned
1969 ARPANET set a message was sent from UCLA’s host computer to Stanford’s host computer
1970 Edgar Frank Codd invented the relational model for database management
1976 SAS Institute delivered the first version of the “Statistical Analysis System”
1977 Oracle Corporation was founded in Santa Clara, California, U.S
1998 Google was founded at the Stanford University in California
1999 Apache software foundation was established
1989 The invention of the World Wide Web at CERN
2003 Google File System was invented
2004 World Wide Web Consortium (W3C), the main international standards organization for the
Web was founded
2005 The start of development on Apache Hadoop which came into production in 2008
2007 The first publicly available dataset on DBpedia was published by the Free University of Berlin
and the Leipzig University
2009 Yahoo released Pig and Facebook created Hive
2011 Start of real-time processing as opposed to batch processing with Apache Storm and Spark
2012 Creation of Kafka by LinkedIn, Google introduced its Knowledge Graph project
2013 The definition of the Lambda architecture for efficient big data processing
2014 The definition of the Kappa architecture and the beginning of hybrid data processing
6 Big Data Analytics
6.1 The Evolution of Analytics
Over the last 50 years, Data Analytics has emerged as an important area of
study for both practitioners and researchers. The Analytics 1.0 era began in
the 1950s and lasted roughly 50 years. As a software approach, this field evolved
significantly with the invention of Relational Databases in the 1970s by Edgar
14 V. Janev
F. Codd, the development of artificial intelligence as a separate scientific discipline, and the invention of the Web by Sir Tim Berners-Lee in 1989. With the
development of Web 2.0-based social and crowd-sourcing systems in the 2000s,
the Analytics 2.0 era started. While the business solutions were tied to relational
and multi-dimensional database models in the Analytics 1.0 era, the Analytics
2.0 era brought NOSQL and big data database models that opened up new priorities and technical possibilities for analyzing large amounts of semi-structured
and unstructured data. Companies and data scientists refer to these two periods in time as before big data (BBD) and after big data (ABD) [100]. The
main limitations observed during the first era were that the potential capabilities of data were only utilised within organisations, i.e. the business intelligence
activities addressed only what had happened in the past and offered no predictions about its future trends. The new generation of tools with fast-processing
engines and NoSQL stores made possible the integration of internal data with
externally sourced data coming from the internet, sensors of various types, public data initiatives (such as the human genome project), and captures of audio
and video recordings. Also significantly developed in this period was the Data
Science field (multifocal field consisting of an intersection of Mathematics &
Statistics, Computer Science, and Domain Specific Knowledge), which delivered
scientific methods, exploratory processes, algorithms and tools that can be easily
leveraged to extract knowledge and insights from data in various forms.
The Analytics 3.0 era started [23] with the development of the “Internet of Things” and cloud computing, which created possibilities for establishing
hybrid technology environments for data storage, real-time analysis and intelligent customer-oriented services. Analytics 3.0 is also named the Era of Impact
or the Era of Data-enriched offerings after the endless opportunities for capitalizing on analytics services. For creating value in the data economy, Davenport
[100] suggests that the following factors need to be properly addressed:
– combining multiple types of data
– adoption of a new set of data management tools
– introduction of new “agile” analytical methods and machine-learning techniques to produce insights at a much faster rate
– embedding analytical and machine learning models into operational and decision processes
– requisite skills and processes to work with innovative discovery tools for data
exploration
– requisite skills and processes to develop prescriptive models that involve largescale testing and optimization and are a means of embedding analytics into
key processes
– leveraging new approaches to decision making and management.
Nowadays, being in the Analytics 4.0 era or the Era of Consumer-controled
data, the goal is to enable the customers to have full or partial control over data.
Also aligned with the Industry 4.0 movement, there are different possibilities for
automating and augmenting human/computer communications by combining
machine translation, smart reply, chat-bots, and virtual assistants.
Chapter 1 Ecosystem of Big Data 15
6.2 Different Types of Data Analytics
In general, analytic problems and techniques can be classified into
– Descriptive - What happened?
– Diagnostic - Why did it happen?
– Predictive - What is likely to happen?
– Prescriptive - What should be done about it?
– Cognitive - What don’t we know?
Descriptive analytics focus on analyzing historic data for the purpose of
identifying patterns (hindsights) or trends. While statistical theory and descriptive methodologies [7] are well documented in scientific literature, that is not
the case for other types of analytics, especially observing the big data and cloud
computing context.
Diagnostic analytics [364] discloses the root causes of a problem and gives
insight. The methods are treated as an extension to descriptive analytics that
provide an explanation to the question “Why did it happen?”.
Predictive analytics-based services apply forecasting and statistical modelling to give insight into “what is likely to happen” in the future (foresight)
based on supervised, unsupervised, and semi-supervised learning models.
Prescriptive analytics-based services [281] answers the question “What
should I do?”. In order to provide automated, time-dependent and optimal decisions based on the provided constraints and context, the software tools utilize
artificial intelligence, optimization algorithms and expert systems approaches.
Cognitive analytics is a term introduced recently in the context of cognitive
computing (see also Deloitte Tech Trends 2019 ). Motivated by the capability of
the human mind, and other factors such as changing technologies, smart devices,
sensors, and cloud computing capabilities, the goal is to develop “AI-based services that are able to interact with humans like a fellow human, interpret the
contextual meaning, analyze the past record of the user and draw deductions
based on that interactive session” [174,176].
7 Challenges for Exploiting the Potential of Big Data
In order to exploit the full potential, big data professionals and researchers have
to address different data and infrastructure management challenges that cannot
be resolved with traditional approaches [72]. Hence, in the last decade, different
techniques have emerged for acquisition, storing, processing and information
derivation in the big data value chains.
In [404], the authors introduced three main categories of challenges as follows:
– Data challenges related to the characteristics of the data itself (e.g. data volume, variety, velocity, veracity, volatility, quality, discovery and dogmatism);
– Process challenges related to techniques (how to capture data, how to
integrate data, how to transform data, how to select the right model for
analysis and how to provide the results);
16 V. Janev
– Management challenges related to organizational aspects such as privacy,
security, governance and ethical aspects.
Data, process and management challenges are interlinked and influence each
other.
7.1 Challenges
The 3 Vs of big data call for the integration of complex data sources (including complex types, complex structures, and complex patterns), as previously
discussed. Therefore, scalability is considered to be a crucial bottleneck of big
data solutions. Following the problem with processing, storage management is
another unavoidable barrier regarding big data. Storing the huge quantity of
data between its acquisition, processing and analysis requires gigantic memory
capacity, thus rendering traditional solutions obsolete.
The inherent complexity of big data (data complexity) makes its perception, representation, understanding and computation far more challenging and
results in sharp increases in the computational complexity required compared to
traditional computing models based on total data. The design of system architectures, computing frameworks, processing modes, and benchmarks for highly
energy-efficient big data processing platforms is the key issue to be addressed
in system complexity [231]. Contemporary cloud-based solutions are also considered to be on the edge of feasibility since responsiveness can be a critical issue,
especially in real-time applications, where upload speeds are considered the main
bottleneck.
When simultaneously working with different data sources, the reliability of
collected data will inevitably fluctuate with missed, partial and faulty measurements being unavoidable, resulting in serious potential trouble later on in the
workflow, such as in the analytics stage. Hence, high-quality data management
(i.e. data cleaning, filtering, transforming and other) actions are mandatory at
the beginning of the process. Besides reliability, the correctness of the data is
considered to be a key aspect of big data processing. High volumes, unstructured
forms, the distributed nature of data in NoSQL data management systems and
the necessity of near-to-real-time responses often lead to corrupted results with
no method being able to guarantee their complete validity.
Other quality dimensions, that impact the design of a big data solution are
completeness, consistency, credibility, timeliness and others.
For instance, in real-time applications (e.g. stock market, financial fraud
detection and transactions parsing, traffic management, energy optimization
etc.), quick responses are required and expected immediately because the
retrieved information can be completely useless if it is derived with high latency
with respect to the collected data.
An additional challenge from the human-computer perspective is the visualization of results. Although various ways in which the data can be displayed
do not affect the data processing segment in any way, visualization is stated in
Chapter 1 Ecosystem of Big Data 17
the literature as a crucial factor because without adequate representation of the
results, the derived knowledge is useless.
Depending on the type of data being processed, security can sometimes be a
crucial component that requires special attention. When considering, for example, a weather forecast or public transport management use case, if a data loss or
theft occurs, it can be considered practically irrelevant compared to a situation
where personal information, names, addresses, location history, social security
information or credit card PIN codes are stolen because in the latter case, data
protection must be upheld at the highest possible standard.
7.2 Example: Analysis of Challenges and Solutions for Traffic
Management
Smart transportation is one of the key big data vertical applications, and refers to
the integrated application of modern technologies and management strategies in
transportation systems. Big data platforms available on the market contribute
to a great extent to smart management of cities and the implementation of
intelligent transportation systems. In order to showcase the usage of different
type of data analytics and to strengthen the discussion on challenges, we will
point to the traffic management system used for monitoring highways in Serbia
[366]. Highways and motorways control systems generate a high volume of data
that is relevant for a number of stakeholder’s from traffic and environmental
departments to transport providers, citizens and the police. The Fig. 3 below
points to (a) the European corridors, and (b) the Corridor 10 that is managed in
Serbia by the public enterprise “Roads of Serbia” using a control system provided
by Institute Mihajlo Pupin. Its holistic supervisory function and control includes
(a) toll collection and motorway and highway traffic control, and (b) urban traffic
control and management. The main challenges on EU level are related to:
Fig. 3. Traffic management
18 V. Janev
– Interoperability of tolling services on the entire European Union road network because the ones introduced at local and national levels from the early
1990s onwards are still generally non-interoperable;
– Smart mobility and the need of users to be more informed about different
options in real-time;
– the need for efficient and effective approaches for assessment and management of air pollution due to improved ambient air quality.
The main components of the traffic control system are:
– The toll collection system10, which is hierarchically structured; it is fully
modular, based on PC technology and up-to date real time operation systems,
relational data base system and dedicated encryption of data transmission.
Toll line controllers are based on industrial PC-technology and dedicated electronic interface boards. The toll plaza subsystem is the supervisory system
for all line controllers. It collects all the data from lane controllers including
financial transactions, digital images of vehicles, technical malfunctions, line
operators’ actions and failures. All data concerning toll collection processes
and equipment status are permanently collected from the plaza computers
and stored in a central system database. The toll collection system also comprises features concerning vehicle detection and classification, license plate
recognition and microwave-based dedicated short-range communications.
– The Main Control Centre is connected through an optical communication
link with the Plaza Control Centres. Also, the Control Centre is constantly
exchanging data with various institutions such as: banks, insurance companies, institutions that handle credit and debit cards, RF tags vendors, etc.
through a computer network. Data analytics is based on data warehouse
architecture enabling optimal performances in near real time for statistical
and historical analysis of large data volumes. Reporting is based on optimized
data structures, allowing both predefined (standardized) reports as well as
ad hoc (dynamic) reports, which are generated efficiently using the Oracle BI
platform. Data analytics includes scenarios, such as
• Predicting and preventing road traffic congestion analytics is used
to improve congestion diagnosis and to enable traffic managers to proactively manage traffic and to organize the activities at toll collection stations before congestion is reached.
• Strategic environmental impact assessment analytics is used to
study the environmental impact and the effect of highways on adjacent
flora, fauna, air, soil, water, humans, landscape, cultural heritage, etc.
based on historical and real-time analysis. Passive pollution monitoring
involves collecting data about the diffusion of air pollutants, e.g. emission estimates based on traffic counting. Passive pollution monitoring has
been used to determine trends in long-term pollution levels. Road traffic
pollution monitoring and visualization requires the integration of high
volumes of (historical) traffic data with other parameters such as vehicle
10 http://www.pupin.rs/en/products-services/traffic-control-systems/pay-toll/.
Chapter 1 Ecosystem of Big Data 19
emission factors, background pollution data, meteorology data, and road
topography.
Here, we have pointed to just one mode of transport and traffic management,
i.e. the control of highways and motorways. However, nowadays, an increasing
number of cities around the world struggle with traffic congestion, optimizing
public transport, planning parking spaces, and planning cycling routes. These
issues call for new approaches for studying human mobility by exploiting machine
learning techniques [406], forecasting models or through the application of complex event processing tools [135].
8 Conclusions
This chapter presents the author’s vision of a Big data ecosystem. It serves as an
introductory chapter to point to a number of aspects that are relevant for this
book. Over the last two decades, advances in hardware and software technologies, such as the Internet of Things, mobile technologies, data storage and cloud
computing, and parallel machine learning algorithms have resulted in the ability
to easily acquire, analyze and store large amounts of data from different kinds
of quantitative and qualitative domain-specific data sources. The monitored and
collected data presents opportunities and challenges that, as well as focusing on
the three main characteristics of volume, variety, and velocity, require research
of other characteristics such as validity, value and vulnerability. In order to automate and speed up the processing, interoperable data infrastructure is needed
and standardization of data-related technologies, including developing metadata
standards for big data management. One approach to achieve interoperability
among datasets and services is to adopt data vocabularies and standards as
defined in the W3C Data on the Web Best Practices, which are also applied in
the tools presented in this book (see Chaps. 4, 5, 6, 7, 8 and 9).
In order to elaborate the challenges and point to the potential of big data,
a case study from the traffic sector is presented and discussed in this chapter,
while more big data case studies are set out in Chap. 9 and Chap. 10.
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.
Chapter 2
Knowledge Graphs: The Layered
Perspective
Luigi Bellomarini1, Emanuel Sallinger2,3(B) , and Sahar Vahdati3
1 Banca d’Italia, Rome, Italy 2 TU Wien, Vienna, Austria
sallinger@dbai.tuwien.ac.at 3 University of Oxford, Oxford, UK
Abstract. Knowledge Graphs (KGs) are one of the key trends among
the next wave of technologies. Many definitions exist of what a Knowledge Graph is, and in this chapter, we are going to take the position
that precisely in the multitude of definitions lies one of the strengths of
the area. We will choose a particular perspective, which we will call the
layered perspective, and three views on Knowledge Graphs.
1 Introduction
Knowledge Graphs (KGs) are one of the key trends among the next wave of
technologies [340]. Despite the highlighted role in practice as well as research, and
the variety of definitions of the notion, there is still no common understanding
of what a Knowledge Graph is. In this introduction, we are not going to choose
one definition of Knowledge Graphs. Many great introductions exist to particular
definitions, and we will refer to some of them in this chapter. Instead, we are
going to take the position that precisely in the multitude of definitions lies one
of the strengths of the area.
At the same time, our aim is not towards a fully exhaustive, historical account
of the evolution of Knowledge Graphs both regarding the term and the concept.
Again, excellent historical and exhaustive accounts already exist, and we will
refer to some of them in this chapter. Instead, we will choose a particular perspective, which we will call the layered perspective, and three views on Knowledge
Graphs.
Views on Knowledge Graphs. While many ways of classifying types of
Knowledge Graphs used in literature are possible, here we concentrate on the
following three views:
– knowledge representation tools: where the focus is on how a Knowledge
Graph is used to represent some form of knowledge.
– knowledge management systems: where the focus is the system managing
the Knowledge Graph, similar to how database management systems play this
role for databases.
c The Author(s) 2020
V. Janev et al. (Eds.): Knowledge Graphs and Big Data Processing, LNCS 12072, pp. 20–34, 2020.
https://doi.org/10.1007/978-3-030-53199-7_2
Chapter 2 Knowledge Graphs: The Layered Perspective 21
– knowledge application services: where the focus is on providing a layer
of applications on top of a Knowledge Graph.
Representation Tool
Management System
Application Services
Interacts with
Applications Services
Interacts with
Data Manager
Interacts with
Knowledge Engineer
Fig. 1. Ordered pyramids of views on KGs.
The Layered Perspective. While these three views certainly have independent
value, they are most interesting when put together as layers: on the first layer
is the representation of knowledge, on the middle layer is the management
system for this knowledge, and on the top layer the application that it solves.
This is illustrated in Fig. 1. There are three additional factors at play here:
– There are generally two ways of looking at the order of these layers. Some
communities tend to see it top-down with the application that the KG solves
as the focus, others tend to see it as bottom-up, with the representation of
knowledge as the focus. Interestingly, there is even another one, as the data
management community often sees the management system in the middle as
the focus.
– The borders between these layers are fuzzy. Many academic and industrial
systems cover two or three of these layers. In some cases, representation tools
partly fulfill some of the characteristics of management systems. The same
applies for application platforms.
– The central aspect of reasoning poses vastly different requirements to the
three layers. Chapter 6 will be fully dedicated to this aspect.
Of course, it is clear that to achieve a great overall system, all layers and their
interactions have to be taken into account; it is hardly possibly to provide a good
knowledge application platform if the knowledge representation layer is not fit
for the purpose.
Organization. The first three sections cover the three views we introduce above.
In Sect. 2, we consider the view of KGs as knowledge representations tools; in
Sect. 3, we consider the view of KGs a knowledge management systems; and in
Sect. 4, we consider the view of KGs as knowledge application platforms. We will
conclude with a section on challenges and opportunities.
22 L. Bellomarini et al.
2 KGs as Knowledge Representation Tools
One of the most common views on Knowledge Graphs, which covers most of the
given definitions, is to primarily view them as knowledge representation tools.
In this section, we will give an overview of some of the notions with a particular
focus on how they fit into the layered view.
Common to all these definitions is that, somewhat unsurprisingly given the
term Knowledge Graph, there is some form of graph encoded by the formalism,
and there is some form of knowledge encoded in it. Yet, in terms of graphs,
what they widely differ is in whether a simple graph is the primary structure or
whether we are actually dealing with richer settings where e.g., the graph has
attributes associated to nodes or edges of the graph, or whether we are actually
dealing with a hyper-graph (similar to full relational structures). Similarly, in
terms of knowledge, what they widely differ is whether the graph is the knowledge, or the knowledge actually generates the entirety or parts of the graph. In
some of the particular communities of computer science, Knowledge Graphs are
explicitly considered as collections of facts about entities, typically derived from
structured data sources such as Babelnet, OpenCyc, DBpedia, Yago, Wikidata,
NELL and their shared features FreeBase [377]. In this way, a collection of facts
represented in different languages but in the same structure is called a KG.
Critically though, forming a bridge to what we discussed in the introduction,
in many cases these differences are only at the surface, and are often a question
of representation, rather than fundamental. For example, it is clear that an
arbitrary relational structure – or, in fact, an arbitrary data structure – can
be encoded as a graph, and vice versa. Similarly, it is in many cases not a
fundamental difference whether technically knowledge is encoded into the graph,
into a separate knowledge representation language, or provided via other AI
and ML frameworks. Still, fundamental differences do remain between different
notions of Knowledge Graphs, and as we mentioned in the beginning, it is our
position that these multifaceted definitions are one of the strengths of the field.
In this section, we will explore such different definitions of Knowledge Graphs,
highlighting both their commonalities and differences.
Views on KGs as Representation Tools for Data. The following definitions
are pointing to the data structure in the representation. They mostly take a
graph representation as a baseline and provide different explanations of how the
graph structure helps with mapping real world information.
A Mathematical Structure. This is often considered to be the first recorded
appearance [399] of the term “knowledge graph” – though not necessarily the
concept of “knowledge graph”. Here, capturing knowledge from the real world as
a teaching-leaning process is considered a way of building a graph of knowledge.
In this work, prerequisites of learning are a necessary set of knowledge units
that should usually be taught to the learner (human or machine) before. In this
paper, a knowledge graph is essentially defined as:
Chapter 2 Knowledge Graphs: The Layered Perspective 23
A mathematical structure with vertices as knowledge units connected
by edges that represent the prerequisite relation. – Marchi and Miquel,
1974 [298]
Although this definition has been given in the context of interactive learning
between students and teachers, the concept can very well be adjusted for current machine learning and machine teaching [488] approaches where Knowledge
Graphs are considered as the base of intelligence. In this definition, the degree of
abstraction is hidden in the mathematical representation of knowledge in nodes
as knowledge units and edges as connectors. Obviously, a specific language or
data structure is not discussed due to its different context – so in our layer of
knowledge representation tools, it is certainly a very abstract form of representation. It is roughly mentioned that knowledge units of a course for students
to learn are represented as nodes of a graph in a game-theoretic way. And the
links between the modes connect the knowledge units where the students can
follow learning paths. In this way, the idea of representing common knowledge
in a graph-based structure works in a similar way between this definition and
today’s KGs. Similar to this view is also represented quite at the same time [387]
where the teacher or the student can be replaced by a computer. It is argued that
the directed graph in which the knowledge is represented in nodes and labeled
links can influence the learning process for data analysis purposes.
A Set of Justified True Beliefs. In a tutorial by Microsoft, Yuqing Gao [146]
follows Plato’s tripartite definition of knowledge as a subset of “Justified true
beliefs” such that knowledge contains a truth condition, a belief condition and
an inference of the former two that leads to justification of that. As example of
such a “Justified true belief” is: A is True. B knows A. B is justified in knowing
A. Knowledge in KGs is represented as triples of (Subject, Predicate, Object),
where Subject and Object are pointing to entities and Predicate represents the
relation. A graph constructed from such triples contains nodes and edges where
the nodes are pointing to entities as subject and object and the edges are for
relations as predicates. There is extra information such as the metadata of each
entity, which are shown as attributes. Following this, a set of key concepts for
Knowledge Graphs as knowledge representation tools are introduced as:
– Entity: as real world entities
– Edge: relations of entities in a schema
– Attribute: metadata about an entity
– Ontology: definition of possible entities, relations and attributes
– Yuqing Gao, 2018 [146]
In this definition, two components of attribute and ontology are the concepts
considered extra than other graph-based views. In fact, considering these components for knowledge representations adds on the characteristics of KGs. Entities
and relations usually capture information stored in a Knowledge Base (KB).
An Unambiguous Graph. As seen before, most of the attempts in defining
Knowledge Graphs have a focus on defining KGs as representing knowledge
24 L. Bellomarini et al.
in a graph structure. Therefore, the KGs are often represented by the main
components of a graph, namely nodes and edges. This graph is often considered
as a directed and labeled graph, without which the structure of the graph cannot
encode any significant meaning. When the nodes and edges are unambiguously
unidentifiable, the graph is considered to be an unambiguous graph. With this
foundation, a Knowledge Graph can be defined as:
“An Unambiguous Graph with a limited set of relations used to label the
edges that encodes the provenance, especially justification and attribution,
of the assertions.” – McCusker et al., 2018 [304]
This definition tried to go beyond representing KGs only as nodes and relations.
In order to fulfills this definition, all the knowledge units of a KG including
relations and nodes should be globally identifiable. In addition, the meaning of
limited set of relations is followed from [440] meaning a core set of essential
classes and relations that are true regardless of context. This level of abstraction
is similar to data representation in triple format with unique resource identifiers.
World Knowledge Graphs and Metadata. At a basic level of observation,
data represents elements as raw values collected from real-world domains of
knowledge. Metadata represent information about the underlying data in a second abstract level. In order to represent knowledge from real world:
1. the real world objects need to be observed at least once and represented as
data,
2. previous representation of such data is required to be captured as metadata
and
3. all of these meta-level definitions on top of the abstractions of the objects of
prime interest need to be connected.
At the formal and technical level, a formal and mathematical data structure,
degree of abstraction, and a syntactic and semantic language are needed. Thus,
characteristics of Knowledge Graphs lead the majority of the community to see
and define them as tools for representing world knowledge in a graph model,
where entities are represented as nodes and relations among entities are represented as directional edges. More formally, let E = {e1, ··· , eNe } be the set
of entities, R = {r1, ··· , rNr } be the set of relations connecting two entities,
D = {d1, ··· , dNd } be the set of relations connecting an entity and a literal, i.e.,
the data relations, and L be the set of all literal values. Then:
“a knowledge graph KG is a subset of (E×R×E)∪(E×D×L) representing
the facts that are assumed to hold.” – Wang et al., 2014 [462].
However, there are different attempts in defining the concept of KGs that we
will present in the following parts of this section.
Views on KGs as a Representation Tool for Knowledge. The following
definitions are pointing to a view where the structure of the graph representation
is not the only advantage but also includes ontological aspects of knowledge.
Chapter 2 Knowledge Graphs: The Layered Perspective 25
The actual knowledge lies in the power of ontologies represented in the graph
alongside the data level. In this way, the representation is enriched to handle
the complexity of real world (not yet complete in coverage) and to empower
learning, reasoning and inference abilities.
A Particular Kind of Semantic Network. The more intensive use of the
term Knowledge Graphs starts from the early 1980s where the concept of Semantic Networks was introduced [13,410,482]. Later it was continued as a project by
two universities from the Netherlands named Knowledge Graph [333,449]. Following the definition of semantic networks as a specific structure of representing
knowledge by labelled nodes and links between these nodes, KGs are defined as
follows:
A knowledge graph is a kind of semantic network representing some scientific theory. – Popping, 2003 [357]
In this view, representation of explicit knowledge is considered by way of its
formulation (logical or structured) [372]. While knowledge can be represented
in multi modals such as text, image etc., this definition is applicable only on
text extraction and analysis. Semantic networks are a way of structural formalism used for knowledge representation in nodes and edges. Such networks are
mainly used in expert systems with a rule base language, a knowledge base sitting in the background, and an inference engine. Knowledge represented and
reasoned by semantic networks are called author graphs with points as concept
units representing meaning and labeled links between concepts. One essential
difference between other views on Knowledge Graphs (in a broader sense) and
the one derived from semantic networks is the explicit choice of only a few types
of relations [219,440].
Representation of Human Knowledge. Although many of the definitions
for Knowledge Graph represent the concept as an formation representing tool,
some views see KGs as a lingua franca of humans and machines. KGs contain
information that is consumable by AI approaches in order to provide applications
such as semantic search, question answering, entity resolution, and representation learning.
“A graph-theoretic representation of human knowledge such that it can
be ingested with semantics by a machine; a set of triples, with each triple
intuitively representing an assertion.” – Kejriwal, 2019 [237]
Knowledge Represented with a Multi-relational Graph. A large volume
of human knowledge can be represented with a multi-relational graph. Binary
relationships encode facts that can be represented in the form of RDF-type
triples (head; predicate; tail), where head and tail are entities and predicate is the
relation type. The combination of all triples forms a multi-relational graph, where
nodes represent entities and directed edges represent relationships. The resulting
multi-relational graph is often referred to as a Knowledge Graph. Knowledge
26 L. Bellomarini et al.
Graphs (KGs) provide ways to efficiently organize, manage and retrieve this type
of information, and are increasingly used as an external source of knowledge for
problems like recommender systems, language modeling [2], question answering
or image classification.
One critical point to emphasize is that while many of the KGs we see today
contain as their knowledge mostly simple ground data, more and more applications need an actionable knowledge representation. To a certain extent, this is
already the case of existing Knowledge Base Management Systems, backed by
ontologies for which reasoning tasks are of different computational complexity
and expressive power. The importance of supporting implicit knowledge becomes
central for KGs as well, especially when they are a component of an Enterprise AI
applications, to the point that intensional knowledge should be considered part
of the KG itself. Consequently, reasoning, i.e., turning intensional into derived
ground knowledge, becomes inherently part of the KG definition.
For example, in a financial Enterprise AI application, the body of regulatory
knowledge and the functioning rules of the specific financial domain are of the
essence. As another example, in a logistics setting, the knowledge of how particular steps in a supply chain interact is often more important than the pure data
underlying the supply chain. Many more such examples could be given.
In total, it is clear that in modern KG-based systems a rich knowledge representation must be considered and properly handled in order to balance the
increased complexity with many other relevant properties including usability,
scalability, performance, and soundness of the KG application. We conclude
with a relatively structured, concrete definition accounting for these aspects:
“A semi-structured datamodel characterized by three components: (i) a
ground extensional component, that is, a set of relational constructs for
schema and data (which can be effectively modeled as graphs or generalizations thereof); (ii) an intensional component, that is, a set of inference rules
over the constructs of the ground extensional component; (iii) a derived
extensional component that can be produced as the result of the application of the inference rules over the ground extensional component (with
the so-called “reasoning” process).” – Bellomarini et al., 2019 – [40].
Here we focus on the knowledge representation aspects covered in this view and
in further layers we will discuss how this definition also sees KGs as management
systems and application platforms.
3 KGs as Knowledge Management Systems
In this section, we present the view of Knowledge Graphs as knowledge management systems. The clear analogy to see here is what a database management
system is for databases: A system to create, manipulate and retrieve data. What
this adds to the previous section’s view of KGs as knowledge representation tools
is the service that a KG as a knowledge management system has to offer. In particular, it has to provide support for the user to (i) add knowledge to a KG (ii)
Chapter 2 Knowledge Graphs: The Layered Perspective 27
derive new knowledge using existing knowledge, and (iii) retrieve data through
a form of general-purpose query language. In both (ii) and (iii), the aspect of
reasoning with and about knowledge becomes essential, which we will discuss in
detail in Chap. 6.
A Network of All Kinds of Things. One of the early attempts after the
appearance KGs in 2012, was a work clarifying the meaning of taxonomy, thesaurus, ontology and Knowledge Graph [54]. These concepts have been used
by scholars mostly without specific borderlines. In some cases, they even utilized interchangeably. Starting from the Simple Knowledge Organization System
(SKOS) as a standard for building an abstract model, taxonomies are introduced as controlled vocabularies to classify concepts and thesauri to express
associations and relations between concepts and their labels including synonyms.
Ontologies are considered as complex and more detailed versions of those domain
conceptualizations when the dependencies between concepts and relations get
more specific. There are also rules and constraints defined for representing knowledge which refer to ontologies as explicit and systematic specification of conceptualization for any kind of existence. By this, in building an abstract model of
the world or a domain, the meaning of all concepts must be formally defined that
can be interpreted correctly by machines. There must also be consensus about
the definition of the concepts such as the meaning in transferred correctly. In
AI-based approaches, the existence of things is defined when they can be represented [172]. Following these concepts, finally Knowledge Graphs are introduced
as enriched models around the aforementioned concepts more precisely:
“Knowledge Graphs could be envisaged as a network of all kinds of things
which are relevant to a specific domain or to an organization. They are not
limited to abstract concepts and relations but can also contain instances
of things like documents and datasets.” – Blumauer, 2014 [54].
The motivation behind having KGs is expressed in posing complex queries over a
broader set of integrated information from different source for knowledge discovery, and in-depth analyses. Knowledge Graphs being the networks of all kinds of
information, the industry-scale of such integration, together with the inclusion
of Taxonomy, Thesaurus and Ontology is seen as Enterprise Knowledge Graphs
(EKGs). Since this definition is mostly using semantic web technologies, the
specific querying language that suits this definition is suggested to be SPARQL,
and Resource Description Framework (RDF) is used as the data and ontology
representation model.
A Graph-based Representation of Knowledge. In a similar way, Knowledge Graphs are considered to be any kind of graph-based representations of
general information from the world [348]. This includes consideration of other
graph-based data models such as the RDF standard pushed by Semantic Web or
any knowledge representation languages such as description logic (DL). A simple
triple of such a graph representation could be seen as two nodes representing
entities which are connected by a relation. There are also predefined structural
relations such as is a relation which denotes the type of entities, or relations
28 L. Bellomarini et al.
denoting class hierarchies. As discussed before, such relations are usually represented as ontologies. In a universally unified level, this allows interlinking of
different datasets, which leads to big data in graph representations, or so called
Knowledge Graphs. Overall, this view mostly follows the basics of semantic representation of knowledge bases on the Web. The community has never come
up with a formal definition but generally, on a technical level, the overlapping
concepts have been coined together and built up a general understanding of the
concept connections. Following this view, a structured list of four characteristics
has been listed such that “a Knowledge Graph:
1. mainly describes real world entities and their interrelations, organized
in a graph,
2. defines possible classes and relations of entities in a schema,
3. allows for potentially interrelating arbitrary entities with each other,
4. covers various topical domains.” – Pullheim, 2017 [348]
Basically, the first characteristic refers to the terminological knowledge about
concepts of a domain, and is represented as TBox in description logic. The
second characteristic points to the assertions knowledge about individual entities
as ABox. By such a definition, a DL knowledge base can be constructed, on
top of which inference of new knowledge from the existence knowledge can be
applied. More in common language, the ontologies without instances and the
datasets without ontologies are not considered as a KG. As this way of knowledge
representation involves logical rules and ontologies, the KG created by this has
reasoning abilities. Complex queries are made possible with the power of data
representation and the existence of ontologies. Thus, this definition also falls into
the category of a KG being a management system.
A Finite Set of Ground Atoms. Looking at KGs as a graph of nodes and
links, assuming R as a set of relations and C a set of entities, the following formal
definition is given:
“A Knowledge Graph G is a finite set of ground atoms of the form p(s, o)
and c(s) over R∪C. With Σg = R, C, the signature of g, we denote
elements of R∪C that occur in g.” – Stepanova, 2018 [413]
This adopts first-order logic (FOL), seeing a set of correct facts as a KG. These
facts are represented as unary and binary triples. In addition to the reasoning
and querying power that comes from this definition, the power of explainability
is also addressed here. Such features are a must now for KGs as management
systems for AI-based downstream tasks.
A Graph of Data with the Intent to Compose Knowledge. In one of the
attempts in (re)defining Knowledge Graphs [55], datasets are seen in graph representations with nodes representing entities and links denoting their relations.
Example graph representation can be considered as:
– directed edge-labelled graphs as labelled edges between entities as nodes,
– property graphs as additional annotations on the edges,
– name graph as a collection of data represented in directed edge-labelled.
Chapter 2 Knowledge Graphs: The Layered Perspective 29
In a succinct view, the definition of KGs is then summarized as:
“A graph of data with the intent to compose knowledge.” – Hogan et al.,
2019 [55]
This definition brings another management action into the picture, namely composing knowledge. This is not only about knowledge representation in a graph
structure but also using that graph for a dedicated purpose. Construction of a
KG under this definition means facilitating complex management steps.
An Open-World Probabilistic Database [58]. Probabilistic databases, often
abbreviated PDBs, as the state of the art of processing large volumes of uncertain
data in a complete platform which is a combination of methods from information
extraction, natural language processing to relational learning [212].
“Knowledge Graphs are addressed as open-world Probabilistic databases
(OpenPDBs).” – Borgwardt, 2017 – [58].
A Knowledge Graph Management System [42]. The authors pose a number of requirements or desiderata for a Knowledge Graph Management System
(KGMS) in terms of the main system capabilities:
– simple modular syntax: easy to add and remove facts and rules
– high expressive power: at least as expressive as Datalog (i.e., full recursion)
– numeric computation and aggregation: real-world required features
– ontological reasoning: at least as expressive as SPARQL and OWL 2 QL
– probabilistic reasoning: should support a form of probabilistic reasoning
– low complexity: the core language should be tractable in data complexity
– rule repository, management and ontology editor: management facilities
– dynamic orchestration: allow orchestration of complex, real-world workflows
They also formulate a number of access/integration requirements, some of which
are what we consider core capabilities in this section, some of which we will
include in the following section on application services. The ones of core relevance
for management systems are:
– big data access: must be able to consume Big Data sources and interface with
such systems
– database and data warehouse access: must seamlessly integrate with relational
databases, graph stores, RDF stores, etc.
– ontology-based data access (OBDA): allow queries on top of ontologies
– multi-query support: allow multiple queries executed in parallel to benefit
from each other
– procedural code support: allow easy integration of procedural code
They subsequently presented the Vadalog system [38] in more technical detail,
focusing on algorithms and data structures to meet the requirement on high
expressive power, ontological reasoning and low complexity at the same time.
Subsequent papers discuss highly parallelizable fragments [44,45,49], how to
achieve maintainability [64] and other related topics, including more fundamental
aspects [43,162].
30 L. Bellomarini et al.
4 KGs as Knowledge Application Services
While not usually providing quotable definitions of Knowledge Graphs, there is
a huge body of work that does not primarily treat KGs as representation tools
or management systems, but as a platform to provide a large number of crucial
applications. So instead of a KG being used to represent information or manage
information, it is rather the capability of the KG to natively or easily support
certain applications that define what a KG is.
For example, [116] introduces KGs not only as the graph containing all of
the Amazon product data, but as a graph that has the special capability of
natively supporting entity resolution (i.e., knowing when two products are the
same) and entity linking (i.e., knowing when two products or other entities are
related). Similar considerations can be found in many KG-related fields. It could
even be argued that the amount of work in KG completion, etc., makes this
application-oriented view of KG the most important one.
Clearly, the border between the two views of management and application is
debatable, and we invite the reader to critically think of what one should consider
as an essential general-purpose service of a knowledge management system, and
what should be part of an application service. We shall explore this aspect in
this section, and in particular in Chap. 6. For example, while question answering
in our opinion would typically be considered as an application service, as would
be offering recommender system capabilities, it is less clear for relatively generalpurpose application services such as entity resolution and link prediction, which
could be seen as a requirement of a general purporse knowledge management
system. Here, we will consider all of four of these as application services as
they clearly offer a well-defined application compared to a management system
offering a query language that supports such applications.
Knowledge Organization System. This view is from the domain of libraries
and humanities where KGs are sees as knowledge organization systems. Even
in a further vision, KGs are seen to integrate the insights derived from analysis
in large-scale domains. This vision is already in practice by reasoning systems
considered as a part of the KG concept.
“Knowledge Graphs represent concepts (e.g., people, places, events) and
their semantic relationships. As a data structure, they underpin a digital
information system, support users in resource discovery and retrieval, and
are useful for navigation and visualization purposes.” – Haslhofer, 2018
[188]
Scholarly communication artifacts, such as bibliographic metadata about scientific publications, research datasets, citations, description of projects,and profile information of researchers, has recently gained a lot of attention with KG
technologies. With the help of Linked Data technologies, interlinking of semantically represented metadata has been made possible. Discovering and providing
links between the metadata of scholarly artifacts is important in scholarly communities. This definition has a particular view of KGs for such purposes. The
Chapter 2 Knowledge Graphs: The Layered Perspective 31
links are generated retrospectively by devising similarity metrics over sets of
attributes of the artifact descriptions. Interlinking of such metadata provides
shareable, extensible, and easily re-usable metadata in the form of KGs. We also
address the scholarly domain as one of the example applications.
Rule Mining and Reasoners. One of the early attempts in systematic definitions of KGs goes beyond seeing them as only a representation tool but more
as a management system close to database management systems.
“A Knowledge Graph acquires and integrates information into an ontology
and applies a reasoner to derive new knowledge.” – Ehrlinger, 2016 – [121].
This is one of the early attempts in defining KGs in a systematic way with
a different view. Similarly, the following definitions sees KGs as a specific data
model. There are several rule mining reasoners around which are purely designed
to consume the ontology and mine relational patterns out of the KG. One example of this category is AMIE [144]. We categorize it under this view because it is
more than just a representation tool and performs some data management steps.
It has RDF as the data model for representing the facts and rules and uses its
own internal functions for rule mining.
Data Application Platform. The VADA project [257] saw many application
services built on top of its Knowledge Graph Management System (KGMS)
Vadalog [164]. Before going into concrete examples, let us inspect the application
service requirements given in [42]:
– data cleaning, exchange, integration: often summarized as “data wrangling”
– web data extraction, interaction and IoT: to interact with the outside world
– machine learning, text mining, NLP, data analytics: providing and interfacing
with external such services. An interesting side-note is that the authors here
invert the perspective: it is not always the knowledge graph system providing
the application service, but sometimes also using it.
– data visualization: for providing data consumable by an end-user or analyst
Let us now proceed to concrete examples of these abstract requirements. Prime
among them is:
– Data Wrangling, i.e., the whole process of bringing raw data into an integrated
format amenable to Big Data Analytics [141,257,258]. Further services seen
as key were at the data acquisition phase the application service
– Data Extraction [132,262,308]. Further key application services are those of
– Recommender Systems [82], including services for downstream machine-learning applications which need feature engineering. A connected but independent
application platform requirement is that of
– Social Choice [89,90] where the application requirement is to choose among
a number of different users’ preferences the best joint solution. A further
one, for which it is somewhat debatable whether it is a management system
requirement or an application service is that of
32 L. Bellomarini et al.
– Machine Learning [41] service integration - bridging typical KGMS services
and machine learning services. Another interesting case is that of a vertical
application service collection, namely that of
– Company Knowledge Graphs [24,39], especially important for the COVID19 perspective raised in one of the works on the economic impact of the
pandemic.
5 KGs in Practice: Challenges and Opportunities
The initial release of KGs was started on an industry scale by Google and further continued with the publication of other large-scale KGs such as Facebook,
Microsoft, Amazon, DBpedia, Wikidata and many more. As an influence of the
increasing hype in KG and advanced AI-based services, every individual company or organization is adapting to KG. The KG technology has immediately
reached industry, and big companies have started to build their own graphs such
as the industrial Knowledge Graph at Siemens [206]. In a joint work [331] for
sharing ideas from large-scale industrial Knowledge Graphs, namely Microsoft,
Google, Facebook, eBay and IMB, authors stated a broad range of challenges
ahead of research and industry involving KGs. Despite the content-wise difference and similarities of those Knowledge Graphs, the discussions involve data
acquisition and provenance problems due to source heterogeneity and scalability of the underlying managements system. Here we introduce the Enterprise
Knowledge Graph of Italian companies for the Central Bank of Italy.
5.1 Integrated Ownership and Company Control
The database at our disposal contains data from 2005 to 2018, regarding unlisted
companies and their shareholders (companies or persons). If we see the database
as a graph, where companies and persons are nodes and shareholding is represented by edges, on average, for each year the graph has 4.059M nodes and
3.960M edges. There are 4.058M Strongly Connected Components (SCC), composed on average of one node, and more than 600K Weakly Connected Components (WCC), composed on average of 6 nodes, resulting in an high level of
fragmentation. Interestingly, the largest SCC has only 15 nodes, while the largest
WCC has more than one million nodes. The average in- and out-degree of each
node is ≈1 and the average clustering coefficient is ≈0.0084, which is very low
compared to the number of nodes and edges. Furthermore, it is interesting to
observe that the maximum in-degree of a node is more than 5K and the maximum out-degree is more than 28K nodes. We also observe a high number of
self-loops, almost 3K, i.e. companies that directly own shares of themselves in
order to subtract them from the market. The resulting graph shows a scale-free
network structure, as most real-world networks [148]: the degree distribution
follows a power-law and there are several nodes in the network that act as hubs.
The Register of Intermediaries and Affiliates (RIAD), the ownership network
of European financial companies run by the European Central Bank, is a good
Chapter 2 Knowledge Graphs: The Layered Perspective 33
example of the company control topology at the European level. It has one large
SCC containing 88 nodes, and all the others with less than 10 nodes; there is
one huge WCC, with 57% of the nodes, with the others scattered around small
WCCs with 11.968 nodes on average and (apart from the largest one), none with
more than 472 nodes.
5.2 Large-Scale Scholarly Knowledge Graphs
The complexity of scholarly data fully follows the 6 Vs of Big Data characteristics towards building Scholarly Knowledge Graphs [405]. The term Big Scholarly
Data (BSD) [474] is coined to represent the vast amount of information about
scholarly networks including stakeholders and artifacts such as authors, organizers, papers, citations, figures. The heterogeneity and complexity of data and
their associated metadata distributed on the Web perfectly qualifies this domain
for Big Data challenges towards building Scholarly KGs:
– Volume refers to the ability to ingest and store very large datasets; in the context of scholarly metadata, at least over 114 million scholarly documents [240]
were recorded in 2014 as being available in PDF format. In computer science, the total number of publications of the different types is reaching 4
million [423]. Different types of publication in different formats are being
published every day in other scientific disciplines.
– Velocity denotes the growth rate generating such data; the average growth
rate of scientific publishing is measured as 8 to 9% [61].
– Variety indicates multiple data formats and models; the domain of scholarly
communication is a complex domain [29] including many different types of
entities with complex interrelationships among them.
– Value concerns the impact of high quality analytics over data; certain
facts play enormously important roles in the reputation and basic life of
research stakeholders. Providing precise and comprehensive statistics supports researchers with already existing success measurement tools such as
number of citations. In additions, deep and mined knowledge with flexible
analytics can provide new insights about artifacts and people involved in the
scholarly communication domain.
– Veracity refers to the biases, ambiguities, and noise in data; this characteristic
is especially applicable in the context of the scholarly communication domain
due to deduplication problems [296] and the ambiguity problem for various
scholarly artifacts as well as person names.
– Variability of the meaning of the metadata [474].
Discovering high quality and relevant research-related information has a certain
influence on the life of researchers and other stakeholders of the communication
system [109]. For examples, scholars search for quality in the meaning of fitness for use in questions such as “the venues should a researcher participate” or
“the papers should be cited”. There are already attempts to assist researchers
34 L. Bellomarini et al.
in this task, however, resulting in recommendations often being rather superficial and the underlying process neglecting the different aspects that are important for authors [439]. Providing recommendation services to researchers and
a comprehensive list of criteria while they are searching for relevant information. Furthermore, having access to the networks of a paper’s authors and their
organizations, and taking into account the events in which people participate,
enables new indicators for measuring the quality and relevance of research that
are not just based on counting citations [438]. Thus each of the Vs of Big Data
needs careful management to provide such services for scholarly communities.
6 Conclusion
In this chapter, we introduced Knowledge Graphs in a layered perspective:
Knowledge Graphs as (1) knowledge representations tools, (2) knowledge management systems, and (3) knowledge application services. We did not focus on
a single definition here but presented a multitude of definitions, putting them
into the context of this layered perspective. We deliberately stopped short of
the chapter being an exhaustive historical overview as excellent overviews have
already been written.
We also pointed toward aspects of particular concern: The different ways that
particular communities see KGs (top-down or bottom-up, or even middle-layer
in focus). We concluded with the practical challenges of KGs by providing typical
industrial and academic applications. Throughout the chapter, we discussed the
aspect of reasoning being a natural counterpart to this “bigger picture” focus
section, and we shall consider reasoning in greater detail in Chap. 6.
Acknowledgements. E. Sallinger acknowledges the support of the Vienna Science and Technology (WWTF) grant VRG18-013 and the EPSRC programme grant
EP/M025268/1.
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.
Chapter 3
Big Data Outlook, Tools,
and Architectures
Hajira Jabeen(B)
CEPLAS, Botanical Institute, University of Cologne, Cologne, Germany
hajira.jabeen@uni-koeln.de
Abstract. Big data is a persistent phenomena, the data is being generated and processed in a myriad of digitised scenarios. This chapter covers
the history of ‘big data’ and aims to provide an overview of the existing
terms and enablers related to big data. Furthermore, the chapter covers
prominent technologies, tools, and architectures developed to handle this
large data at scale. At the end, the chapter reviews knowledge graphs
that address the challenges (e.g. heterogeneity, interoperability, variety)
of big data through their specialised representation. After reading this
chapter, the reader can develop an understanding of the broad spectrum
of big data ranging from important terms, challenges, handling technologies, and their connection with large scale knowledge graphs.
1 Introduction
The digital transformation has impacted almost all aspects of modern society.
The past decade has seen tremendous advancements in the areas of automation,
mobility, the internet, IoT, health, and similar areas. This growth has led to
enormous data-generation facilities, and data-capturing capabilities.
In the first section “Outlook”, we review the definitions and descriptions of
big data and discuss the drivers behind big data generation, the characteristics
exhibited by big data, the challenges offered by big data, and the handling of this
data by creating data value chains. In the section “Tools and Architectures”, we
cover the software solutions and architectures used to realise the big data value
chains. We further cover characteristics and challenges relating to big data. The
section “Harnessing the Big Data as Knowledge Graphs” connects knowledge
graphs and big data, outlining the rationale and existing tools to handle largescale knowledge graphs.
2 Big Data: Outlook
Today, big data is widespread across and beyond every aspect of everyday life.
This trend of increasing data was first envisioned and defined years ago. Notably,
the first evidence of the term big data comes from a paper [87] published in 1997,
c The Author(s) 2020
V. Janev et al. (Eds.): Knowledge Graphs and Big Data Processing, LNCS 12072, pp. 35–55, 2020.
https://doi.org/10.1007/978-3-030-53199-7_3
36 H. Jabeen
where the authors described the problem as BigData when the data do not fit in
the main memory (core) of a computing system, or on the local disk. According
to the Oxford English Dictionary (OED), big data is defined as: “data of a
very large size, typically to the extent that its manipulation and management
present significant logistical challenges.” Later, when the terms velocity, variety,
and volume were associated as characteristics of big data, the newer definitions
of the term ‘big data’ came to cover these characteristics, as listed below:
1. “Big data is high volume, high velocity, and/or high variety information assets
that require new forms of processing to enable enhanced decision making,
insight discovery and process optimization,” [271,297].
2. “When the size of the data itself becomes part of the problem and traditional
techniques for working with data run out of steam,” [288].
3. Big Data is “data whose size forces us to look beyond the tried-and-true
methods that are prevalent at that time,” [217].
4. “Big Data technologies are a new generation of technologies and architectures
designed to extract value economically from very large volumes of a wide
variety of data by enabling high-velocity capture, discovery, and/or analysis,”
[470].
5. “Big data is high-volume, high-velocity and high-variety information assets
that demand cost-effective innovative forms of information processing for
enhanced insight and decision making,” [271].
6. “Big Data is a term encompassing the use of techniques to capture, process,
analyse and visualize potentially large datasets in a reasonable timeframe not
accessible to standard IT technologies.” By extension, the platform, tools and
software used for this purpose are collectively called “Big Data technologies,”
[98].
7. “Big data can mean big volume, big velocity, or big variety,” [414].
8. “The term is used for a collection of datasets so large and complex that it
becomes difficult to process using on-hand database management tools or
traditional data processing applications”1.
9. “Big data represents the information assets characterized by such a high
volume, velocity and variety to require specific technology and analytical
methods for its transformation into value”2.
Regardless of the defining text, big data is a persistent phenomenon and is
here to stay. We take a brief overview of the key enabling technologies that made
big data possible in the following section.
2.1 Key Technologies and Business Drivers
As recently as the year 2000, digital information only constituted about one
quarter of all the stored information worldwide3. Other information was mainly
1 http://en.wikipedia.org/. 2 http://en.wikipedia.org/. 3 https://www.foreignaffairs.com/articles/2013-04-03/rise-big-data.
Chapter 3 Big Data Outlook, Tools, and Architectures 37
stored on paper, film, or other analogue media. Today, by contrast, less than
two percent of all stored information is non-digital. The key enablers of the big
digital data revolution are the advancements in technologies, be it increased
internet speed, the availability of low-cost handheld mobile devices, or the myriad of applications ranging from social media to personal banking. At present,
organizations view the acquisition and possession of data as significant assets. A
report by the World Economic Forum [315], “Big Data, Big Impact,” declared
‘data’ an asset akin to currency or gold. This fact has led to significant changes
in business models. Besides, aggressive acquisition and retention of data have
become more popular among organizations. Prominent examples are internet
companies such as Google, Yahoo, Amazon, or Facebook, which are driven by
new business models. Technology proliferation is one of the major enablers of big
data acquisition. Cheaper and accessible technology is being used in almost all
parts of modern society, be it smart devices, mobile devices, wearable devices, or
resources to store data on the cloud, enabling customers to make purchases and
book vacations among other functions. In the following section, we will cover a
few of the prominent big data enabling technologies and key drivers.
Internet. The advancements in internet bandwidth and streaming have enabled
fast and efficient data transfer between physically distant devices. People around
the globe are accessing the internet via their mobile devices and the number
of connected devices is constantly increasing. The number of internet users
increased from 4.021 billion to 4.39 billion from 2018 to 2019. Almost 4.48 billion
people were active internet users as of October 2019, encompassing 58% of the
global population [83]. In the age of digital society, there is a need for a powerful
wireless network that can rapidly transfer large volumes of data. Presently, we
are moving from 4G LTE to 5G NR, which will enable entirely new applications
and data-collection scenarios. 5G not only comes with better bandwidth and
faster speed but also lower latency. The low latency of 5G was demonstrated
by “Orchestrating the Orchestra”4 – an event that enabled musicians across
different locations to perform at the Bristol 5G Smart Tourism event. Violinist
Anneka Sutcliffe was playing in Bristol, Professor Mischa Dohler was playing the
piano in The Guildhall, London, and vocalist Noa Dohler and violinist Rita Fernandes were at Digital Catapult in Euston, London. These developments have
made it possible to share and curate large amounts of data at high speeds.
Automation and Digitization. Digital automation is a relatively broad term
and it covers tasks that can be done automatically with minimal human assistance, increasing the speed and accuracy as a result. Businesses are more and
more favouring the use of automatization tools to achieve more throughput.
For example, advancements in automatic tools like scanning systems no longer
require manual entry, easing and speeding up the process at the cost of more and
reliable data capture. Similarly, in terms of digital data, cameras and photos are
4 https://www.bristol.ac.uk/news/2019/march/orchestrating-the-orchestra.html.
38 H. Jabeen
another example. The number of digital photos taken in 2017 was estimated to
be 1.2 trillion5, which is roughly 160 pictures for every one of the roughly 7.5 billion people inhabiting planet earth. With more and more devices being digitized,
more data is being created and stored in machine-readable form. Industries and
businesses must harness this data and use it to their advantage.
Commodity Computing. Commodity hardware, also known as off-the-shelf,
non-proprietary hardware, refers to low-cost devices that are widely available.
These devices can be easily replaced with a similar device, avoiding vendor lockin challenges. ‘Commodity cluster computing’ is the preference of using more of
average-performing, low-cost hardware to work in parallel (scalar computing),
rather than having a few high-performance, high-cost items of hardware. Hence,
commodity computing enables the use of a large number of already existing
computing resources for parallel and cluster computing without needing to buy
expensive supercomputers. Commodity computing is supported by the fact that
software solutions can be used to build multiple points of redundancy in the
cluster, making sure that the cluster remains functional in case of hardware
failures. Low-cost cluster computing resources have made it possible to build
proprietary data centres on-premises and to reap the benefits of in-house big
data handling and processing.
Mobile Computing. Handheld smart devices are becoming more and more
common due to increased affordability, relevance, and digital literacy. There were
5.11 billion unique mobile users in 2018 and 5.135 billion in 2019, accounting for
67% of the global population, and it is estimated [83] that by 2020, almost 75%
of the global population will be connected by mobile. Use of mobile computing
has enabled almost everyone to access and generate data, playing a key role in
big data generation and sharing.
Mobile Applications. Mobile devices are playing a key role in the present data
explosion. Mobile phones are no longer only being used for voice calls. Currently,
56% of web access worldwide is generated by mobile devices. At the moment,
more than 57% of the global population use the internet and more than 52% of
the global population use mobile devices [83]. Businesses are developing mobile
applications to not only assist users in ubiquitous computing but also to generate and capture data of interest. The mobile application development industry
is creating mobile apps for almost all fields, and existing mobile applications
cover a range of tasks like online banking, online purchases, social interactions,
travelling, eating, studying, or entertainment. All of these applications not only
assist in automated data collection related to tasks (e.g. orders) but also assist
in generating additional data that was not easily possible before (e.g. correlating
a new order with previous purchases).
5 https://www.statista.com/statistics/617136/digital-population-worldwide/.
Chapter 3 Big Data Outlook, Tools, and Architectures 39
Ubiquitous Devices (IoT). The internet of things (IoT) enables scenarios
where network connectivity and computing capability extends to objects, sensors
and everyday items not normally considered computers, allowing these devices to
generate, exchange and consume data with minimal human intervention. These
devices are not only increasing in number to cover different facets of life but are
also increasing their sensitivity. According to a GSMA report [16], between 2018
and 2025, the number of global IoT connections will triple to 25 billion. In an
estimate by CGI [2], the total volume of data generated by IoT will reach 600
ZB per year by 2020.
Cloud Infrastructure. Cloud computing is the term used for storing, accessing
and processing data over the internet from a remote server. This ability to store
and manipulate data on the cloud using services like AWS [50], Google Cloud
Platform [264], Cloudera [378], etc. has made it possible to store and analyse
data on-demand with a pay-per-use model. Cloud computing saves costs, offers
better performance, reliability, unlimited capacity, and quick deployment possibilities. Cloud computing has assisted organizations with providing the data
centre management and efficient data-handling facilities.
2.2 Characteristics of Big Data
Driven by digital transformation, big data is identified by several key attributes.
Interestingly, they all start with the letter ‘V’, and therefore are also called the
V’s of big data. The number of characteristic attributes is constantly increasing
with advancements in technologies and underlying business requirements. In this
section, we cover a few of the main V’s used to describe big data.
Three Vs of Big Data [271,489]
1. Volume: The size of data is increasing at unprecedented rates. It includes data
generated in all fields including science, education, business, technology and
governance. If we take the social media giant Facebook (FB) as an example,
it has been reported that FB generates approximately 4 petabytes of data in
24 h with 100 million hours of video watch-time. FB users create 4 million
likes per minute, and more than 250 billion photos have been uploaded to
Facebook since its creation, which equates to 350 million photos per day.
Apart from the applications, a vast amount of data is being generated by
web, IoT and many other automation tools continuously. All of this data
must be captured, stored, processed and displayed.
2. Velocity: The speed at which the data is being generated has increased rapidly
over the years. The high rate and speed is contributed by the increase in the
use of portable devices to allow data generation and ever-increasing bandwidth that allows fast data transfer. In addition, the rate of data generation
(from the Internet of Things, social media, etc.) is increasing as well. Google,
for example, receives over 63,000 searches per second on any given day. And
15% of all searches have never been searched before on Google. Therefore,
40 H. Jabeen
it is critical to manage and analyse the data at the same rate at which it is
being generated and stored in the system.
3. Variety: The data comes from a variety of data sources and is generated in
different forms. It can be structured or unstructured data. Data comes in
the form of text (emails, tweets), logs, signals, records, photos, videos, etc.
This data cannot be stored and queried via traditional structured database
management systems. It is important to develop new solutions that are able
to store and query diverse data; 4 Vs of Big Data [106].
4. Veracity: This is the quality, truthfulness or reliability of data. Data might
contain biases, noise, or abnormalities. It is important to be aware of whether
data being used can be trusted for use in making important decisions, or if
the data is meaningful to the problem being analysed. The data is to be
used to make decisions that can bring strategic competitive advantages to
the business; 10 Vs of Big Data [60].
5. Variability: This is the dynamic and evolving nature of data. The data flow
is not constant or consistent. The speed, density, structure, or format of data
can change over time and several factors influence the consistency of data
that changes the pattern, e.g. more shoppers near Christmas, more traffic in
peak hours etc.
6. Value: This refers to the worth of the data being extracted. For an organization, it is important to understand the cost and associated benefits of
collection and analysis of data. It is important to know that the data can
be turned into value by analysis, and that it follows set standards of data
quality, sparsity or relatedness.
7. Visualization is often thought of as the only way in which customers can
interact with models. It is important to visualize the reports and results
that can be communicated and extracted from data in order to understand
underlying patterns and behaviours.
In addition to the characteristics mentioned above, some researchers have
gone as far as to introduce 42 [395], or even 51 [243] different Vs to characterise
big data.
2.3 Challenges of Big Data
The characteristics of data combined with targeted business goals pose plenty of
challenges while dealing with big data. In this section, we briefly cover the main
challenges involved in using big data.
Heterogeneity. Heterogeneity is one of the major features of big data, also
characterised as the variety. It is data of different types and formats. The heterogeneous data introduces the problems of data integration in big data analytics,
making it difficult to obtain the desired value. The major cause of data heterogeneity is disparate sources of data that generate data in different forms. The
data can be text data coming from emails, tweets or replies; log-data coming
from web activities, sensing and event data coming from IoT; and other forms.
Chapter 3 Big Data Outlook, Tools, and Architectures 41
It is an important challenge to integrate this data for value-added analytics and
positive decision making.
Uncertainty of Data. The data gathered from heterogeneous sources like sensors, social media, web activities, and internal-records is inherently uncertain
due to noise, incompleteness and inconsistency (e.g., there are 80% - 90% missing links in social networks and over 90% missing attribute values for a doctor
diagnosis in clinic and health fields). Efficient analysis to discover value from
these huge amounts of data demands tremendous effort and resources. However,
as the volume, variety and velocity of the data increases, the uncertainty inherent in the data also increases, leading to doubtful confidence in the resulting
analytics and predicted decisions.
Scalability. The volume of data is drastically increasing and therefore an important challenge is to deal with the scalability of the data. It is also important to
develop efficient analytics solutions and architectures that can scale up with
the increasing data without compromising the accuracy or efficiency. Most of
the existing learning algorithms cannot adapt themselves to the new big-data
paradigms like dealing with missing data, working with partial data access or
dealing with heterogeneous data sources. While the problem complexity of big
data is increasing at a very fast rate, the computational ability and the solution
capability is not increasing at a similar pace, posing a vital challenge.
Timeliness. When looking for added business values, timing is of prime importance. It is related to capturing data, execution of analytics and making decisions at the right time. In a dynamic and rapidly evolving world, a slight delay
(sometimes microseconds) could lead to incorrect analytics and predictions. In
an example case of a bogus online bank transaction, the transaction must be
disapproved in a timely manner to avoid possible money loss.
Data Security. Data storage and exchange in organizations has created challenges in data security and privacy. With the increasing sizes of data, it is important to protect e.g. transaction logs and data, real-time data, access control data,
communication and encryption data. Also, it is important to keep track of data
provenance, perform granular auditing of logs, and access control data to determine any misuse of data. Besides, the difference between legitimate use of data
and customer privacy must be respected by organizations and they must have
the right mechanisms in place to protect that data.
2.4 Big Data Value Chain
The ability to handle and process big data is vital to any organization. The
previous sections have discussed data generation abilities, and the characteristics
and challenges of dealing with big data. This section covers the required activities
42 H. Jabeen
and actions to handle such data to achieve business goals and objectives. The
term value chain [358] is used to define the chain of activities that an organization
performs to deliver a product for decision support management and services. A
value chain is composed of a sequence of interconnected sub-services, each with
its own inputs, transformation processes, and outputs. The noteworthy services
are described below.
Data Acquisition. This is the process of gathering data, filtering, and cleaning the data for storage and data analysis. Data acquisition is critical, as the
infrastructure required for the acquisition of big data must bear low, predictable
latency for capturing data as well as answering queries. It should be able to
handle very high transaction volumes in a distributed scalable environment, and
be able to support flexible and dynamic heterogeneous data.
Data Analysis. Interpreting the raw data and extraction of information from
the data, such that it can be used in informed decision making, is called data
analysis. There could be multiple domain-specific analysis based on the source
and use of data. The analysis includes filtering, exploring, and transforming
data to extract useful and often hidden information and patterns. The analysis
is further classified as business intelligence, data mining, or machine learning.
Data Curation. This is the active and continuous management of data through
its life cycle [350]. It includes the organization and integration of data from multiple sources and to ascertain that the data meets given quality requirements for
its usage. Curation covers tasks related to controlled data creation, maintenance
and management e.g. content creation, selection, validation or preservation.
Data Storage. This is persistent, scalable data management that can satisfy the needs of applications requesting frequent data access and querying.
RDBMS have remained a de facto standard for organizational data management for decades; however, its ability to handle data of limited capacity and
well-defined structure (ACID properties, Atomicity, Consistency, Isolation, and
Durability) has made it less suitable to handle big data that has variety, in addition to volume and velocity. Novel technologies are being designed to focus on
scalability and cope with a range of solutions handling numerous data models.
Data Usage. This is the analysis of data covering the business activities assisting in business decision making. The analysis is made possible through the use
of specialised tools for data integration or querying.
3 Tools and Architectures
3.1 Big Data Architectures
Several reference architectures have been proposed to support the design of big
data systems. Big data architecture is the conceptual model that defines the
Chapter 3 Big Data Outlook, Tools, and Architectures 43
structure and behaviour of the system used to ingest and process “big data”
for business purposes. The architecture can be considered a blueprint to handle
the ingestion, processing, and analysis of data that is too large or complex for
traditional database systems. The aim is to design a solution based on the business needs of the organization. Based on the requirements, the proposed solution
must be able to handle different types of workloads like batch processing or realtime processing. Additionally, it should be able to perform analytics and mining
on this large-scale data.
Good architecture design can help organizations to reduce costs, assist in
making faster and better decisions, and predict future needs or recommend new
solutions. However, the creation of such a system is not straightforward and
certain challenges exist in designing an optimal architecture.
Data Quality: This is one of the important challenges in all domains of data
handling. The data could be noisy, incomplete or simply missing. Substantial
processing is desired to make sure that the resulting data is of the desired quality.
It is a widely known fact that “data preparation accounts for about 80% of the
work of data scientists”.
Data Integration: The architecture must be able to handle the integration
of heterogeneous data coming from disparate sources. It is challenging to handle
and integrate data of multiple sizes and forms coming at different speeds from
multiple sources. Finally, the system should be able to carry out meaningful
analytics on the data to gain valuable insights.
Data Scale: It is important to design a system that works at an optimal
scale without over-reserving the resources. At the same time, it should be able
to scale up as needed without compromising performance.
In order to comply with the data value chain, any big data architecture comprises
of the components that can allow to perform desired operations.
Data Sources: The data of an organization might be originating from
databases, real-time sources like web-logs, activity data, IoT devices and many
others. There should be data ingestion and integration components embedded
in the architecture to deal with these data sources.
Data Ingestion: If the data is coming from the real-time sources, the architecture must support the real-time data ingestion mechanism.
Data Storage: Depending upon the number and types of data sources, efficient data storage is important for big data architecture. In the case of multiple
types of data sources, a no-SQL “data lake” is usually built.
Data Processing: The data in the system needs to be queried and analysed, therefore it is important to develop efficient data-querying solutions, or
data-processing tools that can process the data at scale. These processing solutions can either be real-time or batch, depending upon the originating data and
organizational needs.
Data Analysis: Specialised tools to analyse data for business intelligence
are needed to extract meaningful insights from the data.
Data Reporting, and Visualisation: These are the tools used to make
reports from the analysed data and to present the results in visual form.
44 H. Jabeen
Process Automation: Moving the data across the big data architecture
pipeline requires automated orchestration. The ingestion and transformation of
the data, moving it for processing, storage, and deriving insights and reporting
must be done in a repeatable workflow to continuously gain insights from the
data.
Depending upon the type of data and the individual requirements of the
organizations, the selected tasks must be handled by choosing corresponding
services. To support the tasks and selected services, the overall architecture to
realise the data value chain is designed. The big data architectures are mainly
divided into three main types as below:
Lambda Architecture. The lambda architecture, first proposed by Nathan
[99], addresses the issue of slow queries results on batch data, while real-time
data requires fast query results. Lambda architecture combines the real-time
(fast) query results with the queries (slow) from batch analysis of older data.
Lambda architecture creates two paths for the data flow. All data coming into the
system goes through these two paths. Batch Layer: also known as the cold path,
stores all the incoming data in its raw form and performs batch processing on the
data. This offers a convenient way to handle reprocessing. This layer executes
long-living batch-processes to do analyses on larger amounts of historical data.
Speed Layer: also known as the hot path, analyses the data in real-time. This
layer is designed for low latency. This layer executes small/mini batch-processes
on data according to the selected time window (e.g. 1 s) to do analyses on the
latest data. Serving Layer: This layer combines the results from the batch and
speed processing layer to enable fast interactive analyses by users.
Kappa Architecture. Kappa architecture was proposed by Jay Kreps [263]
as an alternative to lambda architecture. Like Lambda architecture, all data
in Kappa architecture flows through the system, but uses a single path, i.e.
a stream processing system. Kappa architecture focuses only on data stream
processing, real-time processing, or processing of live discrete events. Examples
are IoT events, social networks, log files or transaction processing systems. The
architecture assumes that: The events are ordered and logged to a distributed file
system, from where they can be read on demand. The platform can repeatedly
request the logs for reprocessing in case of code updates. The system can handle
online machine learning algorithms.
Microservices-Based Architecture. “Microservice Architecture” has emerged over the last few years to describe a particular way of designing software applications as suites of independently deployable services [283]. Microservices architecture makes use of loosely coupled services which can be developed, deployed
and maintained independently. These services can be built for business capability,
automated deployment, intelligence in the endpoints, and decentralized control of
languages and data.
Chapter 3 Big Data Outlook, Tools, and Architectures 45
Microservices-based architecture is enabled by a multitude of technology
advancements like the implementation of applications as services, emergence
of software containers for service deployment, orchestration of containers, development of object stores for storing data beyond container lifecycle, requirement
for continuous integration, automated testing, and code analysis to improve software quality. Microservices-based architecture allows fast delivery of individual
services independently. In this architecture, all the components of big data architecture are treated as services, deployable on a cluster.
3.2 Tools to Handle Big Data
In order to deal with big data, a variety of specialised tools have been created.
This section provides an overview of the existing tools based on their functionalities. A distributed platform handling big data is made up of components needed
for the following tasks. We will cover the tools developed to perform these specific
tasks in the preceding sections (Fig. 1).
Fig. 1. Classification of tools to handle big data
Resource Orchestration. Distributed coordination and consensus is the backbone of distributed systems. Distributed coordination deals with tasks like telling
each node about the other nodes in the cluster and facilitating communication
and high availability. High availability guarantees the presence of the mediator
node and avoids a single point of failure by replication resulting in a fault-tolerant
system. In a distributed setting, the nodes must share common configurations
and runtime variables and may need to store configuration data in a distributed
key-value store. The distributed coordination manages the sharing of the locks,
shared-variables, realtime-configurations at runtime among the nodes.
46 H. Jabeen
In addition, fault-tolerant distributed systems contain methods to deal with
the consensus problem, i.e. the servers or mediators in the distributed system
perform agreement on certain values or variables, e.g. there can be a consensus
that the cluster with 7 servers can continue to operate if 4 servers get down,
i.e. with only 3 servers running successfully. The popular orchestration tools are
Apache zookeeper and etcd. The systems are consistent and provide primitives
to be used within complex distributed systems. Such distributed orchestrators
ease the development of distributed applications and make them more generic
and fault resilient.
Apache Zookeeper: Apache Zookeeper [209] is an open-source project that
originated from the Hadoop ecosystem and is being used in many top-level
projects including Ambari, Mesos, Yarn, Kafka, Storm, Solr, and many more
(discussed in later sections). Zookeeper is a centralised service for managing
cluster configuration information, naming and distributed synchronization and
coordination. It is a distributed key-value store that allows the coordination of
distributed processes through a shared hierarchical name space of data registers
(znodes), like a file system. Zookeeper provides high throughput, low latency,
high availability and strictly ordered access to the znodes. Zookeeper is used
in large distributed clusters and provides fault tolerance and high availability.
These aspects allow it to be used in large complex systems to attain high availability and synchronization for resilient operations. In these complex distributed
systems, Zookeeper can be viewed as a centralized repository where distributed
applications read and write data. It is used to keep the distributed application
functioning together as a single unit by making use of its synchronization, serialization and coordination abilities.
Etcd: Etcd [1] is a strongly consistent distributed reliable key-value store
that is simple, secure and fast. It provides a reliable way to store data that
needs to be accessed by a distributed system to provide consistent cluster coordination and state management. The name etcd is derived from distributing the
Unix “/etc” directory used for global configurations. It gracefully handles leader
elections and can tolerate machine failure, even in the leader node. The leaders
in etcd handle all client requests needing consensus. Requests like reading can
be handled by any cluster node. The leader accepts, replicates and commits the
new changes after the followers verify the receipt.
Etcd uses the raft protocol to maintain the logs of state-changing events. It
uses full replication, i.e. the entire data is available on every node, making it
highly available. This also makes it possible that any node can act as a leader.
The applications can read and write data to etcd and it can be used for storing
database connection details, or feature flags. These values can be watched and
allow the applications to reconfigure themselves when values change. In addition,
etcd consistency is used to implement leader election or distributed locking. etcd
is used as the coordinating mechanism for Kubernetes and Cloud Foundry. It
is also used in production environments by AWS, Google Cloud Platform and
Azure.
Chapter 3 Big Data Outlook, Tools, and Architectures 47
Resource Management. The big data platform works on top of a set of distributed computing and memory resources. The resource manager performs the
task of resource allocation in terms of CPU time and memory usage. In a cluster,
multiple applications are usually deployed at one time, e.g. it is common to have
a distributed application like Apache Spark running in parallel to a distributed
database for storage like Apache Hbase in the same cluster. A resource manager is an authority that arbitrates resources among all the applications in the
system. In addition, the resource manager is also responsible for job scheduling
with the help of a scheduler, or an application master.
YARN: Yet another resource manager (YARN) [444] is an important integral part of the Hadoop ecosystem and mainly supports Hadoop workloads.
In YARN, the application-level resource manager is a dedicated scheduler that
runs on the master daemon and assigns resources to the requesting applications.
It keeps a global view of all resources in the cluster and handles the resource
requests by scheduling the request and assigning the resources to the requesting
application. It is a critical component in the Hadoop cluster and runs on a dedicated master node. The resource manager has two components: a scheduler and
an application manager. The application manager receives the job-submissions,
looks for the container to execute the ApplicationMaster and helps in restarting
the ApplicationMaster on another node in case of failure. The ApplicationMaster
is created for each application and it is responsible for the allocation of appropriate resources from the scheduler, tracking their status and monitoring their
progress. ApplicationMaster works together with the Node Manager. The Node
manager runs on slave daemon and is responsible for the execution of tasks on
each node. It monitors their resource usage and reports it to the ResourceManager. The focus of YARN on one aspect at a time enables YARN to be scalable,
generic and makes it able to support multi-tenant cluster. The High available
version of Yarn uses Zookeeper to establish automatic failover.
Mesos: Apache Mesos is an open-source cluster manager [233] that handles
workloads in a distributed environment through dynamic resource sharing and
isolation. It is also called a distributed systems kernel. Mesos works between the
application layer and the operating system and makes it easier to manage and
deploy applications in large distributed clusters by doing resource management.
It turns a cluster into a single large pool of resources by leveraging the features
of modern kernels of resource isolation, prioritization, limiting, and accounting,
at a higher level of abstraction. Mesos also uses zookeeper to achieve high availability and recovery from master failure. Mesos carries out microscale resource
management as it works as a microkernel.
Data Flow: Message Passing. Message passing is crucial to distributed big
data applications that must deal with real-time data. This data could be event
logs, user activities, sensor signals, stock exchanges, bank transactions, among
many others. Efficient and fault free ingestion of this real-time data is critical
for real-time applications. Message passing solutions are needed for real-time
streaming applications and data flows.
48 H. Jabeen
Message passing tools, as the name suggests, assist in communication between
the software components of a big data processing pipeline. The systems usually decouple the sender and receiver by using a message broker that hides the
implementation details like the operating system or network interface from the
application interfaces. This creates a common platform for messaging that is also
easy to develop for the developers. The applications of message passing pipelines
are website activity monitoring, metrics collection, log aggregation etc. Below
we briefly discuss Apache Kafka, which is frequently used in real-time big data
applications.
Apache Kafka: Apache Kafka [147] is a distributed messaging system that
uses the publish-subscribe mechanism. It was developed to support continuous
and resilient messaging with high throughput at LinkedIn. Kafka is a fast, scalable, durable, and fault-tolerant system. It maintains feeds of messages in categories called topics. These topics are used to store messages from the producers
and deliver them to the consumers who have subscribed to that topic.
Kafka is a durable, high volume message broker that enables applications
to process, persist and re-process streaming data. Kafka has a straightforward
routing approach that uses a routing key to send messages to a topic. Kafka
offers much higher performance than message brokers like RabbitMQ. Its boosted
performance makes it suitable to achieve high throughput (millions of messages
per second) with limited resources.
Data Handling. The data handling and acquisition assists in collecting, selecting, filtering and cleaning the data being received and generated. This data can
be later stored in a data warehouse, or another storage solution, where further
processing can be performed for gaining the insights.
Apache Flume: Apache Flume [198] is a framework to collect massive
amounts of streaming event data from multiple sources, aggregate it, and move
it into HDFS. It is used for collecting, aggregating, and moving large amounts
of streaming data such as log files, events from various sources like network
traffic, social media, email messages etc. to HDFS. Flume provides reliable message delivery. The transactions in Flume are channel-based where two transactions (one sender and one receiver) are maintained for each message. If the read
rate exceeds the write rate, Flume provides a steady flow of data between read
and write operations. Flume allows ingestion of data from multiple servers (and
sources) into Hadoop.
Apache Sqoop: Most of the older companies have stored their data on
RDBMS, but with the increase in data sizes beyond terabytes, it is important
to switch to HDFS. Apache Sqoop [428] is a tool designed to transfer bulk
data between structured data stores such as RDBMS and Hadoop in an efficient
manner. Sqoop imports data from external datastores into HDFS and vice versa.
It can also be used to populate tables in Hive and HBase. Sqoop uses a connectorbased architecture which supports plugins providing smooth connectivity to the
external systems.
Chapter 3 Big Data Outlook, Tools, and Architectures 49
Data Processing. Data-flow processing technologies are mainly categorised
into batch (historical data) processing systems and stream (real-time) processing
systems.
Batch processing systems are high throughput systems for processing high
volumes of data collected over some time. The data is collected, entered, processed and then the batch results generated resulting in high latency systems.
Stream processing systems are high throughput i.e. the system continuously
receives data that is under constant change (e.g. traffic control, sensor data,
social media), low latency stream processing systems. The data is processed
on the fly and produces real-time insights. There are three main methods for
streaming: At least once, At most once, and Exactly once.
Until a few years ago, a clear distinction between these two processing systems existed. However, recent technologies such as Apache Spark and Apache
Flink can handle both kinds of processing, diminishing this distinction. We will
discuss some of the key technologies in the following sections.
Hadoop MapReduce: Hadoop is a platform for distributed storage and
analysis of very large data sets. It has four main modules: Hadoop Common,
HDFS, MapReduce and YARN [153]. MapReduce is the distributed data processing engine of Hadoop. It is a programming model and provides a software framework to write the applications for distributed processing of very large amounts
of data in parallel. MapReduce processes the data in two phases: The map phase
and the reduce phase. In the map phase, the framework reads data from HDFS.
Each dataset is called an input record and split into independent chunks that are
processed by the map tasks in parallel. In the reduce phase, the results from the
map phase are processed and stored. The storage target can either be a database
or back HDFS or something else. Working with MapReduce requires a low level
and specialised design thinking and programming models, making it challenging
for developers to create generic applications. As a result, many tools have been
developed around Hadoop MapReduce to address these limitations. These tools
include:
Apache Pig: This provides a high-level language for expressing data analysis
programs that can be executed in MapReduce [150]. The platform was developed by Yahoo. The developers can write programs for data manipulation and
transformation as data flow sequences using Pig Latin language. These programs are easy to write, understand, and maintain. In addition, Apache Pig
offers an infrastructure to evaluate and optimize these programs automatically.
This allows developers to focus more on semantics and productivity. Apache Pig
can execute its jobs in Apache Tez, or Apache Spark (covered in the following
sections).
Apache Hive: This offers a higher-level API to facilitate reading, writing, and
managing large datasets [203] residing in distributed storage (e.g. HDFS) using
SQL-like queries in a custom query language, called HiveQL. Implicitly, each
query is translated into MapReduce commands.
Apache Mahout: This is a machine learning library [337] developed to be used
with MapReduce. It provides an API for distributed or scalable machine learn-
50 H. Jabeen
ing algorithms mostly focusing on linear algebra. It provides algorithms like
classification, likelihood estimation, and clustering. All algorithms are implicitly
transformed into MapReduce jobs.
Apache Spark: Apache Spark is a generic, in-memory data processing
engine [480]. It provides high-level APIs in Java, Python and Scala. Apache
Spark has simplified the programming complexity by introducing the abstraction
of Resilient Distributed Datasets (RDD), i.e. a logical collection of data partitioned across machines. The rich API for RDDs manipulation follows the models
for processing local collections of data, making it easier to develop complex programs. Spark provides higher-level constructs and libraries to further facilitate
users in writing distributed applications. At the time of writing, Apache Spark
provides four libraries:
Spark SQL - Offers support for SQL querying of data stored in RDDs, or an
external data source. It allows structured data processing using high-level collections named dataset and data frame. A Dataset is a distributed collection of
data and a DataFrame is a Dataset organized into named columns. It is conceptually similar to a table in a relational database. The DataFrames can be
constructed in numerous different ways like reading from structured data files,
tables in Hive, external databases, or existing RDDs.
Spark streaming - Spark implements stream processing by ingesting data in minibatches. Spark streaming makes it easy to build scalable fault-tolerant real-time
applications. The data can be ingested from a variety of streaming sources like
Kafka, Flume (covered in earlier sections). This data can be processed using
complex real-time algorithms using a high-level API.
MLlib Machine Learning Library - Provides scalable machine learning algorithms. It provides common algorithms for classification, regression, clustering,
algorithms for feature extraction, feature selection and dimensionality reduction,
high-level API for machine learning pipelines, saving and loading algorithms, and
utilities for linear algebra and statistics.
GraphX - Provides a distributed graph processing using graph-parallel computation. GraphX extends the Spark RDD by introducing “Graph”: a directed
multigraph with properties attached to each vertex and edge. GraphX comes
with a variety of graph operators like subgraph, joinVertices, or algorithms like
pageRank, ConnectedComponents, and several graph builders that allow building a graph from a collection of vertices and edges from RDD or other data
sources.
Apache Flink: Apache Flink is a true distributed streaming data-flow
engine [69] and offers a unified stream and batch processing. It treats batch
processing as a special case of streaming with bounded data. The APIs offered
by Flink are similar but the implementation is different. Flink executes arbitrary dataflow programs in a data-parallel and pipelined manner. It offers a
complete software stack of libraries using building blocks, exposed as abstract
data types, for streams (DataStream API), for finite sets (DataSet API) and
for relational data processing (relational APIs - the Table API and SQL). The
high-level libraries offered by Apache Flink are:
Chapter 3 Big Data Outlook, Tools, and Architectures 51
Gelly: Flink Graph - provides methods and utilities to simplify the development
of graph analysis applications in Flink. The graphs can be transformed and
modified using high-level functions similar to the ones provided by the batch
processing API. Gelly provides graph algorithms like pageRank, communityDetection, connectedComponents, or shortestPath finding.
Machine Learning: FlinkML is a machine learning library aimed to provide a
list of machine learning algorithms. At the moment, it has been temporarily
deprecated in Apache Flink 1.9.0 for the sake of developing ML core and ML
pipeline interfaces using high-level APIs.
FlinkCEP: Complex event processing for Flink - Allows detection of event patterns in the incoming stream.
State Processor API - provides functionality to read, write, and modify save
points and checkpoints using DataSet API. It also allows using relational Table
API or SQL queries to analyze and process state data.
Data Storage: Distributed File Systems. Distributed file systems allow
access to the files from multiple hosts, in addition to distributing the storage of
large files over multiple machines. Such systems mostly provide the interfaces
and semantics, similar to the existing local files systems, while the distributed
file system handles the network communication, data movement and distributed
directories seamlessly.
Hadoop Distributed File System (HDFS): HDFS, the main component
of the Hadoop ecosystem, has become the de facto standard for distributed file
systems [62]. It is known as the most reliable storage system. HDFS is designed
to run on commodity hardware, making it more popular for its cost-effectiveness.
In addition to working with the conventional file management commands (e.g. ls,
rm, mkdir, tail, copy, etc), HDFS also works with a REST API that complies with
the FileSystem/FileContext interface for HDFS. HDFS architecture is designed
to store very large files and does not suit models with large numbers of small
files. The files are split into blocks which are then distributed and replicated
across the nodes for fault-tolerance. HDFS stores data reliably, even in the case
of hardware failure. HDFS provides parallel access to data, resulting in high
throughput access to application data.
Data Storage and Querying. RDBMS and SQL have remained the main
choice for data storage and management for organizations for years. Gradually,
the main strength of RDBMS technology (the fixed schema design) has turned
into its fundamental weakness in the era of big and heterogeneous data. Today’s
data appears in structured and unstructured forms and originates from a variety
of sources such as emails, log files, social media, sensor events etc. Besides, high
volumes of data are being generated and are subject to high rates of change. On
the other hand, one of the key characteristics of big data applications is that
they demand real-time responses, i.e. data needs to be stored, such that it can
be accessed quickly when required. The non-conventional, relatively new NoSQL
(not only SQL) stores are designed to efficiently and effectively tackle these big
52 H. Jabeen
data requirements. Not only do these stores support dynamic schema design
but they also offer increased flexibility, scalability and customization compared
to relational databases. These stores are built to support distributed environments, with the ability to scale horizontally by adding new nodes as demand
increases. Consistent with the CAP theorem (which states that distributed systems can only guarantee at most two properties from Consistency, Availability
and Partition tolerance), NoSQL stores compromise consistency in favour of
high availability and scalability. Generally, NoSQL stores support flexible data
models, provide simple interfaces and use weak consistency models by abandoning the ACID (Atomicity, Consistency, Isolation, and Durability) transactions in favour of BASE (Basically Available, Soft state, Eventually Consistent) transaction models. Based on the data models supported by these systems,
NoSQL databases can be categorised into four groups, i.e. key-value stores, document stores, column-oriented stores and graph databases. The following section
describes these NoSQL database models in further detail and lists a few examples
of the technologies per model.
Key-Value Stores: Key-value stores can be categorised as the simplest
NoSQL databases. These stores are designed for storing schema-free data as
Key-Value pairs. The keys are the unique IDs for the data, and they can also
work as indexes for accessing the data. The Values contain the actual data in
the form of attributes or complex objects. All the values may not share the same
structure.
Examples: Redis, Riak KV, Amazon DynamoDB, Memcached, Microsoft
Azure Cosmos DB, and etcd.
Document Stores: Document stores are built upon the idea of key-value
stores. They pair each key with a complex data structure described as a document. These documents may contain different key-value pairs, key-array pairs or
even nested documents. The document stores are designed for storing, retrieving and managing document-oriented information, also known as semi-structured
data. There is no schema that all documents must adhere to as in the case for
records in relational databases. Each document is assigned a unique key, which
is used to retrieve the document. However, it is possible to access documents
by querying their internal structure, e.g searching for a field with the specified value. The capability of the query interface is typically dependent on the
encoding format like XML or JSON.
Examples: CouchDB, MongoDB
Column-Oriented Stores: Column-oriented stores are also known as widecolumn stores and extensible record stores. They store each column continuously,
i.e. on disk or in-memory each column is stored in sequential blocks. Instead of
storing data in rows, these databases are designed for storing data tables as
sections of columns of data. Therefore, these stores enable faster column-based
analytics compared to traditional row-oriented databases.
Examples: Apache HBase, Cassandra
Chapter 3 Big Data Outlook, Tools, and Architectures 53
4 Harnessing Big Data as Knowledge Graphs
Today, the term big data is potentially misleading as the size is only one of many
important aspects of the data. The word big promotes the misconception that
more data means good data and stronger insights. However, it is important to
realise that data volume alone is not sufficient to get good answers. The ways we
distribute, organize, integrate, and represent the data matters as much as, if not
more than, the size of the data. In this section, we briefly cover the variety or the
heterogeneity of the data and the possibility of organizing this data as a graph.
Organizing the data as a graph has several advantages compared to alternatives
like database models. Graphs provide a more intuitive and succinct abstraction
for the knowledge in most of the domains. Graphs encode the entities as nodes,
and their relationships as edges between entities. For example, in social interactions the edges could represent friendship, co-authorship, co-worker-ship, or
other types of relations, whereas people are represented as the nodes. Graphs
have the ability to encode flexible, incomplete, schema-agnostic information that
is typically not possible in the relational scenario. Many graph query languages
cannot only support standard operations like joins but also support specialised
operators like arbitrary path-finding. At the same time, formal knowledge representation (based on Ontologies) formats could also be used to create Graphs in
a semantically coherent and structured representation (RDF, RDFS). The term
knowledge graph was popularised in 2012 by Google with the slogan “things
not strings” with an argument that the strings can be ambiguous but in the
Knowledge Graphs, the entities (the nodes in a Knowledge Graph) can be disambiguated more easily by exploiting their relationships (edges/properties) with
other entities. Numerous definitions of Knowledge Graphs have been proposed
in the literature, and a recent and generic definition portrays the “knowledge
graph as a graph of data intended to accumulate and convey knowledge of the
real world, whose nodes represent entities of interest and whose edges represent
relations between these entities” [199]. A high number of public, open, crossdomain knowledge graphs have been created and published online. Examples
include DBPedia, Wikidata or YAGO, which are either created by the community or extract knowledge from Wikipedia. Domain dependent open knowledge
graphs have also been published covering areas like geography, life sciences, and
tourism. At the same time, numerous enterprise knowledge graphs (mostly inhouse) are created by e.g. IBM, Amazon, Facebook, LinkedIn and many others.
The creation of these knowledge graphs mainly involves three methods.
Manual Curation e.g. Cyc, Wikidata, Freebase etc.
Creation using Semi-structured sources e.g. Wikipedia (from Wikipedia
infoboxes), YAGO (WordNet, Wikipedia etc.) BableNet etc.
Creation from Unstructured Sources e.g. NELL (free text), WebIsA (free
text)
As briefly discussed above, such graphs could be created schema-agnostically,
as well as using a formal ontology that defines the set of concepts and categories
54 H. Jabeen
in a given domain alongside their properties and the relations. The knowledge
contained in the knowledge graphs can be characterized around two main dimensions: a) Coverage of a single domain, which can be defined by the number of
Instances. The instances depict the details covered in a given knowledge graph
in a particular area, and more instances mean more details. Coverage could further be defined by the number of assertions, i.e. the relationships contained in
the graph. Also, the link degree (average, median) can also assist in estimation
of the coverage of the graph. For b) Knowledge Coverage (multiple domains),
one can consider the number of classes in the schema, the number of relations,
the class hierarchy (depth and width), or the complexity of schema can help in
assessing the breadth and depth of the knowledge covered by a given knowledge
graph. In practice, the graphs can differ in their sizes in orders of magnitude,
but the complexity (linkage) of smaller graphs could still be higher. Similarly,
the underlying schema could either be simple or rather deep and detailed. The
number of instances per class could vary; on the contrary, there could be fewer
instances per class, covering more classes in total. In conclusion, the knowledge
graphs differ strongly in size, coverage, and level of detail.
4.1 Graph Stores
In order to handle large sizes of this relatively new-hyped knowledge representation format, several tools have been created which can be categorised into two
types, one more general and simple, like graphs, and other relatively formal for
RDF data named as Triple Stores.
Graph Databases. Graph databases are based on graph theory and store
data in graph structures using nodes and edges connecting each other through
relations. These databases are designed for data containing elements which are
interconnected, with an undetermined number of relations between them. Graph
databases usually provide index-free adjacency, i.e. every element contains a
direct pointer to its adjacent elements and no index lookups are necessary. Examples: Neo4J, FlockDB, HyperGraphDB.
Triple Stores. Triple stores are database management systems for the data
modelled using RDF. RDF data can be thought of as a directed labelled graph
wherein the arcs start with subject URIs, are labelled with predicate URIs, and
end up pointing to object URIs or scalar values. This RDF data can be queried
using SPARQL query language. Triple stores can be classified into three categories: Native triple stores - Triple stores implemented from scratch exploiting
the RDF data model to efficiently store and access the RDF data. Examples:
Stardog, Sesame, OWLIM RDBMS-backed triple stores - Triple stores built by
adding an RDF specific layer to an existing RDBMS. Example: OpenLink Virtuoso NoSQL triple stores - Triple stores built by adding an RDF specific layer to
existing NoSQL databases. Example: CumulusRDF (built on top of Cassandra).
Chapter 3 Big Data Outlook, Tools, and Architectures 55
Efficient handling of large-scale knowledge graphs requires the use of distributed file systems, distributed data stores, and partitioning strategies. Apart
for several centralised systems, many recent graph processing systems have been
built using existing distributed frameworks, e.g. Jena-HBase [241] and H2RDF
[341], H2RDF+ [342] make use of HBase, Rya [363] makes use of Accumulo,
D-SPARQ [320] works using MongoDB. S2RDF [385], S2X [384], SPARQLGX
[168] and SparkRDF [78] handle RDF data using Apache Spark. The main idea
behind representing data as a graph is not only querying the data, but also
efficient knowledge retrieval including reasoning, knowledge base completion,
enrichment (from other sources), entity linking and disambiguation, path mining, and many other forms of analytics. It can be seen from many recent surveys
[192,235,473] that several systems have been proposed in the literature to deal
with one or a few of the many aspects of large-scale knowledge graph processing.
It is important to realize this gap and the need for a scalable framework that
caters for different tasks for large-scale knowledge graphs.
5 Conclusion
This chapter connects the term big data and knowledge graphs. The first section
of this chapter provides an overview of big data, its major enabling technologies,
the key characteristics of big data, the challenges that it poses, and the necessary
activities to create a big data value chain. In the second section, we cover the
big data architectures and provide a taxonomy of big data processing engines.
In the last section, we connect the big data with large-scale knowledge graphs
covered in Chap. 1 and Chap. 2 of this book. We discuss a few key technologies
and cover the possibilities and key challenges to harness large-scale knowledge
graphs.
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.

Chapter 4
Creation of Knowledge Graphs
Anastasia Dimou(B)
Department of Electronics and Information Systems,
Ghent University, Ghent, Belgium
anastasia.dimou@ugent.be
Abstract. This chapter introduces how Knowledge Graphs are generated. The goal is to gain an overview of different approaches that were
proposed and find out more details about the current prevalent ones.
After reading this chapter, the reader should have an understanding
of the different solutions available to generate Knowledge Graphs and
should be able to choose the mapping language that best suits a certain
use case.
1 Introduction
The real power of the Semantic Web will be realized once a significant number of
software agents requiring information from different heterogeneous data sources
become available. However, human and machine agents still have limited ability
to interact with heterogeneous data as most data is not available in the form
of knowledge graphs, which are the fundamental cornerstone of the Semantic
Web. They have different structures (e.g., tabular, hierarchical), appear in heterogeneous formats (e.g., CSV, XML, JSON) and are accessed via heterogeneous
interfaces (e.g., database interfaces or Web APIs).
Therefore, different approaches were proposed to generate knowledge graphs
from existing data. In the beginning, custom implementations were proposed
[67,292] and they remain prevalent today [71,177]; however, more generic
approaches emerged as well. Such approaches were originally focused on
data with specific formats, namely dedicated approaches for, e.g., relational
databases [93], data in Excel (e.g. [274]), or in XML format (e.g. [272]). However, data owners who hold data in different formats need to learn and maintain
several tools [111].
To deal with this, different approaches were proposed for integrating heterogeneous data sources while generating knowledge graphs. Those approaches
follow different directions, but detaching the rules definition from their execution
prevailed, because they render the rules interoperable between implementations,
whilst the systems that process those rules are use-case independent. To generate
knowledge graphs, on the one hand, dedicated mapping languages were proposed,
e.g., RML [111], and, on the other hand, existing languages for other tasks were
repurposed as mapping languages, e.g., SPARQL-Generate [278].
c The Author(s) 2020
V. Janev et al. (Eds.): Knowledge Graphs and Big Data Processing, LNCS 12072, pp. 59–72, 2020.
https://doi.org/10.1007/978-3-030-53199-7_4
60 A. Dimou
We focus on dedicated mapping languages. The most prevalent dedicated
mapping languages are extensions of R2RML [97], the W3C recommendation
on knowledge graph generation from relational databases. RML was the first
language proposed as an extension of R2RML, but there are more alternative approaches and extensions beyond the originally proposed language. For
instance, xR2RML [305], for generating knowledge graphs from heterogeneous
databases, and KR2RML [407], for generating knowledge graphs from heterogeneous data.
In the remainder of this chapter, we introduce the Relational to RDF Mapping Language (R2RML) [97] and the RDF Mapping Language (RML) [111]
which was the first mapping language extending R2RML to support other heterogeneous formats. Then we discuss other mapping languages which extended
or complemented R2RML and RML, or their combination.
2 R2RML
The Relational to RDF Mapping Language (R2RML) [97] is the W3C recommendation to express customized mapping rules from data in relational databases to
generate knowledge graphs represented using the Resource Description Framework (RDF) [94]. R2RML considers any custom target semantic schema which
might be a combination of vocabularies. The R2RML vocabulary namespace is
http://www.w3.org/ns/r2rml# and the preferred prefix is r2rml.
In R2RML, RDF triples are generated from the original data in the relational
database based on one or more Triples Maps (rr:TriplesMap, Listing 4.1, line 3).
Each Triples Map refers to a Logical Table (rr:LogicalTable, line 4), specified
by its table name (rr:tableName). A Logical Table (rr:LogicalTable) is either
a SQL base table or view, or an R2RML view. An R2RML view is a logical table
whose contents are the result of executing a SQL query against the input database.
The SQL query result is used to generate the RDF triples (Table 1).
Table 1. Results of female pole vault for 2019 world championship
Rank Name Nationality Mark Notes
1 Anzhelika Sidorova Authorized Neutral Athlete 4.95 WL,PB
2 Sandi Morris United States (USA) 4.90 SB
3 Katerina Stefanidi Greece 4.85 SB
4 Holly Bradshaw Great Britain 4.80
5 Alysha Newman Canada 4.80
6 Angelica Bengtsson Sweden 4.80 NR
1 @prefix rr: <http://www.w3.org/ns/r2rml#>.
2
3 <#FemalePoleVault> rr:logicalTable <#PoleVaultersDBtable> .
4 <#PoleVaultersDBtable> rr:tableName "femalePoleVaulters" .
Listing 4.1. A Triples Map refers to a Logical Table specified by its name
Chapter 4 Creation of Knowledge Graphs 61
A Triples Map defines how an RDF triple is generated. It consists of three
parts: (i) one Logical Table (rr:LogicalTable, Listing 4.1), (ii) one Subject Map
(rr:SubjectMap, Listing 4.2, line 2), and (iii) zero or more Predicate-Object Maps
(rr:PredicateObjectMap, Listing 4.2, lines 3 and 4).
1 # Triples Map
2 <#FemalePoleVault> rr:subjectMap <#Person_SM> ;
3 rr:predicateObjectMap <#Mark_POM> ;
4 rr:predicateObjectMap <#Nationality_POM> .
Listing 4.2. A Triples Map consists of one Logical Table one Subject Map and zero
or more Predicate Object Maps
The Subject Map (rr:SubjectMap, Listing 4.3, line 2) defines how unique
identifiers, using IRIs [118] or blank nodes, are generated. The RDF term generated from the Subject Map constitutes the subject of all RDF triples generated
from the Triples Map that the Subject Map is related to.
A Predicate-Object Map (rr:PredicateObjectMap, Listing 4.3, lines 5 and
10) consists of (i) one or more Predicate Maps (rr:PredicateMap, line 5), and
(ii) one or more Object Maps (rr:ObjectMap, line 6) or Referencing Object Maps
(rr:ReferencingObjectMap, line 11).
1 # Subject Map
2 <#Person_SM>. rr:template "http:://ex.com/person/{name}"
3
4 # Predicate Object Map with Object Map
5 <#Mark_POM> rr:predicate ex:score ;
6 rr:objectMap [ rr:column "Mark" ;
7 rr:language "en" ] .
8
9 # Predicate Object Map with Referencing Object Map
10 <#Nationality_POM> rr:predicateMap <#Country_PM> ;
11 rr:objectMap <#Country_ROM> ;
Listing 4.3. A Predicate Object Map consists of one or more Predicate Maps and one
or more Object Maps or Referencing Object Maps
A Predicate Map (rr:PredicateMap, Listing 4.3, lines 5 and 10) is a Term
Map (rr:TermMap) defining how a triple’s predicate is generated. An Object Map
(rr:ObjectMap, line 6) or Referencing Object Map (rr:ReferencingObjectMap,
Listing 4.4, line 11) defines how a triple’s object is generated.
A Referencing Object Map defines how the object is generated based on
the Subject Map of another Triples Map. If the Triples Maps refer to different
Logical Tables, a join between the Logical Tables is required. The join condition
(rr:joinCondition, Listing 4.4, line 3) performs joins as joins are executed in
SQL. The join condition consists of a reference to a column name that exists in
the Logical Table of the Triples Map that contains the Referencing Object Map
(rr:child, line 4) and a reference to a column name that exists in the Logical
Table of the Referencing Object Map’s Parent Triples Map (rr:parent, line 5).
62 A. Dimou
1 # Referencing Object Map
2 <#Country_ROM> rr:parentTriplesMap <#Country_TM> ;
3 rr:join [
4 rr:cild "nationality" ;
5 rr:parent "country_name"] .
6
7 <#Country_TM> rr:logicalTable [ rr:tableName "country" ];
8 rr:subjectMap rr:template "http://ex.com/country/{country_name}" .
Listing 4.4. A Referencing Object Map generates an object based on the Subject Map
of another Triples Map
A Term Map (rr:TermMap) defines how an RDF term (an IRI, a blank node,
or a literal) is generated and it can be constant-, column- or template-valued.
A constant-valued Term Map (rr:constant, Listing 4.3, line 5) always generates the same RDF term which is by default an IRI.
A column-valued term map (rr:column, Listing 4.3, line 6) generates a literal by default that is a column in a given Logical Table’s row. The language
(rr:language, line 7) and datatype (rr:datatype) may be optionally defined.
A template-valued Term Map (rr:template, Listing 4.3, line 8) is a valid
string template containing referenced columns and generates an IRI by default.
If the default termtype is desired to be changed, the term type (rr:termType)
needs to be defined explicitly (rr:IRI, rr:Literal, rr:BlankNode).
3 RML
The RDF Mapping Language (RML) [110,111] expresses customized mapping
rules from heterogeneous data structures, formats and serializations to RDF.
RML is a superset of R2RML, aiming to extend its applicability and broaden its
scope, adding support for heterogeneous data. RML keeps the mapping rules as
in R2RML but excludes its database-specific references from the core model. This
way, the input data that is limited to a certain database in R2RML (because
each R2RML processor may be associated to only one database), becomes a
broad set of one or more input data sources in RML.
RML provides a generic way of defining mapping rules referring to different
data structures, combined with case-specific extensions, but remains backwards
compatible with R2RML, as relational databases form such a specific case. RML
enables mapping rules defining how a knowledge graph is generated from a set of
sources that altogether describe a certain domain, can be defined in a combined
and uniform way. The mapping rules may be re-used across different sources
describing the same domain to incrementally form well-integrated datasets.
The RML vocabulary namespace is http://semweb.mmlab.be/ns/rml# and
the preferred prefix is rml.
In the remainder of this subsection, we will talk in more details about data
retrieval and transformations in RML, as well as other representations of RML.
3.1 Data Retrieval
Data can originally (i) reside on diverse locations, e.g., files or databases
on the local network, or published on the Web; (ii) be accessed using different
Chapter 4 Creation of Knowledge Graphs 63
interfaces, e.g., raw files, database connectivity for databases, or different interfaces from the Web such as Web APIs; and (iii) have heterogeneous structures and formats, e.g., tabular, such as databases or CSV files, hierarchical,
such as XML or JSON format, or semi-structured, such as HTML.
In this section, we explain how RML performs the retrieval and extraction
steps required to obtain the data whose semantic representation is desired.
Logical Source. RML’s Logical Source (rml:LogicalSource, Listing 4.5) extends
R2RML’s Logical Table and determines the data source with the data to generate the knowledge graph. The R2RML Logical Table definition determines a
database table, using the Table Name (rr:tableName). In the case of RML,
a broader reference to any input source is required. Thus, the Logical Source
(rml:source) is introduced to specify the source with the original data.
For instance, if the data about countries were in an XML file, instead of a
Logical Table, we would have a Logical Source <#PoleVaultersXML> (Listing 4.5,
line 3):
1 @prefix rml: <http://semweb.mmlab.be/ns/rml#>.
2
3 <#Countries> rml:logicalSource <#CountriesXML> ;
4 <#CountriesXML> rml:source <http://rml.io/data/lambda/countries.xml> .
Listing 4.5. A Triples Map refers to a Logical Source whose data is in XML format
The countries data can then be in XML format as below:
1 <countries>
2 <country continent="Europe">
3 <country_abb>GR</country_abb>
4 <country_name country_language="en">Greece</country_name>
5 <country_name country_language="nl">Griekenland</country_name>
6 </country>
7 <country continent="Europe">
8 <country_abb>UK</country_abb>
9 <country_name country_language="en">United Kingdom</country_name>
10 <country_name country_language="nl">Verenigd Koninkrijk</country_name>
11 </country>
12 <country continent="America">
13 <country_abb>CA</country_abb>
14 <country_name country_language="en">Canada</country_name>
15 <country_name country_language="nl">Canada</country_name>
16 </country>
17 ...
18 </countries>
Listing 4.6. Country data in XML format
Reference Formulation. RML deals with different data serialisations which use
different ways to refer to data fractions. Thus, a dedicated way of referring to
the data’s fractions is considered, while the mapping definitions that define how
the RDF terms and triples are generated remain generic. RML considers that
any reference to the Logical Source should be defined in a form relevant to the
input data, e.g. XPath for XML data or JSONPath for JSON data. To this end,
the Reference Formulation (rml:referenceFormulation) declaration is introduced
(Listing 4.7, line 4), indicating the formulation (for instance, a standard, query
language or grammar) used to refer to its data.
64 A. Dimou
1 @prefix rml: <http://semweb.mmlab.be/ns/rml#>.
2
3 <#Countries> rml:logicalSource <#CountriesXML> .
4 <#CountriesXML> rml:referenceFormulation ql:XPath .
5 <#CountriesXML> rml:iterator "/countries/country" .
Listing 4.7. A Logical Source specifies its Reference Formulation and iterator
Iterator. While in R2RML it is already known that a per-row iteration occurs, as
RML remains generic, the iteration pattern, if any, cannot always be implicitly
assumed, but it needs to be determined. Thereafter, the iterator (rml:iterator)
is introduced (Listing 4.7, line 5). The iterator determines the iteration pattern
over the data source and specifies the extract of the data during each iteration.
The iterator is not required to be explicitly mentioned in the case of tabular
data sources, as the default per-row iteration is implied.
Source. Data can originally reside on diverse, distributed locations and be
accessed using different access interfaces [112]. Data can reside locally, e.g., in
files or in a database at the local network, or can be published on the Web. Data
can be accessed using diverse interfaces. For instance, metadata may describe
how to access the data, such as dataset’s metadata descriptions in the case of
data catalogues, or dedicated access interfaces might be needed to retrieve data
from a repository, such as database connectivity for databases, or different Web
interfaces, such as Web APIs.
RML considers an original data source, but the way this input is retrieved
remains out of scope, in the same way it remains out of scope for R2RML how
the SQL connection is established. Corresponding vocabularies can describe how
to access the data, for instance the dataset’s metadata (Listing 4.8), hypermediadriven Web APIs or services, SPARQL services, and database connectivity
frameworks (Listing 4.9) [112].
1 <#FemalePoleVault> rr:logicalTable <#PoleVaultersCSVtable> ;
2 <#PoleVaultersCSVtable> rml:source <#CSVW_source> .
3
4 <#CSVW_source> a csvw:Table;
5 csvw:url "femalePoleVaulters.csv" ;
6 csvw:dialect [ a csvw:Dialect; csvw:delimiter ";" ] .
Listing 4.8. A CSV file on the Web as RML Data Source
1 <#FemalePoleVault> rr:logicalTable <#PoleVaultersDBtable> ;
2 <#PoleVaultersDBtable> rml:source <#DB_source>;
3 rr:sqlVersion rr:SQL2008;
4 rr:tableName "femalePoleVaulters" .
5
6 <#DB_source> a d2rq:Database;
7 d2rq:jdbcDSN "CONNECTIONDSN";
8 d2rq:jdbcDriver "com.mysql.cj.jdbc.Driver";
9 d2rq:username "root";
10 d2rq:password "" .
Listing 4.9. A table as RML Data Source
Chapter 4 Creation of Knowledge Graphs 65
Logical Reference. According to R2RML, a column-valued or template-valued
term map is defined as referring to a column name. In the case of RML, a more
generic notion is introduced, the logical reference. Its value must be a valid
reference to the data of the input dataset according to the specified reference
formulation. Thus, the reference’s value should be a valid expression according
to the Reference Formulation defined at the Logical Source.
1 # Predicate Object Map with Object Map
2 <#CountryName_POM> rr:predicate ex:name ;
3 rr:objectMap [
4 rml:reference "country_name" ;
5 rml:languageMap [ rml:reference "@country_language"] ] .
Listing 4.10. An Object Map in RML with a reference to data according to the
Reference Formulation and a language Map to define the language.
RDF Term Maps are instantiated with data fractions referred to using a
reference formulation relevant to the corresponding data format. Those fractions
are derived from data extracted at a certain iteration from a Logical Source. Such
a Logical Source is formed by data retrieved from a repository accessed as defined
by the corresponding dataset or service description vocabulary.
Language Map. RML introduces a new Term Map for defining the language, the
Language Map (rml:LanguageMap, Listing 4.10, line 5), which extends R2RML’s
language tag (rr:language). The Language Map allows not only constant values
for language but also references derived from the input data. rr:language is
considered then an abbreviation for the rml:languageMap, as rr:predicate is
for the rr:predicateMap.
3.2 Data Transformations: FnO
Mapping rules involve (re-)modeling the original data, describing how objects are
related by specifying correspondences between data in different schemas [126],
and deciding which vocabularies and ontologies to use. Data transformations, as
opposed to schema transformations that the mapping rules represent, are needed
to support any changes in the structure, representation or content of data [367],
for instance, performing string transformations or computations.
The Function Ontology (FnO) [102,104] describes functions uniformly, unambiguously, and independently of the technology that implements them. As RML
extends R2RML with respect to schema transformations, the combination of
RML with FnO extends R2RML with respect to data transformations.
A function (fno:Function) is an activity which has input parameters, output, and implements certain algorithm(s) (Listing 4.11, line 1). A parameter
(fno:Parameter) is a function’s input value (Listing 4.11, line 4). An output
(fno:Output) is the function’s output value (Listing 4.11, 5). An execution
(fno:Execution) assigns values to the parameters of a function for a certain execution. An implementation (fno:Implementation) defines the internal workings
of one or more functions.
66 A. Dimou
1 grel:string_split a fno:Function;
2 fno:name "split";
3 dcterms:description "split";
4 fno:expects (grel:string_s grel:string_sep);
5 fno:returns (grel:output_array).
Listing 4.11. A function described in FnO that splits a string
The Function Map (fnml:FunctionMap) is another Term Map, introduced
as an extension of RML, to facilitate the alignment of the two, RML and FnO.
A Function Map is generated by executing a function instead of using a constant
or a reference to the raw data values. Once the function is executed, its output
value is the term generated by this Function Map. The fnml:functionValue
property indicates which instance of a function needs to be executed to generate
an output and considering which values.
1 <#FemalePoleVault> rr:predicateObjectMap [
2 rr:predicate ex:record;
3 rr:objectMap [
4 fnml:functionValue [
5 rr:predicateObjectMap [
6 rr:predicate fno:executes ;
7 rr:objectMap [ rr:constant grel:split ] ] ;
8 rr:predicateObjectMap [
9 rr:predicate grel:string_s ;
10 rr:objectMap [ rml:reference "notes" ] ] ;
11 rr:predicateObjectMap [
12 rr:predicate grel:string_sep ;
13 rr:objectMap [ rr:constant "," ] ] ] ].
Listing 4.12. A Function Map aligns FnO with RML
3.3 Other Representations: YARRRML
YARRRML [103,196] is a human readable text-based representation for mapping rules. It is expressed in YAML [46], a widely used human-friendly data
serialization language. YARRRML can be used with both R2RML and RML.
A mapping (Listing 4.13, line 1) contains all definitions that state how subjects, predicates, and objects are generated. Each mapping definition is a keyvalue pair. The key sources (line 3) defines the set of data sources that are used
to generate the entities. Each source is added to this collection via a key-value
pair. The value is a collection with three keys: (i) the key access defines the local
or remote location of the data source; (ii) the key reference formulation defines
the reference formulation used to access the data source; and (iii) the key iterator
(conditionally required) defines the path to the different records over which to
iterate. The key subjects (line 5) defines how the subjects are generated. The key
predicateobjects (line 6) defines how combinations of predicates and objects are
generated. Below the countries example (Listing 4.6) is shown in YARRRML:
Chapter 4 Creation of Knowledge Graphs 67
1 mappings:
2 country:
3 sources:
4 - ['countries.xml~xpath', '/countries/country']
5 s: http://ex.com/$(country_abb)
6 po:
7 - [ex:name, $(country_name)]
8 - [ex:abbreviation, $(country_abb)]
Listing 4.13. A YARRRML set of mapping rules
4 [R2]RML Extensions and Alternatives
Other languages were proposed based on differentiation on (i) data retrieval
and (ii) data transformations. The table below (Table 2) summarizes the
mapping languages extensions, their prefixes and URIs. xR2RML [306] and
KR2RML [407] are the two most prominent solutions that showcase extensions and alternatives respectively for data retrieval. On the one hand, xR2RML
extends R2RML following the RML paradigm to support heterogeneous data
from non-relational databases. On the other hand, KR2RML extends R2RML
relying on the Nested Relational Model (NRM) [455] as an intermediate form
to represent data originally stored in relational databases. KR2RML also provided an alternative for data transformations, but FunUL is the most prominent
alternative to FnO.
Table 2. [R2]RML extensions, their URIs and prefixes
Language Prefix URI
R2RML rr http://www.w3.org/ns/r2rml#
RML rml http://semweb.mmlab.be/ns/rml#
xR2RML xrr http://www.i3s.unice.fr/ns/xr2rml#
FnO+RML fnml http://semweb.mmlab.be/ns/fnml#
FnO fno https://w3id.org/function/ontology#
4.1 XR2RML
xR2RML [306] was proposed in 2014 in the intersection of R2RML and RML.
xR2RML extends R2RML beyond relational databases and RML to include nonrelational databases. xR2RML extends R2RML following the RML paradigm
but is specialized for non-relational databases and, in particular, NoSQL and
XML databases. NoSQL systems have heterogeneous data models (e.g., keyvalue, document, extensible column, or graph store), as opposed to relational
databases. xR2RML assumes, as R2RML does, that a processor executing the
rules is connected to a certain database. How the connection or authentication
is established against the database is out of the language’s scope, as in R2RML.
The xR2RML vocabulary preferred prefix is xrr and the namespace is the
following: http://www.i3s.unice.fr/ns/xr2rml#.
68 A. Dimou
Data Source. Similarly to RML, an xR2RML Triples Map refers to a Logical Source (xrr:logicalSource, Listing 4.14, line 3), but similarly to R2RML,
this Logical Source can be either an xR2RML base table (xrr:sourceName, for
databases where tables exist) or an xR2RML view representing the results of
executing a query against the input database (xrr:query, line 4).
1 @prefix xrr: <http://www.i3s.unice.fr/ns/xr2rml#> .
2
3 <#CountriesXML> xrr:logicalSource [
4 xrr:query """for $i in ///countries/country return $i; """;
5 rml:iterator "//countries/country";];
6 <#CountryName_POM> rr:predicate ex:name ;
7 rr:objectMap [ xrr:reference "country_name"] .
Listing 4.14. xR2RML logical source over an XML database supporting XQuery
Iterator. xR2RML originally introduced the xrr:iterator, according to the
rml:iterator, to iterate over the results. In a later version, xR2RML converged
using the rml:iterator (Listing 4.14, line 5).
Format or Reference Formulation. In contrast to RML that considers a formulation (rml:referenceFormulation) to refer to its input data, xR2RML originally specified explicitly the format of data retrieved from the database using the
property xrr:format (Listing 4.15, line 2). For instance, RML considers XPath
or XQuery or any other formulation to refer to data in XML format, xR2RML
would refer to the format, e.g. xrr:XML. While RML allows for other kinds of
query languages to be introduced, xR2RML decides exactly which query language to use. In an effort to converge with RML, xR2RML considers optionally
a reference formulation.
1 <#FemalePoleVault> xrr:logicalSource <#PoleVaultersCSVtable> ;
2 <#PoleVaultersCSVtable> xrr:format xrr:Row .
Listing 4.15. A CSV file on the Web as xR2RML Logical Source
Reference. Similar to RML, xR2RML uses a reference (xrr:reference) to refer
to the data elements (Listing 4.14, line 7). xR2RML extends RML’s reference
to refer to data elements in data with mixed formats. xR2RML considers cases
where different formats are nested; for instance, a JSON extract is embedded in
a cell of a tabular structure. A path with mixed syntax consists of the concatenation of several path expressions separated by the slash ‘/’ character.
Collections and Containers. Several RDF terms can be generated by a Term
Map during an iteration if multiple values are returned. This can normally generate several triples, but it can also generate hierarchical values in the form of
RDF collections or containers. To achieve the latter, xR2RML extends R2RML
by introducing corresponding datatypes to support the generation of containers. xR2RML introduces new term types (rr:termType): xrr:RdfList for an
rdf:List, xrr:RdfBag for rdf:Bag, xrr:RdfSeq for rdf:Seq and xrr:RdfAlt
for rdf:Alt. All RDF terms produced by the Object Map during one triples
Chapter 4 Creation of Knowledge Graphs 69
map iteration step are then grouped as members of one term. To achieve this,
two more constructs are introduced: Nested Term Maps and Push Downs.
1 <#Countries> rr:predicateObjectMap [
2 rr:predicate ex:name;
3 rr:objectMap [
4 xrr:reference "country_name";
5 rr:termType xrr:RdfList;
6 xrr:pushDown [ xrr:reference "@continent"; xrr:as "continent" ];
7 xrr:nestedTermMap [
8 rr:template "{continent}: {country_name}" ;
9 rr:termType rr:Literal ;
10 rr:dataType xsd:string ] ].
Listing 4.16. An xrr:RdfList in xR2RML
Nested Term Map. A Nested Term Map (xrr:NestedTermMap, Listing 4.16, line 7)
accepts the same properties as a Term Map and can be used to specify a term type,
a language tag or a data type for the members of the generated RDF collection or
container.
Push Down. Within an iteration, it may be needed to access data elements higher
in the hierarchical documents in the context of hierarchical data formats, such
as XML or JSON. To deal with this, xR2RML introduces the xrr:pushDown
property (Listing 4.16, line 6).
4.2 KR2RML
KR2RML [407] extends R2RML in a different way than xR2RML. KR2RML
relies on the Nested Relational Model (NRM) as an intermediate form to represent data. The data is mapped into tables by translating it into tables and rows
where a column in a table can be either a scalar value or a nested table. Besides
the data retrieval part, KR2RML extends R2RML with data transformations
using User Defined Functions (UDFs) written in Python.
Data Source. Mapping tabular data (e.g., CSV) into the Nested Relational Model
is straightforward. The model has a one-to-one mapping of tables, rows, and
columns, unless a transformation like splitting on a column occurs, which will
create a new column that contains a nested table.
Mapping hierarchical data (e.g., JSON, XML) into the Nested Relational
Model requires a translation algorithm for each data format next to the mapping
language. Such an algorithm is considered for data in XML and JSON format. If
the data is in JSON, an object maps to a single row table in NRM with a column
for each field. Each column is populated with the value of the appropriate field.
Fields with scalar values do not need translation, but fields with array values
are translated to their own nested tables: if the array contains scalar or object
values, each array element becomes a row in the nested table. If the elements
are scalar values like strings as in the tags field, a default column name “values”
is provided. If a JSON document contains a JSON array at the top level, each
element is treated like a row in a database table. If the data is in XML format,
70 A. Dimou
its elements are treated like JSON objects, and its attributes and repeated child
elements as single-row nested table where each attribute is a column.
References. The column-valued term map is not limited to SQL identifiers as it
occurs in R2RML to support mapping nested columns in the NRM. A JSON
array is used to capture the column names that make up the path to a nested
column from the document root. The template-valued term map is also extended
to include columns that do not exist in the original input but are the result of
the transformations applied by the processor.
Joins. Joins are not supported because they are considered to be impractical
and require extensive planning and external support.
Execution Planning. A tag (km-dev:hasWorksheetHistory) is introduced to
capture the cleaning, transformation and modeling steps.
Data Transformations. The Nested Transformation Model can also be used to
embed transformation functions. A transformation function can create a new set
of nested tables instead of transforming the data values.
4.3 FunUL
FunUL [232] is an alternative to FnO for data transformations. FunUL allows
the definition of functions as part of the mapping language. In FunUL, functions
have a name and a body. The name needs to be unique. The body defines the
function using a standardized programming language. It has a return statement
and a call refers to a function with an optional set of parameters.
The FunUL vocabulary namespace is http://kdeg.scss.tcd.ie/ns/rrf# and the
preferred prefix is rrf.
The class rrf:Function defines a function (Listing 4.17, line 3). A function
definition has two properties defining the name (rrf:functionName, line 4), and
the function body (rrf:functionBody, line 5).
A function can be called using the property rrf:functionCall (Listing 4.17,
line 13). This property refers to a rrf:Function with the property rr:function
(line 14). Parameters are defined using rrf:parameterBindings (line 15).
1 @prefix rrf: <http://kdeg.scss.tcd.ie/ns/rrf#> .
2
3 <#SplitTransformation> a rrf:Function ;
4 rrf:functionName "splitTransformation" ;
5 rrf:functionBody
6 """function split(value, separator) {
7 str = value.split(separator).trim();
8 return str; ""; } """ ; .
9
10 <#FemalePoleVault> rr:predicateObjectMap [
11 rr:predicate ex:record;
12 rr:objectMap [
13 rrf:functionCall [
14 rrf:function <#SplitTransformation> ;
15 rrf:parameterBindings (
Chapter 4 Creation of Knowledge Graphs 71
16 [ rml:reference "notes" ]
17 [ rml:reference "," ] ); ];
Listing 4.17. A Function Call aligns FunUL with RML
5 Conclusions
A lack of in-depth understanding of the complexity of generating knowledge
graphs and the many degrees of freedom in modeling and representing knowledge
prevents human and software agents from profiting of the Semantic Web potential. This chapter identified the different approaches that were proposed in recent
years for generating knowledge graphs from heterogeneous data sources. Then,
the chapter focused on approaches that distinguish mapping rules definition
from their execution. Two types of mapping languages prevailed, dedicated mapping languages and repurposed mapping languages. The chapter further focused
on dedicated mapping languages because they follow the W3C-recommended
R2RML.
This chapter presents the author’s view on knowledge graph generation. It
serves as an introductory chapter to knowledge graphs, which are considered in
greater detail in the following chapters. The next two chapters will explain how
to perform federated querying and reasoning over knowledge graphs (Table 3).
Table 3. Mapping Languages comparison with respect to data retrieval
R2RML RML xR2RML KR2RML
Extends – R2RML R2RML & RML R2RML
Data source rr:LogicalTable rml:LogicalSource xrr:LogicalSource rr:LogicalTable
Data
references
– Reference formulation xrr:format –
Reference rr:column
rr:template
rml:reference
rr:template
xrr:reference
rr:template
rr:column rr:template
Reference
formulation
SQL SQL/XPath/
JSONPath acc.
Reference formulation
SQL/XPath/
JSONPath
acc. xrr:format
SQL/XPath/ JSONPath
Join rr:join rr:join (extended) rr:join (extended) Not supported
Declarative
iterator
No Yes Yes No
Iterator – rml:iterator xrr:iterator –
Query rr:sqlQuery rml:query xrr:query rr:sqlQuery
Lists – – xrr:RdfList –
72 A. Dimou
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.
Chapter 5
Federated Query Processing
Kemele M. Endris1,2(B) , Maria-Esther Vidal1 , and Damien Graux3
1 TIB Leibniz Information Centre For Science and Technology, Hannover, Germany 2 L3S Research Center, Hannover, Germany
endris@L3S.de 3 ADAPT SFI Research Centre, Trinity College, Dublin, Ireland
Abstract. Big data plays a relevant role in promoting both manufacturing and scientific development through industrial digitization and emerging interdisciplinary research. Semantic web technologies have also experienced great progress, and scientific communities and practitioners have
contributed to the problem of big data management with ontological models, controlled vocabularies, linked datasets, data models, query languages,
as well as tools for transforming big data into knowledge from which decisions can be made. Despite the significant impact of big data and semantic web technologies, we are entering into a new era where domains like
genomics are projected to grow very rapidly in the next decade. In this
next era, integrating big data demands novel and scalable tools for enabling
not only big data ingestion and curation but also efficient large-scale exploration and discovery. Federated query processing techniques provide a solution to scale up to large volumes of data distributed across multiple data
sources. Federated query processing techniques resort to source descriptions to identify relevant data sources for a query, as well as to find efficient
execution plans that minimize the total execution time of a query and maximize the completeness of the answers. This chapter summarizes the main
characteristics of a federated query engine, reviews the current state of the
field, and outlines the problems that still remain open and represent grand
challenges for the area.
1 Introduction
The number and variety of data collections have grown exponentially over recent
decades and a similar growth rate is expected in the coming years. In order to
transform the enormous amount of disparate data into knowledge from where
actions can be taken, fundamental problems, such as data integration and query
processing, must be solved. Data integration requires the effective identification of
entities that, albeit described differently, correspond to the same real-world entity.
Moreover, data is usually ingested in myriad unstructured formats and may suffer reduced quality due to biases, ambiguities, and noise. These issues impact on
the complexity of the solutions for data integration. Semantic integration of big
data entails variety by enabling the resolution of several interoperability conflicts
c The Author(s) 2020
V. Janev et al. (Eds.): Knowledge Graphs and Big Data Processing, LNCS 12072, pp. 73–86, 2020.
https://doi.org/10.1007/978-3-030-53199-7_5
74 K. M. Endris et al.
[159,446], e.g., structuredness, schematic, representation, completeness, granularity, and entity matching conflicts. Conflicts arise because data sources may have
different data models or none, follow various schemes for data representation, and
contain complementary information. Furthermore, a real-world entity may be represented using diverse properties or at various levels of detail. Thus, techniques
able to solve interoperability issues while addressing data complexity challenges
imposed by big data characteristics are required [402].
Existing solutions to the problem of query processing over heterogeneous
datasets rely on a unified interface for overcoming interoperability issues, usually based on metamodels [224]. Different approaches have been proposed, mainly
with a focus on data ingestion and metadata extraction and management.
Exemplary approaches include GEMMS [365], PolyWeb [244], BigDAWG [119],
Ontario [125], and Constance [179]. These systems collect metadata about the
main characteristics of the heterogeneous data collections, e.g., formats and
query capabilities. Additionally, they resort to a global ontology to describe
contextual information and relationships among data sets. Rich descriptions of
the properties and capabilities of the data have shown to be crucial for enabling
these systems to effectively perform query processing.
In the context of the Semantic Web, the problem of federated query processing has also been actively studied. As a result, diverse federated SPARQL query
engines have been defined that enable users to execute queries over a federation
of SPARQL endpoints. State-of-the-art techniques include FedX [389], ANAPSID [6], and MULDER [124]. FedX implements adaptive techniques to identify
relevant sources to evaluate a query. It is able to contact SPARQL endpoints
on the fly to decide the subqueries of the original query that can be executed
over the endpoints of the federation. ANAPSID makes use of metadata about the
vocabularies used on the RDF datasets to perform source selection. Based on the
selected sources, ANAPSID decomposes original queries and finds efficient plans
to collect the answers incrementally. Finally, MULDER resorts to description of
the RDF datasets based on the classes and relations of the dataset vocabularies.
MULDER proposes the concept of the RDF Molecule Templates (RDF-MTs)
to describe the datasets and efficiently perform source selection and query planning. The rich repertoire of federated query engines just reveals the importance
of query processing against the RDF dataset, as well as the attention that the
problem has received from the database and semantic web communities.
The contributions of the work are summarized as follows:
– A description of the concept of the data integration system and an analysis
of the different parameters that impact on the complexity of a system.
– A characterization of the challenges addressed by federated query engines and
analysis of the current state of the federated query processing field.
– A discussion of the analysis of the grand challenges in this area and future
directions.
The remainder of the chapter is structured as follows: Sect. 2 presents an
overview of the data integration system and the roles that they play in the
problem of accessing and processing queries over heterogeneous data sources.
Chapter 5 Federated Query Processing 75
Section 3 describes the problem of federated query processing, the main challenges to be addressed by a federated query engine, and the state of the art.
Finally, grand challenges and future directions are outlined in Sect. 4.
2 Data Integration Systems
An enormous amount of data is being published on the web [379]. In addition,
different data sources are being generated and stored within enterprises as well
due to technological advances in data collection, generation, and storage. These
data sources are created independently of each other and might belong to different
administrative entities. Hence, they have different data representation formats as
well as access interfaces. Such properties of the data sources hinder the usage of the
information available in them. Data integration is the process of providing uniform
access to a set of distributed (or decentralised), autonomous, and heterogeneous
data sources [114]. Data integration systems provide a global schema (also known
as mediated schema) to provide a reconciled view of all data available in different
data sources. Mapping between the global schema and source schema should be
established to combine data residing in data sources considered in the integration
process. Generally, data integration system is formally defined as follows [280]:
Definition 1 (Data Integration System). A data integration system, I, is
defined as a triple <G, S, M>, where:
– G is the global schema, expressed in a language LG over an alphabet AG. The
alphabet comprises a symbol for each element of G.
– S is the source schema, expressed in a language LS over an alphabet AS. The
alphabet AS includes a symbol for each element of the sources.
– M is the mapping between G and S, constituted by a set of assertions of
the forms: qS → qG, qG → qS; where qS and qG are two queries of the same
arity, respectively over the source schema S, and over the global schema G. An
assertion specifies the connection between the elements of the global schema
and those of the source schema.
Defining schema mapping is one of the main tasks in a data integration system.
Schema mapping is the specification of correspondences between the data at the
sources and the global schema. The mappings determine how the queries posed
by the user using the global schema are answered by translating to the schema
of the source that stores the data. Two basic approaches for specifying such
mappings have been proposed in the literature for data integration systems are
Global-as-View (GAV) [140,180] and Local-as-View (LAV) [282,433].
Rules defined using the Global-as-View (GAV) approach define concepts in
the global schema as a set of views over the data sources. Using the GAV approach, the mapping rules in M define the concepts of the schema in the sources,
S, with each element in the global schema. A query posed over the global schema,
G, needs to be reformulated by rewriting the query with the views defined in,
M. Such rewriting is also known as query unfolding – the process of rewriting
76 K. M. Endris et al.
the query defined over global schema to a query that only refers to the source
schema. Conceptually, GAV mappings specify directly how to compute tuples
of the global schema relations from tuples in the sources. This characteristics of
GAV mappings makes them easier for query unfolding strategy. However, adding
and removing sources in the GAV approach may involve updating all the mappings in the global schema, which requires knowledge of all the sources. Mappings
specified using the Local-as-View (LAV) approach describe the data sources as
views over the global schema, contrary to the GAV approach that defines the
global schema as views over the data sources. Using the LAV approach, the mapping rules in M associates a query defined over the global schema, G, to each
elements of source schema, S. Adding and removing sources in LAV is easier than
GAV, as data sources are described independently of each other. In addition, it
allows for expressing incomplete information as the global schema represents a
database whose tuples are unknown, i.e., the mapping M defined by LAV approach might not contain all the corresponding sources for all the elements in the
global schema, G. As a result, query answering in LAV may consist of querying
incomplete information, which is computationally more expensive [114].
In this chapter, we define a source description model, RDF Molecule Template
(RDF-MT), an abstract description of entities that share the same characteristics,
based on the GAV approach. The global schema is defined as a consolidation of
RDF-MTs extracted from each data source in the federation. Rule-based mappings, such as RML, are used to define the GAV mappings of heterogeneous data
sources. RDF-MTs are merged based on their semantic descriptions defined by
the ontology, e.g., in RDFS.
2.1 Classification of Data Integration Systems
Data integration systems can be classified with respect to the following three
dimensions: autonomy, distribution, and heterogeneity [338], Fig. 1. Autonomy
dimension characterizes the degree to which the integration system allows each
data source in the integration to operate independently. Data sources have autonomy over choice of their data model, schema, and evolution. Furthermore, sources
Fig. 1. Dimensions of data integration systems
Chapter 5 Federated Query Processing 77
also have autonomy to join or leave the integration system at any time as well
as to select which fragments of data to be accessible by the integration system and its users. Distribution dimension specifies the data that is physically
distributed across computer networks. Such distribution (or decentralization)
can be achieved by controlled distribution or by the autonomous decision of the
data providers. Finally, heterogeneity may occur due to the fact that autonomous
development of systems yields different solutions, for reasons such as different
understanding and modeling of the same real-world concepts, the technical environment, and particular requirements of the application [338]. Though there are
different types of heterogeneity of data sources, the important ones with respect
to data interoperability are related to data model, semantic, and interface heterogeneity. Data model heterogeneity captures the heterogeneity created by various modeling techniques such that each data model has different expressive
power and limitations, e.g., relational tables, property graph, and RDF. Semantic heterogeneity concerns the semantics of data and schema in each source. The
semantics of the data stored in each source are defined through the explicit definition of their meanings in the schema element. Finally, interface heterogeneity
exists if data sources in the integration system are accessible via different query
languages, e.g., SQL, Cypher, SPARQL, and API call.
Fig. 2. Classification of data integration systems
Figure 2 shows different classifications of data integration systems with
respect to distribution and heterogeneity dimensions. The first type of data
integration systems, Fig. 2.(1), loads heterogeneous data from data sources to
a centralized storage after transforming them to a common data representation
format. The second type of data integration systems, Fig. 2.(2), supports data
distributed across networks; however, they only support if the data sources in
78 K. M. Endris et al.
the system are homogeneous in terms of data model and access methods. The
third type of data integration systems, Fig. 2.(3), supports data heterogeneity
among data sources in the integration system. However, these data integration
systems are managed in a centralized way and data is stored in a distributed
file system (DFS), such as Hadoop1. Finally, the fourth type of data integration systems, Fig. 2.(4), supports data distributed across networks as well as
heterogeneity of data sources. Such integration systems utilize special software
components to extract data from the data sources using native query language
and access mechanism. They can also transform data extracted from the sources
to data representation defined by the integration system. Data sources in the
integration system might also be autonomous. Such types of system are different
from the third type by how data is distributed and stored. While the fourth
type supports any storage management, including DFS, the third type of data
integration systems supports only DFS in a centralized way. Mostly the distribution task is handled by the file system. For instance, data might be stored in
a multi-modal data management system or in Data Lake storage based only on
a distributed file system (DFS). In the third type of data integration system,
data is loaded from the original source to the centralized storage for further processing. Federated query processing systems fall in the second and fourth type
of integration system when the data sources are autonomous.
Data integration systems also have to make sure that data that is current (fresh) is accessed and integrated. Especially, for DFS-based Data Lakes,
Fig. 2.(2), and the centralized, Fig. 2.(4), integration systems, updates of the original data sources should be propagated to guarantee the freshness of data. Furthermore, when accessing an original data source from the provider is restricted,
or management of data in a local replica is preferred, integration systems
Fig. 2.(1) and (3), need to guarantee data freshness by propagating changes.
2.2 Data Integration in the Era of Big Data
In the era of big data, a large amount of structured, semi-structured, and unstructured data is being generated at a faster rate than ever before. Big data systems
that integrate different data sources need to handle such characteristics of data
efficiently and effectively. Generally, big data is defined as data whose volume,
acquisition speed, data representation, veracity, and potential value overcome
the capacity of traditional data management systems [77]. Big data is characterized by the 5Vs model: Volume denotes that generation and collection of data are
produced at increasingly big scales. Velocity represents that data is generated
and collected rapidly. Variety indicates heterogeneity in data types, formats,
structuredness, and data generation scale. Veracity refers to noise and quality
issues in the data. Finally, Value denotes the benefit and usefulness that can be
obtained from processing and mining big data.
1 https://hadoop.apache.org/.
Chapter 5 Federated Query Processing 79
There are two data access strategies for data integration: schema-on-write
and schema-on-read. In the schema-on-write strategy, data is cleansed, organized, and transformed according to a pre-defined schema before loading to the
repository. In schema-on-read strategy, raw data is loaded to the repository as-is
and schema is defined only when the data is needed for processing [27]. Data
warehouses provide a common schema and require data cleansing, aggregation,
and transformation in advance, hence, following the schema-on-write strategy.
To provide scalable and flexible data discovery, analysis, and reporting, Data
Lakes have been proposed. Unlike data warehouses, where data is loaded to the
repository after it is transformed to a target schema and data representation,
Data Lakes store data in its original format, i.e., the schema-on-read strategy.
Data Lakes provide a central repository for raw data that is made available
to the user immediately and defer any aggregation or transformation tasks to
the data analysis phase, thus addressing the problem of disconnected information silos, which is the result of non-integrated heterogeneous data sources in
isolated repositories with diverse schema and query languages. Such a central
repository may include different data management systems, such as distributed
file systems, relational database management systems, graph data management
systems, as well as triple stores for specialized data model and storage.
3 Federated Query Processing
A federated query processing system2, provides a unified access interface to a set
of autonomous, distributed, and heterogeneous data sources. While distributed
query processing systems have control over each dataset, federated query processing engines have no control over datasets in the federation and data providers
can join or leave the federation at any time and modify their datasets independently. Query processing in the context of data sources in a federation is more
difficult than in centralized systems because of the different parameters involved
that affect the performance of the query processing engine [114]. Data sources
in a federation might contain fragments of data about an entity, have different
processing capabilities, support different access patterns, access methods, and
operators. The role of a federated query engine is to transform a query expressed
in terms of the global schema, i.e., the federated query, into an equivalent query
expressed in the schema of the data sources, i.e., local query. The local query
represents the actual execution plan of the federated query by the data sources
of the federation. The transformation of the federated query to a local query
needs to be both effective and efficient. Query transformations are effective if
the generated query is equivalent to the original one, i.e., both the original and
the transformed queries produce same results. On the other hand, query transformations are efficient if the execution strategy of the transformed query makes
use of minimum computational resources and communication cost. Producing
2 We use the terms federated query processing system, federated query engine, and
federated query processing system interchangeably.
80 K. M. Endris et al.
Fig. 3. Federated query processing basic components
an efficient execution strategy is difficult as many equivalent and correct transformations can be produced and each equivalent execution strategy leads to
different consumption of resources [338]. The main objective of federated query
processing is to transform a query posed on a federation of data sources into a
query composed of the union of subqueries over individual data sources of the
federation. Further, a query plan is generated in order to speed up the processing
of each individual subquery over the selected sources, as well as the gathering of
the results into the query answer. An important part of query processing in the
context of federated data sources is query optimization as many execution plans
are correct transformations of the same federated query. The one that optimizes
(minimizes) resource consumption should be retained. Query processing performance can be measured by the total cost that will be used in query processing
and the response time of the query, i.e., time elapsed for executing the query.
As an RDF data model continues gaining popularity, publicly available RDF
datasets are growing in number and size. One of the challenges emerging from
this trend is how to efficiently and effectively execute queries over a set of
autonomous RDF datasets. Saleem et al. [380] study federated RDF query
engines with web access interfaces. Based on their survey results, the authors
divide federation approaches into three main categories: Query Federation over
SPARQL endpoints, Query Federation over Linked Data (via URI lookups), and
Query Federation on top of Distributed Hash Tables. Moreover, Acosta et al. [5]
classified federated RDF query processing engines based on the type of data
sources they support into three categories: Federation of SPARQL endpoints,
Federation of RDF Documents, and Federation of Triple Pattern Fragments.
Conceptually, federated query processing involves four main sub-problems
(components): (i) data source description, (ii) query decomposition and source
selection, (iii) query planning and optimization, and (iv) query execution. Federated query engines also include two additional sub-problems: query parsing and
Chapter 5 Federated Query Processing 81
result conciliation. Query parsing and result conciliation sub-problems deal with
syntactic issues of the given query and formatting the results returned from the
query execution, respectively. Below we provide an overview of the data source
description, query decomposition and source selection, query planning and optimization as well as query execution sub-problems.
3.1 Data Source Description
The data source description sub-problem deals with describing the data available
in data sources and managing catalogs about data sources that are participating
in the federation. Data source descriptions encode information about available
data sources in the federation, types of data in each data source, access method
of data sources, and privacy and access policies of these data sources [114]. The
specification of what data exist in data sources and how the terms used in data
sources are related to the global schema are specified by the schema mapping.
Schema mappings also represent privacy and access control restrictions as well
as statistics on the available data in each data source. Federated query engines
rely on the description of data sources in the federation to select relevant sources
that may contribute to answer a query. Data source descriptions are utilized by
source selection, query decomposition, and query optimization sub-problems.
A catalog of data source descriptions can be collected offline or during query
running-time. Based on the employed catalog of source descriptions, SPARQL
federation approaches can be divided into three categories [380]: pre-computed
catalog assisted, on-the-fly catalog assisted, and hybrid (uses both pre-computed
and on-the-fly) solutions. Pre-computed catalog-assisted federated SPARQL
query engines use three types of catalogs: service descriptions, VoID (Vocabulary of Interlinked Datasets) description, and list of predicates [335]. The first
two catalogs are computed and published by the data source providers that
contains descriptions about the set of vocabularies used, a list of classes and
predicates, as well as some statistics about the instances such as number of
triples per predicate, or class. Specifically in VoID descriptions, there is information about external linksets that indicate the existence of owl:sameAs and
other linking properties. The third type of catalog, i.e., a list of predicates, is
generated by contacting the data source endpoints and issuing SPARQL queries
and extracting predicates from the other two types of catalog.
FedX [389] does not require a catalog of source descriptions computed beforehand but uses triple pattern-wise ASK queries sent to data sources at query
time. Triple pattern-wise ASK queries are SPARQL ASK queries which contain
only one triple pattern in the graph expression of the given query. Lusail [4], like
FedX, uses an on-the-fly catalog solution for source selection and decomposition.
Unlike FedX, Lusail takes an additional step to check if pairs of triple patterns
can be evaluated as one subquery over a specific endpoint; this knowledge is
exploited by Lusail during query decomposition and optimization. Posting too
many SPARQL ASK queries can be a burden for data sources that have limited compute resources, which may result in DoS. Pre-computed catalog of data
source descriptions can be used to reduce the number of requests sent to the
82 K. M. Endris et al.
data sources. ANAPSID [6] is a federated query processing engine that employs
a hybrid solution and collects a list of RDF predicates of the triple patterns
that can be answered by the data sources and sends ASK queries when required
during query time. Publicly available dataset metadata are utilized by some
federated query processing engines as catalogs of source descriptions. SPLENDID [160] relies on instance-level metadata available as Vocabulary of Interlinked
Datasets (VoID) [10] for describing the sources in a federation. SPLENDID provides a hybrid solution by combining VoID descriptions for data source selection
along with SPARQL ASK queries submitted to each dataset at run-time for
verification. Statistical information for each predicate and types in the dataset
are organized as inverted indices, which will be used for data source selection
and join order optimization. Similarly, Semagrow [75] implements a hybrid solution, like SPLENDID, and triple pattern-wise source selection method which
uses VoID descriptions (if available) and SPARQL ASK queries.
MULDER [124] and Ontario [125] federated query engine employs source
description computed based on the concept of RDF molecules; a set of triples
that share the same subject values are called RDF Molecules. RDF Molecule
Templates (RDF-MTs) encode an abstract description of a set of RDF molecules
that share similar characteristics such as semantic type of entities. RDF Molecule
Template-based source descriptions leverage the semantics encoded in data
sources. It is composed of a semantic concept shared by RDF molecules, a
set of mapping rules, a list of properties that a molecule can have, and a list
of intra- and inter-connections between other RDF molecule templates. Such
description models provide a holistic view over the set of entities and their relationships within the data sources in the federation. For instance, Fig. 4 shows
RDF-MT based descriptions of the FedBench benchmark composed on 10 RDF
data sources.
3.2 Query Decomposition and Source Selection
Selecting the relevant data sources for a given query is one of the sub-problems
in federated query processing. Given a federated query parsed with no syntactic
problems, the query is first checked if it is semantically correct with respect to
the global schema. This step eliminates an incorrect query that yields no results
early on. The query is then simplified by, for example, removing redundant predicates. The task of source selection is to select the actual implementation of subqueries in the federation at specific data sources. The sources schema and global
schema are given by the data source descriptions as input to this sub-problem.
The query decomposition and source selection sub-problem decomposes the federated query into subqueries associated with data sources in the federation that
are selected for executing the subqueries. The number of data sources considered
for selection are bounded by the data source description given to the federated
query processing engine. Each sub-query may be associated to zero or more data
source, thus, if the query contains at least one sub-query without data source(s)
associated with it, then the global query can be rejected. Source selection task is
a critical part of query optimization. Failure to select correct data sources might
Chapter 5 Federated Query Processing 83
Fig. 4. RDF-MT-based description of FedBench. The graph comprises 387 RDFMTs and 6, 317 intra- and inter-dataset links. The dots in each circle represent RDFMTs. A line between dots in the same circle shows intra-dataset links, while a line
between dots in different circles corresponds to inter-dataset links. In numbers, there
is only one RDF-MT in ChEBI, 234 in DBpedia, six in Drugbank, one in Geonames,
11 in Jamendo, four in KEGG, 53 in LinkedMDB, two in NYTimes, and 80 in SWDF
dataset. Four of these RDF-MTs belong to at least two FedBench datasets, modeled
as separate circular dots.
lead to incomplete answers as well as high response time and resource consumption. The output of this component is a decomposed query into subqueries that
are associated with the selected data sources in the federation. Identifying the
relevant sources of a query not only leads to a complete answer but also faster
execution time.
3.3 Query Planning and Optimization
The goal of query planning is to generate an execution plan that represent the
steps on how the query is executed and which algorithms (operators) are used.
The task of query plan generation produces query execution plans, e.g., a treebased plan where the leaf of the tree corresponds to the sub-queries to be executed in selected data sources and the internal nodes corresponds to the physical
(algebraic) operators, such as join, union, project, and filter, that perform algebraic operations by the federated query processing engine. Many semantically
equivalent execution plans can be found by permuting the order of operators
and subqueries. However, the cost of executing different ordering of a query is
not always the same. In a federated setting, the number of intermediate results
as well as the communication costs impacts the performance of query execution. Federated query processing engines should use an optimization techniques
to select an optimal execution plan that reduces execution time and resource
usage, such as memory, communication, etc. Optimization of the query execution plan starts from selecting only relevant sources, decomposition and finally
84 K. M. Endris et al.
making decisions on the selection of appropriate implementation of join operations. These optimization techniques include making decisions on selection of
the join methods, ordering, and adapting to the condition of the sources. The
objective of the planning and optimization sub-problem is to find an execution
plan that minimizes the cost of processing the given query, i.e., finding the “best”
ordering of operators in the query, which is close to optimal solution. Finding
an optimal solution is computationally intractable [210]. Assuming a simplified
cost function, it is proven that the minimization of this cost function for a query
with many joins is NP-Complete. To select the ordering of operators, it is necessary to estimate execution costs of alternative candidate orderings. There are
two type of query optimization in the literature: cost-based and heuristics-based
query optimization. In cost-based optimization techniques, estimating the cost
of the generated plans, i.e., candidate orderings, requires collecting statistics on
each of the data sources either before query executions, static optimization or
during query execution, dynamic optimization. In federated settings, where data
sources are autonomous, collecting such statistics might not always be possible.
Cost-based approaches are often not possible because the data source descriptions do not have the needed statistics. Heuristic-based optimization techniques
can be used to estimate the execution cost using minimum information collected
from sources as well as the properties of the operators in the query, such as type
of predicates, operators, etc. The output of the query planning and optimization
is an optimized query, i.e., query execution plan, with operations (join, union)
between subqueries.
3.4 Query Execution
Query execution is performed by data sources that are involved in answering
sub-query(s) of the given query. Each sub-query executed in each data source is
then optimized using the local schema and index (if available) of the data source
and executed. The physical operator (and algorithms) to perform the relational
operators (join, union, filter) may be chosen. Five different join methods are used
in federated query engines: nested loop join, bound-join, hash join, symmetric
join, and multiple join [335]. In nested-loop join (NLJ) the inner sub-query is
executed for every binding of the intermediate results from the outer sub-query of
the join. The bindings that satisfy the join condition are then included in the join
results. Bound-join, like NLJ, executes inner sub-query for the set of bindings,
unlike NLJ which executes the inner sub-query for every single binding of the
intermediate results from the outer sub-query. This set of bindings can be sent as
a UNION or FILTER SPARQL operators can be used to send multiple bindings
to the inner sub-query. In the hash-join method, each sub-query (operands of the
join operation) is executed in parallel and the join is performed locally using a
single hash table at the query engine. The fourth type of join method, symmetric
Chapter 5 Federated Query Processing 85
(hash) join, is a non-blocking hash-based join that pipelines parallel execution
of the operands and generates output of the join operation as early as possible.
Several extended versions of this method are available, such as XJoin [436],
agjoin [6], and adjoin [6]. Finally, the multiple (hash) join method uses multiple
hash tables to join more than two sub-queries running at the same time.
4 Grand Challenges and Future Work
In this section, we analyze the grand challenges to be addressed in the definition
and implementation of federated query engines against distributed sources of big
data. These challenges can be summarized as follows:
– Definition of formal models able to describe not only the properties and relationships among data sources, but also represent and explain causality relations, bias, and trustworthiness.
– Adaptive query processing techniques able to adjust query processing schedules according to the availability of the data, as well as to the validity and
trustworthiness of the published data.
– Machine learning models able to predict the cost of integrating different
sources, and the benefits that the fusion of new data sources adds to the
accuracy, validity, and trustworthiness of query processing.
– Hybrid approaches that combine computational methods with human knowledge with the aim to enhance, certify, and explain the outcomes of the main
data-driven tasks, e.g., schema matching, and data curation and integration.
– Query processing able to interoperate during query execution. Furthermore,
data quality assessment and bias detection methods are required in order to
produce answers that ensure validity and trustworthiness.
– Methods capable of tracing data consumed from the selected sources, and
explainable federated systems able to justify all the decisions made to produce
the answer of a query over a federation of data sources.
The diversity of the problems that remain open presents enormous opportunities both in research and development. Advancement in this area will contribute
not only more efficient tools but also solutions that users can trust and understand. As a result, we expect a paradigm shift in the area of big data integration
and processing towards explainability and trustworthiness – issues that have
thus far prevented global adoption of data-driven tools.
86 K. M. Endris et al.
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the
chapter’s Creative Commons license, unless indicated otherwise in a credit line to the
material. If material is not included in the chapter’s Creative Commons license and
your intended use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright holder.
Chapter 6
Reasoning in Knowledge Graphs:
An Embeddings Spotlight
Luigi Bellomarini1, Emanuel Sallinger2,3(B) , and Sahar Vahdati3
1 Banca d’Italia, Rome, Italy 2 TU Wien, Vienna, Austria
sallinger@dbai.tuwien.ac.at 3 University of Oxford, Oxford, UK
Abstract. In this chapter we introduce the aspect of reasoning in
Knowledge Graphs. As in Chap. 2, we will give a broad overview focusing
on the multitude of reasoning techniques: spanning logic-based reasoning, embedding-based reasoning, neural network-based reasoning, etc. In
particular, we will discuss three dimensions of reasoning in Knowledge
Graphs. Complementing these dimensions, we will structure our exploration based on a pragmatic view of reasoning tasks and families of reasoning tasks: reasoning for knowledge integration, knowledge discovery
and application services.
1 Introduction
The notion of intelligence is closely intertwined with the ability to reason. In
turn, this ability to reason plays a central role in AI algorithms. This is the
case not only for the AI of today but for any form of knowledge representation,
understanding and discovery, as stated by Leibniz in 1677: “It is obvious that
if we could find characters or signs suited for expressing all our thoughts as
clearly and as exactly as arithmetic expresses numbers or geometry expresses
lines, we could do in all matters insofar as they are subject to reasoning all that
we can do in arithmetic and geometry. For all investigations which depend on
reasoning would be carried out by transposing these characters and by a species
of calculus” [279].
Research in reasoning was carried out by mathematicians and logicians, and
naturally adopted and also carried out by computer scientists later on. Concrete
references of having knowledgeable machines date back to at least the 1940s – V.
Bush talked about a machine able to think like a human in his influential essay in
1945 “As We May Think” [65]. Later in 1950, with Alan Turing’s seminal work
[432], the idea behind Artificial Intelligence and impressing thinking power to
machines began with mathematically employed reasoning. The developments of
symbolic reasoning continued towards providing mathematical semantics of logic